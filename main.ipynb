{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "#import pyautogui # If keyboard/mouse don't work, use pyautogui\n",
    "import keyboard\n",
    "import mouse\n",
    "import pytesseract\n",
    "from time import sleep\n",
    "import winsound\n",
    "from re import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory efficient loop\n",
    "# Helper to find specific screen regions. Use Spritex, then confirm with this.\n",
    "\n",
    "sleep(3)\n",
    "\n",
    "with mss() as sct:\n",
    "    monitor = {\"top\": 180, \"left\": 1, \"width\": 249-1, \"height\": 213-180} # Memory efficiency.\n",
    "    data = sct.grab(monitor)\n",
    "    data = Image.frombytes(\"RGB\", data.size, data.bgra, 'raw', 'BGRX')\n",
    "    data = data.convert(\"P\")\n",
    "    data = np.array(data)\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "plt.imshow(data)\n",
    "plt.show()\n",
    "\n",
    "consequence = pytesseract.image_to_string(data, config='--psm 6')\n",
    "print(consequence)\n",
    "\n",
    "consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence)\n",
    "\n",
    "print(consequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import adaptiveThreshold, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY\n",
    "\n",
    "datathresh = adaptiveThreshold(data,255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,11,5) # 11, 5\n",
    "\n",
    "consequence = pytesseract.image_to_string(datathresh, config='--psm 8')\n",
    "print(consequence)\n",
    "\n",
    "consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence)\n",
    "\n",
    "print(consequence)\n",
    "\n",
    "plt.imshow(datathresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Generates input maps and commands for Hakisa.\n",
    "\n",
    "    Remember: command_types = list of strings, actions1 and 2 = list of strings(keyboard), X coordinates or None(mouse)\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        command_types = None,\n",
    "        actions1 = None,\n",
    "        actions2 = None,\n",
    "        explore_train_steps=1000,\n",
    "        memory_size=100,\n",
    "        top=0,\n",
    "        left=0,\n",
    "        width=1920,\n",
    "        height=1080,\n",
    "        resize=None\n",
    "    ):\n",
    "\n",
    "        # Window resolutions for the screen grabber\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.resize = resize # For reducing the images. Must be a tuple (Height, Width)\n",
    "\n",
    "        self.steps = explore_train_steps\n",
    "\n",
    "        self.data = None # This will be created during training. However, it's possible to load a ready-made data for training.\n",
    "\n",
    "        self.command_type = command_types\n",
    "        self.actions1 = self._create_commands_dictionary(input_maps=actions1)\n",
    "        self.actions2 = self._create_commands_dictionary(input_maps=actions2)\n",
    "\n",
    "        self.knn = None # Creating variable so we don't have to fit KNN at every step.\n",
    "\n",
    "        self.knn_actions1 = self._fit_knn(self.actions1)\n",
    "        print(\"KNN fitted in actions 1\")\n",
    "        self.knn_actions2 = self._fit_knn(self.actions2)\n",
    "        print(\"KNN fitted in actions 2\\nAll action maps have been properly fitted by their respective KNN algorithm\")\n",
    "\n",
    "        self.key_actions1 = actions1 # For efficiency in each step\n",
    "        self.key_actions2 = actions2\n",
    "\n",
    "        self.labels = None # Used for studying\n",
    "        self.rewards = None # Also used for studying.\n",
    "\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = []\n",
    "\n",
    "\n",
    "    # Pytorch's Dataset functions will only be used in Studying mode\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        inputs = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "        rewards = self.rewards[idx]\n",
    "\n",
    "        return inputs, labels, rewards\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def _grab_frame(self):\n",
    "        # Unfortunately, this whole operation takes about 0.6 seconds, so we'll probably have to deal with a single frame each 1~3 seconds.\n",
    "        with mss() as sct:\n",
    "            frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "            frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if self.resize:\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame = np.array(frame, dtype=np.float32)\n",
    "\n",
    "            frame = torch.from_numpy(frame)\n",
    "        \n",
    "        frame = frame.view(1, frame.size(2), frame.size(0), frame.size(1)).to(device) # (Batch, Channels, Height, Width)\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def _create_commands_dictionary(self, input_maps):\n",
    "        idx2key = []\n",
    "        key2idx = {}\n",
    "\n",
    "        for key in input_maps:\n",
    "            if key not in key2idx:\n",
    "                idx2key.append(key)\n",
    "                key2idx[key] = len(idx2key) - 1\n",
    "        \n",
    "        del idx2key\n",
    "\n",
    "        maximum = max(key2idx.values())\n",
    "\n",
    "        for key, value in key2idx.items():\n",
    "\n",
    "            scaled_value = (value-0)*2.0 / (maximum - 0)-1.0\n",
    "\n",
    "            key2idx[key] = scaled_value * ((len(key2idx)+1)//2) # Using a wider range scaling to make things easier for her and for KNN.\n",
    "\n",
    "        return key2idx\n",
    "\n",
    "    def _fit_knn(self, dictionary):\n",
    "        \n",
    "        values = list(dictionary.values())\n",
    "\n",
    "        values = np.array(values).reshape(-1,1)\n",
    "\n",
    "        knn = KNN(n_neighbors=1, algorithm='kd_tree').fit(values)\n",
    "\n",
    "        del values\n",
    "\n",
    "        return knn\n",
    "        \n",
    "\n",
    "    def get_command(self, cmd_type, action1, action2):\n",
    "        '''\n",
    "        Hakisa's output: (command_type, action1, action2) ----> (key, Down, z) or (click, 100, 60)\n",
    "        command_type is the argmax output from a logsoftmax function and will be used as index for its respective list.\n",
    "        action1 and action 2 are both floats and will be passed through KNN in order to get the proper command.\n",
    "        '''\n",
    "\n",
    "        if cmd_type.ndim > 1: # Sometimes, cmd_type isn't a vector, so we must extract a vector from it in order to use it as index.\n",
    "\n",
    "            cmd_type = np.argmax(cmd_type, 1).item()\n",
    "        \n",
    "        else:\n",
    "            cmd_type = cmd_type.item()\n",
    "\n",
    "        cmd_type = self.command_type[cmd_type] # Here, cmd_type must be a vector(an array without any dimension). Remember this if you get an error here.\n",
    "\n",
    "        _, index = self.knn_actions1.kneighbors(action1)\n",
    "\n",
    "        for subarray in index:\n",
    "            for i in subarray:\n",
    "                action1 = self.key_actions1[i]\n",
    "\n",
    "        _, index = self.knn_actions2.kneighbors(action2)\n",
    "\n",
    "        for subarray in index:\n",
    "            for i in subarray:\n",
    "                action2 = self.key_actions2[i]\n",
    "        \n",
    "        del index, subarray, i\n",
    "\n",
    "        command = (cmd_type, action1, action2)\n",
    "\n",
    "        del cmd_type, action1, action2\n",
    "\n",
    "        return command\n",
    "\n",
    "    def get_consequences(self, top, left, width, height, togray=False, threshold=False, thresh_gauss=171, thresh_C=13, tesseract_config='--psm 8'):\n",
    "        '''\n",
    "        Used after Hakisa performed an input, in order to get its consequences(ex: score change, bombs, kills, deaths...).\n",
    "        Returns a string according to Tesseract's OCR.\n",
    "        '''\n",
    "\n",
    "        with mss() as sct:\n",
    "            consequence = sct.grab(monitor={\"top\": top, \"left\": left, \"width\": width, \"height\": height})\n",
    "\n",
    "            consequence = Image.frombytes(\"RGB\", consequence.size, consequence.bgra, 'raw', 'BGRX')\n",
    "\n",
    "        if togray is True:\n",
    "\n",
    "            consequence = consequence.convert(\"P\") # Sometimes, simply converting to grayscale is enough.\n",
    "\n",
    "            if threshold is True: # Thresholding can only be applied to grayscale images.\n",
    "                if \"ADAPTIVE_THRESH_GAUSSIAN_C\" and \"adaptiveThreshold\" and \"THRESH_BINARY\" not in dir():\n",
    "                    from cv2 import adaptiveThreshold, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY\n",
    "\n",
    "                consequence = adaptiveThreshold(data,255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,thresh_gauss,thresh_C)\n",
    "        \n",
    "        consequence = pytesseract.image_to_string(consequence, config=tesseract_config) \n",
    "\n",
    "        # OCR adds some strange characters(even with the whitelist function). Let's remove them.\n",
    "\n",
    "        consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence) # Attention: 0, 1 and 8 can be seen as O, l and B.\n",
    "\n",
    "        return consequence\n",
    "\n",
    "    def create_memory(self, frame, keys, values, reward):\n",
    "        '''\n",
    "        Saves data in the memory list.\n",
    "        Memory is saved in the format (frame, (command_type, action1_key, action2_key), (command_index, action1_value, action2_value), reward)\n",
    "\n",
    "        During study mode, frame will be used as input during training. The tuple of values and reward, as labels.\n",
    "        The tuple of keys is used for visualization, and reward also works as weights(helps discarding bad decisions and saving good ones).\n",
    "        \n",
    "        Use cumulative rewards, but also try avoiding numbers that are too big (like above 1000). Multiply by a number between 0 and 1 if necessary.\n",
    "\n",
    "        Memory will only be changed once it reaches its full size.\n",
    "        '''\n",
    "\n",
    "        memory = (frame, keys, values, reward) # A tuple makes each item in the list iterable...and its easier to visualize than lists of lists.\n",
    "\n",
    "\n",
    "        if len(self.memory) < self.memory_size:\n",
    "\n",
    "            self.memory.append(memory)\n",
    "        \n",
    "        else:\n",
    "            self.memory = sorted(self.memory, key=lambda x: x[3]) # Sorting list according to rewards values.\n",
    "            self.memory.pop(0) # Removing the item with lowest reward value\n",
    "            \n",
    "            self.memory.append(memory)\n",
    "        \n",
    "        del memory\n",
    "\n",
    "    def create_data_for_study(self):\n",
    "\n",
    "        # Creating dataset for studying\n",
    "\n",
    "        inputs = [i[0].cpu() for i in self.memory] # game frames. Using cpu to avoid CudaMemory errors.\n",
    "        labels = [i[2] for i in self.memory] # (command_type index, action1 value, action2 value)\n",
    "        rewards = [i[3] for i in self.memory] # Reward got in that step.\n",
    "\n",
    "        inputs = torch.cat(inputs, 0)\n",
    "\n",
    "        labels = np.stack(labels, 0).astype(np.float32) # Now converting to float here to avoid numpy.dtype == object\n",
    "        rewards = np.stack(rewards, 0).astype(np.float32)\n",
    "\n",
    "        labels = torch.from_numpy(labels)\n",
    "        rewards = torch.from_numpy(rewards)\n",
    "\n",
    "        self.data = inputs\n",
    "        self.labels = labels.to(device)\n",
    "        self.rewards = rewards.to(device)\n",
    "\n",
    "        del inputs, labels, rewards\n",
    "\n",
    "    def use_readymade_data(self, data, labels, rewards):\n",
    "        '''\n",
    "        In case you already have some data prepared with you.\n",
    "        Making a proper record function is quite difficult due to data overflow. keyboard and mouse modules has .record() functions.\n",
    "        However, in mouse's case, the slightest mouse movement will generate a list with dozens of items.\n",
    "\n",
    "        Consider using SerpentAI for this.\n",
    "\n",
    "        Also, since we're following the TD learn idea, timeseries aren't really necessary, as Hakisa\n",
    "        must be able to predict how the next state will be based on the current one, not based on a time series.\n",
    "        '''\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def save_memory(self, memory_name):\n",
    "\n",
    "        import pickle\n",
    "\n",
    "        with open(f'Hakisa_memory_{memory_name}.pkl', 'wb') as f:\n",
    "            pickle.dump(self.memory, f)\n",
    "        \n",
    "        print(f\"Memory saved! You can load it again with\")\n",
    "        print(f\"open('Hakisa_memory_{memory_name}.pkl', 'rb') as f:\\n\\tdataset.memory = pickle.load(f)\")\n",
    "        print(\"Don't forget to close the file!\")\n",
    "\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2out(input, kernel, stride, padding):\n",
    "    x = 2*padding\n",
    "    y = 1*(kernel-1)\n",
    "    z = (input + x - y - 1)/stride\n",
    "\n",
    "    output = z + 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv2out(200, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New strucutre, new techniques!\n",
    "\n",
    "Now, Hakisa will try to predict the reward for her action and for that step during Study and Play Mode.\n",
    "She'll also classify her action as bad(0), neutral(1) or good based on her commands output and will also use her previous actions as input.\n",
    "Each of those predictions will also be used for conditioning the next output, and for backpropagation.\n",
    "\n",
    "Rewards are now always cumulative and, in the Play Mode, will be multiplied by a discount factor.\n",
    "\n",
    "Exploration Mode:\n",
    "\n",
    "    No inputs ---------> Random command output\n",
    "\n",
    "Study Mode:\n",
    "\n",
    "    Frame Input ----------> Command output, Predicted Reward\n",
    "\n",
    "    The Predicted Reward will be passed to a MSE Loss having the actual Reward as target.\n",
    "    In the command output:\n",
    "        command_type will be passed to a NLLLoss\n",
    "        action1 and action 2 will, each one, be passed to a MSE Loss\n",
    "\n",
    "    The study loss will be the sum of the reward loss, command_type loss, action1 and action2 losses.\n",
    "\n",
    "    The idea stills the same: using semi-supervised learning in order to make Hakisa associate patterns in the images(state)\n",
    "    in order to choose the best output and also try to predict the result of such output.\n",
    "\n",
    "    Again, we'll be using a dataset based on the memory list, so set a explore_train_steps >> memory_size in order to correctly store\n",
    "    a proper set of state + good output + high reward. Otherwise, she'll simply learn how to make random moves.\n",
    "\n",
    "Play Mode:\n",
    "\n",
    "    Frame Input + Previous action + Previous Reward ---------> Command Output, Action quality, Predicted Reward\n",
    "\n",
    "    The previous step action will condition the next action through a concatenation in the linear layers.\n",
    "    The action quality predicted for that step will be used as label for the action quality predicted in the previous step.\n",
    "    The Previous reward will condition the output, but the predicted reward will still be compared to the actual reward.\n",
    "\n",
    "    The custom Gameplay Loss function will be discarded.\n",
    "\n",
    "    New GameplayLoss = Cross-Entropy(previous_action_quality) + MSE(predicted_reward, actual_reward)\n",
    "\n",
    "    In order to avoid instability, backpropagation will occur after a certain amount of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hakisa(torch.nn.Module):\n",
    "    '''\n",
    "    Hakisa itself.\n",
    "\n",
    "    She have 2 ways of acting, according to her current mode:\n",
    "\n",
    "        if mode = 'Explore', her inputs can be None, and will generate random outputs.\n",
    "\n",
    "        if mode = 'Study', she'll receive game frames as inputs, extract the most relevant features and,\n",
    "        in the end, will generate 2 outputs:\n",
    "\n",
    "            output 1: a tuple of commands (command_type, action1, action2). Sizes (Batch, 1)\n",
    "            output 2: prediction of the reward to be obtained through that action. Size (Batch, 1)\n",
    "\n",
    "        if mode = 'Play', she'll receive grame frames, previous output 2 and previous output 3 and generate output 2 and output 3,\n",
    "        conditioned by the inputs.\n",
    "\n",
    "    Hakisa also has an .execute_command() function, which uses keyboard and mouse modules.\n",
    "    If those don't work, uncomment the pyautogui equivalents.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_command_types, mode='Explore'):\n",
    "\n",
    "        super(Hakisa, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.n_command_types = n_command_types\n",
    "\n",
    "        # This structure must be changed with the input size...unless you'd like to use adaptive pooling\n",
    "\n",
    "        # Let's begin supposing that we're gonna use 200x200 RGB images ---> (3, 200, 200)\n",
    "\n",
    "        # 200x200\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 100, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(100)\n",
    "        self.conv2 = torch.nn.Conv2d(100, 100, kernel_size=3, stride=1, padding=1, bias=False) # 200x200\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(100)\n",
    "        self.pool2x2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # 100x100\n",
    "        self.conv3 = torch.nn.Conv2d(100, 200, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(200)\n",
    "        self.conv4 = torch.nn.Conv2d(200, 400, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(400)\n",
    "        # Add pool 2x2 ---> 50x50\n",
    "        self.conv5 = torch.nn.Conv2d(400, 800, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv6 = torch.nn.Conv2d(800, 1000, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm6 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 25x25\n",
    "        self.conv7 = torch.nn.Conv2d(1000, 1000, kernel_size=4, stride=1, bias=False) # 22x22\n",
    "        self.batchnorm7 = torch.nn.BatchNorm2d(1000)\n",
    "        self.conv8 = torch.nn.Conv2d(1000, 1000, kernel_size=3, stride=1, bias=False) # 20x20\n",
    "        self.batchnorm8 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 10x10\n",
    "        self.conv9 = torch.nn.Conv2d(1000, 800, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm9 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv10 = torch.nn.Conv2d(800, 400, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm10 = torch.nn.BatchNorm2d(400)\n",
    "        # Add pool 2x2 ---> 5x5\n",
    "        self.neuron1 = torch.nn.Linear(400*5*5, 200*2*2, bias=False)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(200*2*2)\n",
    "\n",
    "        if self.n_command_types > 1:\n",
    "            # If n_command_types = 1, the Exploration Mode will always output the same actions due to this bottleneck.\n",
    "\n",
    "            self.neuron_command_study = torch.nn.Linear(200*2*2, self.n_command_types, bias=False)\n",
    "            self.neuron_command_play = torch.nn.Linear(1200, self.n_command_types, bias=False)\n",
    "\n",
    "            # Considering the command_type that has been predicted, what should be the action1 and action2?\n",
    "\n",
    "            self.neuron2 = torch.nn.Linear(self.n_command_types, 100*2*2, bias=False)\n",
    "\n",
    "        else:\n",
    "        \n",
    "            self.neuron2_study = torch.nn.Linear(200*2*2, 100*2*2, bias=False)\n",
    "            self.neuron2_play = torch.nn.Linear(1200, 100*2*2, bias=False)\n",
    "\n",
    "        self.neuron_quality = torch.nn.Linear(3, 200, bias=False)\n",
    "\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(100*2*2)\n",
    "        self.neuron_action1 = torch.nn.Linear(100*2*2, 1, bias=False)\n",
    "        self.neuron_action2 = torch.nn.Linear(100*2*2, 1, bias=False)\n",
    "\n",
    "        self.neuron_reward = torch.nn.Linear(1, 200, bias=False)\n",
    "        self.layer_normcat = torch.nn.LayerNorm(1200)\n",
    "        self.neuron_predquality = torch.nn.Linear(2+self.n_command_types, 3, bias=False)\n",
    "        self.neuron_predreward1 = torch.nn.Linear(2+self.n_command_types, 1000, bias=False)\n",
    "        self.neuron_predreward2 = torch.nn.Linear(1000, 1, bias=False)\n",
    "\n",
    "        self.PRelu = torch.nn.PReLU(1)\n",
    "\n",
    "        self.softmax = torch.nn.LogSoftmax(-1) # Since we're using softmax here, use NLLLoss during study and play mode.\n",
    "    \n",
    "\n",
    "    def forward(self, input=None, frame_sequence=None, previous_action=None, previous_reward=None):\n",
    "\n",
    "        if self.mode == \"Explore\":\n",
    "            # Reinventing the wheel didn't work. Now, in exploration mode, Hakisa will simply generate random numbers.\n",
    "\n",
    "            if self.n_command_types > 1:\n",
    "                command_type = torch.randint(0, self.n_command_types, size=(1,), device=device) # Outputting a vector.\n",
    "            else:\n",
    "                command_type = torch.ones((0), device=device)\n",
    "                \n",
    "            action1 = torch.normal(0, max(dataset.actions1.values()), size=(1, 1), device=device)\n",
    "            action2 = torch.normal(0, max(dataset.actions2.values()), size=(1, 1), device=device)\n",
    "\n",
    "            return (command_type, action1, action2)\n",
    "\n",
    "\n",
    "        elif self.mode == 'Study':\n",
    "\n",
    "            x = self.conv1(input)\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv3(x)\n",
    "            x = self.batchnorm3(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv4(x)\n",
    "            x = self.batchnorm4(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv5(x)\n",
    "            x = self.batchnorm5(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv6(x)\n",
    "            x = self.batchnorm6(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv7(x)\n",
    "            x = self.batchnorm7(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv8(x)\n",
    "            x = self.batchnorm8(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv9(x)\n",
    "            x = self.batchnorm9(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv10(x)\n",
    "            x = self.batchnorm10(x)\n",
    "            x = self.PRelu(x)\n",
    "            \n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = x.view(x.size(0), -1) # (batch, 400*5*5)\n",
    "\n",
    "            x = self.neuron1(x) # (batch, 200*2*2)\n",
    "            x = self.layer_norm1(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            if self.n_command_types > 1:\n",
    "\n",
    "                command_type = self.neuron_command_study(x)\n",
    "                command_type = self.softmax(command_type) # (Batch, n_commands)\n",
    "\n",
    "                x = self.neuron2(command_type)\n",
    "\n",
    "            else:\n",
    "                command_type = torch.ones((input.size(0), 1), device=device)\n",
    "\n",
    "                x = self.neuron2_study(x)\n",
    "\n",
    "            x = self.layer_norm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            action1 = self.neuron_action1(x) # (Batch, 1)\n",
    "            action2 = self.neuron_action2(x) # (Batch, 1)\n",
    "\n",
    "            x = torch.cat((command_type.detach(), action1.detach(), action2.detach()), 1) # (Batch, 1+1+n_commands)\n",
    "\n",
    "            x = self.neuron_predreward1(x)\n",
    "            predicted_reward = self.neuron_predreward2(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "            return (command_type, action1, action2), predicted_reward\n",
    "\n",
    "        else:\n",
    "\n",
    "            x = self.conv1(input)\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv3(x)\n",
    "            x = self.batchnorm3(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv4(x)\n",
    "            x = self.batchnorm4(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv5(x)\n",
    "            x = self.batchnorm5(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv6(x)\n",
    "            x = self.batchnorm6(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv7(x)\n",
    "            x = self.batchnorm7(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv8(x)\n",
    "            x = self.batchnorm8(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv9(x)\n",
    "            x = self.batchnorm9(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv10(x)\n",
    "            x = self.batchnorm10(x)\n",
    "            x = self.PRelu(x)\n",
    "            \n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = x.view(x.size(0), -1) # (batch, 400*5*5)\n",
    "\n",
    "            x = self.neuron1(x) # (batch, 200*2*2)\n",
    "\n",
    "            if previous_action==None and previous_reward==None: # For first iteration\n",
    "\n",
    "                previous_action = (torch.zeros((1, 3), device=device), torch.zeros(1, device=device), torch.zeros(1, device=device))\n",
    "                previous_reward = torch.zeros((input.size(0), 1), device=device)\n",
    "\n",
    "            a, b, c = previous_action\n",
    "            previous_action = a + b + c\n",
    "\n",
    "            del a, b, c\n",
    "\n",
    "            previous_action = self.neuron_quality(previous_action) # (batch, 200)\n",
    "            previous_reward = self.neuron_reward(previous_reward) # (batch, 200)\n",
    "\n",
    "            x = torch.cat((x, previous_action, previous_reward), 1) # (batch, 1200)\n",
    "\n",
    "            x = self.layer_normcat(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            if self.n_command_types > 1:\n",
    "\n",
    "                command_type = self.neuron_command_play(x)\n",
    "                command_type = self.softmax(command_type) # (Batch, n_commands)\n",
    "\n",
    "                x = self.neuron2(command_type)\n",
    "\n",
    "            else:\n",
    "                command_type = torch.ones((input.size(0), 1), device=device)\n",
    "\n",
    "                x = self.neuron2_play(x) # (Batch, 1)\n",
    "\n",
    "            x = self.layer_norm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            action1 = self.neuron_action1(x) # (Batch, 1)\n",
    "            action2 = self.neuron_action2(x) # (Batch, 1)\n",
    "\n",
    "            x = torch.cat((command_type, action1, action2), 1) # (Batch, 1+1+n_commands)\n",
    "\n",
    "            command_quality = self.neuron_predquality(x) # (Batch, 3)\n",
    "            command_quality = self.softmax(command_quality)\n",
    "\n",
    "            x = self.neuron_predreward1(x)\n",
    "            predicted_reward = self.neuron_predreward2(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "            return (command_type, action1, action2), command_quality, predicted_reward\n",
    "\n",
    "\n",
    "    def execute_command(self, command):\n",
    "        '''\n",
    "        Command must be a tuple(command_type, action1, action2), where:\n",
    "\n",
    "            command_type: key(keyboard) or move, rightclick, click(mouse)\n",
    "            action1: Up, Down, press(keyboard), X coordinate(mouse) or None(no mouse movement, when using PyAutoGUI)\n",
    "            action2: 'a', 'z', 'shift'...(keyboard), Y coordinate(mouse) or None(no mouse movement, when using PyAutoGUI)\n",
    "\n",
    "            PS: Using None when using mouse module will throw an error.\n",
    "\n",
    "        Make sure all key actions(action2) are lowered.\n",
    "\n",
    "        Have in mind that Hakisa might output command_type 'key' and action1 that is equivalent to a mouse action.\n",
    "        '''\n",
    "\n",
    "        if \"key\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                \n",
    "                if \"Up\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        #pyautogui.keyUp(command[2])\n",
    "                        keyboard.release(command[2])\n",
    "                \n",
    "                    except:\n",
    "                        pass # If Hakisa predicts a mouse action for a keyboard command, she won't do anything.\n",
    "\n",
    "                elif \"Down\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        #pyautogui.keyDown(command[2])\n",
    "                        keyboard.press(command[2])\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif \"press\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        keyboard.send(command[2]) # Some games won't work with pyautogui.press(), so use keyboard module, since we'll import it for Play Mode.\n",
    "                    \n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            except:\n",
    "\n",
    "                pass # If Hakisa predicts a keyboard command, but outputs a mouse action, she won't do anything.\n",
    "\n",
    "        elif \"move\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                #pyautogui.moveTo(command[1], command[2], duration=0.19) # Duration = 0.19 seconds to be more realistic\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "\n",
    "            except:\n",
    "                pass # If Hakisa predict a mouse command, but outputs a keyboard action, she won't do anything.\n",
    "\n",
    "        elif \"rightclick\" in command[0]:\n",
    "            \n",
    "            try:\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "                mouse.right_click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif \"click\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                #pyautogui.moveTo(command[1], command[2], duration=0.19)\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "                mouse.click() # Same case as press. Use mouse module.\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError # It was probably you who made a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jigoku Kisetsukan\n",
    "\n",
    "command_type = ['key']\n",
    "\n",
    "actions1 = ['Down', 'Up']\n",
    "\n",
    "actions2 = ['up', 'down', 'left', 'right', 'z', 'x', 'shift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN fitted in actions 1\n",
      "KNN fitted in actions 2\n",
      "All action maps have been properly fitted by their respective KNN algorithm\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(command_types=command_type, actions1=actions1, actions2=actions2, explore_train_steps=100, memory_size=10, resize=(200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['key']\n",
      "{'Down': -1.0, 'Up': 1.0}\n",
      "{'up': -4.0, 'down': -2.666666666666667, 'left': -1.3333333333333335, 'right': 0.0, 'z': 1.333333333333333, 'x': 2.666666666666667, 'shift': 4.0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.command_type)\n",
    "print(dataset.actions1)\n",
    "print(dataset.actions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN fitted in actions 1\n",
      "KNN fitted in actions 2\n",
      "All action maps have been properly fitted by their respective KNN algorithm\n"
     ]
    }
   ],
   "source": [
    "# Bullet Heaven\n",
    "\n",
    "command_types = ['move', 'click', 'rightClick']\n",
    "\n",
    "actions1 = [i for i in range(1, 1919)] # Avoiding using the extremes so we don't have to shut down PyAutoGUI safety lock.\n",
    "\n",
    "actions2 = [i for i in range(1, 1079)]\n",
    "\n",
    "dataset = Dataset(command_types, actions1, actions2, explore_train_steps=100, memory_size=10, resize=(200,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hakisa = Hakisa(n_command_types=3, mode='Explore').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Jigoku(score):\n",
    "    # For the game Jigoku Kisetsukan: Sense of the Seasons\n",
    "\n",
    "    score = score.replace('S', '5').replace('s', '8').replace('e', '2').replace('O', '0').replace('B', '8').replace('o', '4').replace('b', '4')\n",
    "    score = score.replace('I', '1').replace('l', '1').replace('.', '')\n",
    "\n",
    "    try:\n",
    "        score = float(score)\n",
    "\n",
    "    except ValueError:\n",
    "            score = 1.0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_BH2(score):\n",
    "    # For the game Bullet Heaven 2\n",
    "\n",
    "    score = score.replace('S', '5').replace('s', '8').replace('e', '2').replace('O', '0').replace('B', '8').replace('o', '0').replace('.', '')\n",
    "    score = sub('[^0-9]', '', score)\n",
    "\n",
    "    try:\n",
    "        score = float(score)\n",
    "\n",
    "    except ValueError:\n",
    "        score = 1.0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration loop - Jigoku Kisetsukan\n",
    "\n",
    "from win32gui import GetWindowText, GetForegroundWindow # To make sure she's playing your game\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "game_window = GetWindowText(GetForegroundWindow())\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "reward = 0\n",
    "\n",
    "for step in range(dataset.steps):\n",
    "\n",
    "    if GetWindowText(GetForegroundWindow()) != game_window:\n",
    "        print(\"I don't want to play anymore!\")\n",
    "        break\n",
    "\n",
    "    start_step = time()\n",
    "\n",
    "    frame = dataset._grab_frame() # Getting environment state\n",
    "\n",
    "    cmds = hakisa()\n",
    "\n",
    "    command = dataset.get_command(cmds[0].cpu().numpy(), cmds[1].cpu().numpy(), cmds[2].cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    # If Hakisa acts too fast, the consequence for her action might not appear right now. Not that I wouldn't want her to be fast and efficient...\n",
    "\n",
    "    score = dataset.get_consequences(1008, 1429, 1723-1429, 1046-1008, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_Jigoku(score)\n",
    "\n",
    "    mult_score = dataset.get_consequences(933, 1536, 1723-1536, 978-933, tesseract_config='--psm 8')\n",
    "\n",
    "    mult_score = preprocess_Jigoku(mult_score)\n",
    "    mult_score = mult_score/100\n",
    "\n",
    "    life = dataset.get_consequences(849, 400, 498-400, 904-849, tesseract_config='--psm 8')\n",
    "\n",
    "    life = preprocess_Jigoku(life)\n",
    "    life = life/100\n",
    "\n",
    "    power = dataset.get_consequences(923, 405, 503-405, 978-923, tesseract_config='--psm 8')\n",
    "\n",
    "    power = preprocess_Jigoku(power)\n",
    "    power = power/100\n",
    "\n",
    "    aura = dataset.get_consequences(1001, 400, 1045-1001, 503-400, tesseract_config='--psm 8')\n",
    "\n",
    "    aura = preprocess_Jigoku(aura)\n",
    "    aura = aura/100\n",
    "\n",
    "    if life == 0:\n",
    "\n",
    "        try:\n",
    "                reward += -(100/(score * mult_score))\n",
    "        \n",
    "        except ZeroDivisionError:\n",
    "                reward += -10\n",
    "\n",
    "    else:\n",
    "\n",
    "        reward += ((score * mult_score) + (power * aura))*1e-6 # Jigoku Kisetsukan deals with score numbers around hundreds of thousands.\n",
    "\n",
    "    dataset.create_memory(frame, keys=command, values=(cmds[0].cpu(), cmds[1].cpu(), cmds[2].cpu()), reward=reward)\n",
    "\n",
    "    end_step = time()\n",
    "\n",
    "    print(f\"Step complete! {end_step-start_step}\")\n",
    "\n",
    "    if step == (dataset.steps - 1):\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en.wav', winsound.SND_FILENAME) # Because yes\n",
    "\n",
    "end = time()\n",
    "print(f\"Loop complete!\\nTime spent: {end-start} seconds\\nNumber of steps: {step}\")\n",
    "\n",
    "del frame, command, score, mult_score, life, power, aura, reward, step, start, end\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_rest_1_en.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step complete! 2.2599964141845703\n",
      "Step complete! 0.8769965171813965\n",
      "Step complete! 0.846994161605835\n",
      "Step complete! 0.7700028419494629\n",
      "Step complete! 0.7539963722229004\n",
      "Step complete! 0.7410037517547607\n",
      "Step complete! 0.7169966697692871\n",
      "Step complete! 0.7269859313964844\n",
      "Step complete! 0.7450008392333984\n",
      "Step complete! 0.7675182819366455\n",
      "Step complete! 0.7320003509521484\n",
      "Step complete! 0.7339982986450195\n",
      "Step complete! 0.7585504055023193\n",
      "Step complete! 0.8449945449829102\n",
      "Step complete! 0.7750005722045898\n",
      "Step complete! 0.7490017414093018\n",
      "Step complete! 0.7495441436767578\n",
      "Step complete! 0.7270002365112305\n",
      "Step complete! 0.7509994506835938\n",
      "Step complete! 0.7410016059875488\n",
      "Step complete! 0.7530055046081543\n",
      "Step complete! 0.7521631717681885\n",
      "Step complete! 0.7340030670166016\n",
      "Step complete! 0.7449967861175537\n",
      "Step complete! 0.7280025482177734\n",
      "Step complete! 0.7369980812072754\n",
      "Step complete! 0.7351484298706055\n",
      "Step complete! 0.7169976234436035\n",
      "Step complete! 0.747002363204956\n",
      "Step complete! 0.7565133571624756\n",
      "Step complete! 0.7269995212554932\n",
      "Step complete! 0.7520086765289307\n",
      "Step complete! 0.7080028057098389\n",
      "Step complete! 0.7530238628387451\n",
      "Step complete! 0.7480020523071289\n",
      "Step complete! 0.7449908256530762\n",
      "Step complete! 0.7875170707702637\n",
      "Step complete! 0.7600023746490479\n",
      "Step complete! 0.7650115489959717\n",
      "Step complete! 0.7440016269683838\n",
      "Step complete! 0.7349996566772461\n",
      "Step complete! 0.7669966220855713\n",
      "Step complete! 0.7681949138641357\n",
      "Step complete! 0.757000207901001\n",
      "Step complete! 0.7516357898712158\n",
      "Step complete! 0.7460026741027832\n",
      "Step complete! 0.7089982032775879\n",
      "Step complete! 0.7570207118988037\n",
      "Step complete! 0.7229981422424316\n",
      "Step complete! 0.7960042953491211\n",
      "Step complete! 0.7469968795776367\n",
      "Step complete! 0.7499992847442627\n",
      "Step complete! 0.7600045204162598\n",
      "Step complete! 0.7719926834106445\n",
      "Step complete! 0.7450027465820312\n",
      "Step complete! 0.7160069942474365\n",
      "Step complete! 0.7050025463104248\n",
      "Step complete! 0.7019987106323242\n",
      "Step complete! 0.7150006294250488\n",
      "Step complete! 0.7180001735687256\n",
      "Step complete! 0.7139995098114014\n",
      "Step complete! 0.7500026226043701\n",
      "Step complete! 0.7175233364105225\n",
      "Step complete! 0.7349998950958252\n",
      "Step complete! 0.7319996356964111\n",
      "Step complete! 0.713005542755127\n",
      "Step complete! 0.7209982872009277\n",
      "Step complete! 0.7300014495849609\n",
      "Step complete! 0.6925127506256104\n",
      "Step complete! 0.7379958629608154\n",
      "Step complete! 0.7300004959106445\n",
      "Step complete! 0.7190027236938477\n",
      "Step complete! 0.705998420715332\n",
      "Step complete! 0.7195165157318115\n",
      "Step complete! 0.7240066528320312\n",
      "Step complete! 0.7389967441558838\n",
      "Step complete! 0.7489957809448242\n",
      "Step complete! 0.7270009517669678\n",
      "Step complete! 0.7635242938995361\n",
      "Step complete! 0.7740037441253662\n",
      "Step complete! 0.7249982357025146\n",
      "Step complete! 0.7420003414154053\n",
      "Step complete! 0.7760004997253418\n",
      "Step complete! 0.7420015335083008\n",
      "Step complete! 0.7600016593933105\n",
      "Step complete! 0.7455177307128906\n",
      "Step complete! 0.7199974060058594\n",
      "Step complete! 0.7670025825500488\n",
      "Step complete! 0.7480018138885498\n",
      "Step complete! 0.7319989204406738\n",
      "Step complete! 0.7270016670227051\n",
      "Step complete! 0.7895128726959229\n",
      "Step complete! 0.7360000610351562\n",
      "Step complete! 0.7400057315826416\n",
      "Step complete! 0.7250006198883057\n",
      "Step complete! 0.7319996356964111\n",
      "Step complete! 0.7255311012268066\n",
      "Step complete! 0.7160003185272217\n",
      "Step complete! 0.7350113391876221\n",
      "Step complete! 0.7630424499511719\n",
      "Loop complete!\n",
      "Time spent: 83.76602411270142 seconds\n",
      "Number of steps: 99\n"
     ]
    }
   ],
   "source": [
    "# Exploration loop - Bullet Heaven 2\n",
    "\n",
    "from win32gui import GetWindowText, GetForegroundWindow # To make sure she's playing your game\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "start = time()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "game_window = GetWindowText(GetForegroundWindow())\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "reward = 0\n",
    "\n",
    "for step in range(dataset.steps):\n",
    "\n",
    "    if GetWindowText(GetForegroundWindow()) != game_window:\n",
    "        print(\"I don't want to play anymore!\")\n",
    "        break\n",
    "\n",
    "    start_step = time()\n",
    "\n",
    "    frame = dataset._grab_frame() # Getting environment state\n",
    "\n",
    "    cmds = hakisa()\n",
    "\n",
    "    command = dataset.get_command(cmds[0].cpu().numpy(), cmds[1].cpu().numpy(), cmds[2].cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    # If Hakisa acts too fast, the consequence for her action might not appear right now. Not that I wouldn't want her to be fast and efficient...\n",
    "\n",
    "    score = dataset.get_consequences(180, 1, 249-1, 213-180, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_BH2(score)\n",
    "\n",
    "    try:\n",
    "\n",
    "        reward += math.log10(score)\n",
    "    \n",
    "    except:\n",
    "\n",
    "        reward += 0.0\n",
    "\n",
    "    dataset.create_memory(frame, keys=command, values=(cmds[0].cpu(), cmds[1].cpu(), cmds[2].cpu()), reward=reward)\n",
    "\n",
    "    end_step = time()\n",
    "\n",
    "    print(f\"Step complete! {end_step-start_step}\")\n",
    "\n",
    "    if step == (dataset.steps - 1):\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en.wav', winsound.SND_FILENAME) # Because yes\n",
    "\n",
    "end = time()\n",
    "print(f\"Loop complete!\\nTime spent: {end-start} seconds\\nNumber of steps: {step}\")\n",
    "\n",
    "del frame, command, score, reward, step, start, end\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_rest_1_en.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, this is where we got the CUDA RuntimeError. Try using resize in Dataset creator and adjust Hakisa accordingly.\n",
    "\n",
    "dataset.create_data_for_study()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "hakisa.mode = 'Study'\n",
    "costs = []\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = None\n",
    "start_epoch = 0\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to continue from a checkpoint.\n",
    "\n",
    "params = torch.load(f'Hakisa/Hakisa_checkpoint.tar')\n",
    "start_epoch = params['Epoch'] + 1\n",
    "hakisa.load_state_dict(params['Hakisa_params'])\n",
    "lr = params['Hakisa_LR']\n",
    "\n",
    "del params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studying loop - Classic supervised learning. Will help Hakisa try to create certain patterns for situations and her reactions.\n",
    "\n",
    "# Can be applied to any game\n",
    "\n",
    "import os\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.1)\n",
    "\n",
    "command_type_loss = torch.nn.NLLLoss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    for i, (input_frame, label, reward) in enumerate(dataloader):\n",
    "\n",
    "        label, reward = label.to(device), reward.to(device)\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        cmds, predicted_reward = hakisa(input_frame.to(device))\n",
    "\n",
    "        del input_frame\n",
    "\n",
    "        if len(dataset.command_type) != 1:\n",
    "\n",
    "            command_type_cost = command_type_loss(cmds[0], label[:, 0].long())\n",
    "        \n",
    "        else:\n",
    "            command_type_cost = 0.\n",
    "\n",
    "        action1_loss = mse_loss(cmds[1].view(-1), label[:, 1])\n",
    "        action2_loss = mse_loss(cmds[2].view(-1), label[:, 2])\n",
    "\n",
    "        reward_loss = mse_loss(predicted_reward.view(-1), reward)\n",
    "\n",
    "        study_loss = command_type_cost + action1_loss + action2_loss + reward_loss\n",
    "\n",
    "        study_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        for n, p in hakisa.named_parameters():\n",
    "\n",
    "            if 'neuron1.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        if study_loss.item() < best_loss:\n",
    "\n",
    "            best_loss = study_loss.item()\n",
    "            best_params = hakisa.state_dict()\n",
    "\n",
    "        if i % dataset.memory_size == 0:\n",
    "            print(f\"{epoch}/{epochs}\")\n",
    "            print(f\"Best Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "            print(f\"Predicted Reward: {predicted_reward[0].item()}\\tActual Reward: {reward[0].item()}\")\n",
    "            print(f\"Reward loss: {reward_loss}\")\n",
    "            print(f\"command_type loss: {command_type_cost}\\taction1_loss: {action1_loss}\\taction2_loss: {action2_loss}\")\n",
    "\n",
    "            if save_path is None:\n",
    "                try:\n",
    "                    os.mkdir(\"Hakisa\")\n",
    "                    save_path = \"Hakisa\"\n",
    "                except:\n",
    "                    save_path = \"Hakisa\"\n",
    "                    \n",
    "            torch.save({\n",
    "                'Epoch': epoch,\n",
    "                'Hakisa_params': best_params,\n",
    "                'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "            }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "            print(\"Model saved!\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_memory(memory_name='jigoku_kisetsukan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Hakisa_memory_jigoku_kisetsukan.pkl', 'rb') as f:\n",
    "\n",
    "    dataset.memory = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing loop - She learns as she plays\n",
    "# Jigoku Kisetsukan\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_learnweaponskill_Rapier_2_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "hakisa.mode = 'Play'\n",
    "\n",
    "reward = 0. # Cumulative reward\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = 'Hakisa'\n",
    "steps = 0\n",
    "save_point = 10 # Also optimization point\n",
    "uncertainty_factor = 0.9 # Also known as gamma or discount factor\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.1)\n",
    "\n",
    "action_quality_loss = torch.nn.NLLLoss()\n",
    "reward_loss = torch.nn.MSELoss()\n",
    "\n",
    "while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "\n",
    "    if steps == 0: # First iteration\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame)\n",
    "\n",
    "    else:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame, previous_action=(cmds[0].detach(), cmds[1].detach(), cmds[2].detach()), previous_reward=predicted_reward.detach())\n",
    "\n",
    "    del frame\n",
    "\n",
    "    command = dataset.get_command(cmds[0].detach().cpu().numpy(), cmds[1].detach().cpu().numpy(), cmds[2].detach().cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    score = dataset.get_consequences(1008, 1429, 1723-1429, 1046-1008, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_Jigoku(score)\n",
    "\n",
    "    mult_score = dataset.get_consequences(933, 1536, 1723-1536, 978-933, tesseract_config='--psm 8')\n",
    "\n",
    "    mult_score = preprocess_Jigoku(mult_score)\n",
    "\n",
    "    life = dataset.get_consequences(849, 400, 498-400, 904-849, tesseract_config='--psm 8')\n",
    "\n",
    "    life = preprocess_Jigoku(life)\n",
    "\n",
    "    power = dataset.get_consequences(923, 405, 503-405, 978-923, tesseract_config='--psm 8')\n",
    "\n",
    "    power = preprocess_Jigoku(power)\n",
    "\n",
    "    aura = dataset.get_consequences(1001, 400, 1045-1001, 503-400, tesseract_config='--psm 8')\n",
    "\n",
    "    aura = preprocess_Jigoku(aura)\n",
    "    aura = aura/100\n",
    "\n",
    "    if life == 0:\n",
    "\n",
    "        try:\n",
    "            reward += -(100./(score * mult_score))\n",
    "        \n",
    "        except ZeroDivisionError:\n",
    "            reward += -10.\n",
    "\n",
    "    else:\n",
    "\n",
    "        reward += ((score * mult_score) + (power * aura))*1e-6\n",
    "\n",
    "    del score, mult_score, power, aura, life\n",
    "\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        previous_command_quality = command_quality.detach()\n",
    "\n",
    "    action_quality_cost = action_quality_loss(previous_command_quality, command_quality.argmax(1).detach()) # Input = (1, Classes), Target = (1)\n",
    "\n",
    "    predicted_reward = predicted_reward * uncertainty_factor\n",
    "\n",
    "    reward_cost = reward_loss(predicted_reward, reward)\n",
    "\n",
    "    gameplay_loss = action_quality_cost + reward_cost\n",
    "\n",
    "    gameplay_loss.backward()\n",
    "\n",
    "    previous_command_quality = command_quality.detach()\n",
    "\n",
    "    del command_quality\n",
    "\n",
    "    for n, p in hakisa.named_parameters(): # Checking how the grads and backpropagation are going\n",
    "\n",
    "            if 'neuron1.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if gameplay_loss.item() < best_loss:\n",
    "\n",
    "        best_loss = gameplay_loss.item()\n",
    "        best_params = hakisa.state_dict()\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Best Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward}\")\n",
    "\n",
    "        torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing loop - She learns as she plays\n",
    "# Bullet Heaven 2\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_learnweaponskill_Rapier_2_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "hakisa.mode = 'Play'\n",
    "\n",
    "reward = 0. # Cumulative reward\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = 'Hakisa'\n",
    "steps = 0\n",
    "save_point = 5 # Also optimization point\n",
    "uncertainty_factor = 0.9 # Also known as gamma or discount factor\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.1)\n",
    "\n",
    "action_quality_loss = torch.nn.NLLLoss()\n",
    "reward_loss = torch.nn.MSELoss()\n",
    "\n",
    "while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame)\n",
    "\n",
    "    else:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame, previous_action=(cmds[0].detach(), cmds[1].detach(), cmds[2].detach()), previous_reward=predicted_reward.detach())\n",
    "\n",
    "    del frame\n",
    "\n",
    "    command = dataset.get_command(cmds[0].detach().cpu().numpy(), cmds[1].detach().cpu().numpy(), cmds[2].detach().cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    score = dataset.get_consequences(180, 1, 249-1, 213-180, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_BH2(score)\n",
    "\n",
    "    try:\n",
    "\n",
    "        reward += math.log10(score)\n",
    "    \n",
    "    except:\n",
    "\n",
    "        reward += 0.0\n",
    "\n",
    "    del score\n",
    "\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        previous_command_quality = command_quality.detach()\n",
    "\n",
    "    action_quality_cost = action_quality_loss(previous_command_quality, command_quality.argmax(1).detach()) # Input = (1, Classes), Target = (1)\n",
    "\n",
    "    predicted_reward = predicted_reward * uncertainty_factor\n",
    "\n",
    "    reward_cost = reward_loss(predicted_reward, reward)\n",
    "\n",
    "    gameplay_loss = action_quality_cost + reward_cost\n",
    "\n",
    "    gameplay_loss.backward()\n",
    "\n",
    "    previous_command_quality = command_quality.detach()\n",
    "\n",
    "    del command_quality\n",
    "\n",
    "    for n, p in hakisa.named_parameters():\n",
    "\n",
    "            if 'neuron1.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if gameplay_loss.item() < best_loss:\n",
    "\n",
    "        best_loss = gameplay_loss.item()\n",
    "        best_params = hakisa.state_dict()\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Best Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward}\")\n",
    "        print(command)\n",
    "\n",
    "        torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

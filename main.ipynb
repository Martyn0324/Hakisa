{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "#import pyautogui\n",
    "import keyboard\n",
    "import mouse\n",
    "import pytesseract\n",
    "from time import sleep\n",
    "import winsound\n",
    "from re import sub\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Generates commands for Hakisa. Based on NLP/Classic RL approach.\n",
    "\n",
    "        command_type = list of command types (rightclick, click, keyboard).\n",
    "        action1 = list of actions1 (up, down, press, X_coordinate). X_coordinate is for mouse actions.\n",
    "        action2 = list of actions2 (keyboard_key, Y_coordinate). The keyboard key must be lowered.\n",
    "        top, left, width, height = denotes the capture space for the frame capture.\n",
    "        resize = A tuple (Height, Width), if you'd like to resize your image in order to consume less memory.\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        command_type=None,\n",
    "        actions1=None,\n",
    "        actions2=None,\n",
    "        top=0,\n",
    "        left=0,\n",
    "        width=1920,\n",
    "        height=1080,\n",
    "        resize=None\n",
    "    ):\n",
    "\n",
    "        # Window resolutions for the screen grabber\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.resize = resize # For reducing the images. Must be a tuple (Height, Width)\n",
    "\n",
    "        self.command_type = command_type\n",
    "        self.actions1 = actions1\n",
    "        self.actions2 = actions2\n",
    "\n",
    "        # For Study Mode\n",
    "        self.data = None\n",
    "        self.encoded_command_type = None\n",
    "        self.encoded_actions1 = None\n",
    "        self.encoded_actions2 = None\n",
    "\n",
    "\n",
    "    # Pytorch's Dataset functions will only be used in Study mode\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        frames = self.data[idx]\n",
    "        command_type = self.encoded_command_type[idx]\n",
    "        action1 = self.encoded_actions1[idx]\n",
    "        action2 = self.encoded_actions2[idx]\n",
    "\n",
    "        return frames, command_type, action1, action2\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def _grab_frame(self):\n",
    "        # Unfortunately, this whole operation takes about 0.6 seconds, so we'll probably have to deal with a single frame each 1~3 seconds.\n",
    "        with mss() as sct:\n",
    "            frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "            frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if self.resize:\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame = np.array(frame, dtype=np.float32)\n",
    "\n",
    "            frame = torch.from_numpy(frame)\n",
    "        \n",
    "        frame = frame.view(1, frame.size(2), frame.size(0), frame.size(1)).to(device) # (Batch, Channels, Height, Width)\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def get_command(self, command_type_idx, action1_idx, action2_idx):\n",
    "        '''\n",
    "        Hakisa output for true commands = (command_type, action1, action2)\n",
    "\n",
    "        Remember to use int(command_idx.detach().cpu().item()). before passing the inputs.\n",
    "        '''\n",
    "\n",
    "        command_type = self.command_type[command_type_idx]\n",
    "        action1 = self.actions1[action1_idx]\n",
    "        action2 = self.actions2[action2_idx]\n",
    "\n",
    "        command = (command_type, action1, action2)\n",
    "\n",
    "        return command\n",
    "\n",
    "    def get_consequences(self, top, left, width, height, togray=False, threshold=False, thresh_gauss=171, thresh_C=13, tesseract_config='--psm 8'):\n",
    "        '''\n",
    "        Used after Hakisa performed an input, in order to get its consequences(ex: score change, bombs, kills, deaths...).\n",
    "        Returns a string according to Tesseract's OCR.\n",
    "\n",
    "        With a reward model, this function might be used to generate an input for the reward model.\n",
    "        '''\n",
    "\n",
    "        with mss() as sct:\n",
    "            consequence = sct.grab(monitor={\"top\": top, \"left\": left, \"width\": width, \"height\": height})\n",
    "\n",
    "            consequence = Image.frombytes(\"RGB\", consequence.size, consequence.bgra, 'raw', 'BGRX')\n",
    "\n",
    "        if togray is True:\n",
    "\n",
    "            consequence = consequence.convert(\"P\") # Sometimes, simply converting to grayscale is enough\n",
    "\n",
    "            if threshold is True:\n",
    "                if \"ADAPTIVE_THRESH_GAUSSIAN_C\" and \"adaptiveThreshold\" and \"THRESH_BINARY\" not in dir():\n",
    "                    from cv2 import adaptiveThreshold, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY\n",
    "\n",
    "                consequence = adaptiveThreshold(np.array(consequence),255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,thresh_gauss,thresh_C)\n",
    "                consequence = Image.fromarray(consequence)\n",
    "        \n",
    "        consequence = pytesseract.image_to_string(consequence, config=tesseract_config) \n",
    "\n",
    "        # OCR adds some strange characters(even with the whitelist function). Let's remove them.\n",
    "\n",
    "        consequence = sub('[^A-Za-z0-9/.]', '', consequence) # Attention: 0, 1 and 8 can be seen as O, l and B.\n",
    "\n",
    "        return consequence\n",
    "\n",
    "    def record_gameplay(self, number_of_screenshots, screenshot_delay, grayscale=False, resize=False, path=None):\n",
    "\n",
    "        # Resizing and grayscaling isn't really necessary here, but can save you some time later.\n",
    "        # Both saving you from writing more code and from making your hardware having to process more and more data at once.\n",
    "\n",
    "        print(f\"Ok. Screenshot capture will begin in 5 seconds\")\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "        for i in range(number_of_screenshots):\n",
    "\n",
    "            with mss() as sct:\n",
    "\n",
    "                frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "                frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if grayscale:\n",
    "\n",
    "                frame = frame.convert('L')\n",
    "\n",
    "            if resize:\n",
    "\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame.save(f\"{path}/{i}.png\")\n",
    "\n",
    "            sleep(screenshot_delay)\n",
    "        \n",
    "        print(\"Screenshot capture finished!\")\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "    def create_data(self, data, commands):\n",
    "        '''\n",
    "        If you'd like, you can also generate a dataset for dataloader for Study Mode.\n",
    "\n",
    "        data = tensor of size (N_samples, Channels, Height, Width) containing game frames.\n",
    "            The range of pixel values must be the same range\n",
    "            that will be used during Reinforcement Learning, that is, if you use scaled images here, you must also use the same scaling during RL.\n",
    "            Unscaled data in Regression tasks are prone to exploding gradients. However, since we're using HuberLoss and clippings, this won't be a problem.\n",
    "            There wasn't any problem during tests with the ClassicRL model.\n",
    "\n",
    "            Remarks: PPO Atari version scales the input frames from [0, 255] to [0, 1]. It's not clear whether Ruo-Ze et al used scaled data or not.\n",
    "\n",
    "        labels = a list of tuples with length (N_samples), with each sample being a tuple composed of (command_type, action1, action2), where:\n",
    "\n",
    "            command_type: a tensor of the action command type index-encoded with indices within range [0, len(command_types)].\n",
    "            action1: the action1 index-encoded with indices within range [0, len(actions1)].\n",
    "            action2: the action2 index-encoded with indices within range [0, len(action2)].\n",
    "\n",
    "            Providing a reward is optional and up to you during the Study Mode. The main focus on Study Mode is to train the Policy.\n",
    "        '''\n",
    "\n",
    "        # HierNet uses data in sequences, but this might be too costly and I don't know how much this would improve performance.\n",
    "        # Let's just stick to normal TD-Learning.\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        encoded_command_type = []\n",
    "        encoded_actions1 = []\n",
    "        encoded_actions2 = []\n",
    "\n",
    "        for sample in commands:\n",
    "\n",
    "            command_type = to_categorical(sample[0], len(self.command_type))\n",
    "            command_type = torch.from_numpy(command_type)\n",
    "            command_type = command_type.unsqueeze(0).to(device) # So you don't have to use [number] for your commands tuple to get a command_type with shape [N_samples, 1]\n",
    "            encoded_command_type.append(command_type)\n",
    "\n",
    "            encoded_action1 = to_categorical(sample[1], len(self.actions1))\n",
    "            encoded_action1 = torch.from_numpy(encoded_action1)\n",
    "            encoded_action1 = encoded_action1.unsqueeze(0).to(device)\n",
    "            encoded_actions1.append(encoded_action1)\n",
    "\n",
    "            encoded_action2 = to_categorical(sample[2], len(self.actions2))\n",
    "            encoded_action2 = torch.from_numpy(encoded_action2)\n",
    "            encoded_action2 = encoded_action2.unsqueeze(0).to(device)\n",
    "            encoded_actions2.append(encoded_action2)\n",
    "\n",
    "        encoded_command_type = torch.cat(encoded_command_type, 0)\n",
    "        encoded_actions1 = torch.cat(encoded_actions1, 0)\n",
    "        encoded_actions2 = torch.cat(encoded_actions2, 0)\n",
    "\n",
    "        self.encoded_command_type = encoded_command_type\n",
    "        self.encoded_actions1 = encoded_actions1\n",
    "        self.encoded_actions2 = encoded_actions2\n",
    "\n",
    "        print(\"All done! Use Hakisa in the Study Mode to properly train her Policy Network(mapping states to certain actions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jigoku Kisetsukan\n",
    "\n",
    "command_types = ['key']\n",
    "\n",
    "actions1 = ['Down', 'Up']\n",
    "\n",
    "actions2 = ['up', 'down', 'left', 'right', 'z', 'x', 'shift']\n",
    "\n",
    "dataset = Dataset(command_types, actions1, actions2, resize=(200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Jigoku(score):\n",
    "    # When using Tesseract for the game Jigoku Kisetsukan: Sense of the Seasons\n",
    "    # Not recommended: Prefer training your own OCR model specifically for this game.\n",
    "\n",
    "    score = score.replace('S', '5').replace('s', '8').replace('e', '2').replace('O', '0').replace('B', '8').replace('o', '4').replace('b', '4')\n",
    "    score = score.replace('I', '1').replace('l', '1').replace('.', '')\n",
    "\n",
    "    try:\n",
    "        score = float(score)\n",
    "\n",
    "    except ValueError:\n",
    "            score = 1.0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bullet Heaven\n",
    "\n",
    "command_types = ['move', 'click', 'rightclick']\n",
    "\n",
    "actions1 = [i for i in range(1, 1919)] # Attention: Discarding (0,0) might cause trouble in tf.to_categorical()\n",
    "\n",
    "actions2 = [i for i in range(1, 1079)] # In this case, use range(0, 1079) and do the appropriate modification in SL function\n",
    "\n",
    "dataset = Dataset(command_types, actions1, actions2, resize=(200,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_BH2(score):\n",
    "    # For the game Bullet Heaven 2\n",
    "    # Also not recommended: Prefer training your own OCR model specifically for this game.\n",
    "\n",
    "    score = score.replace('S', '5').replace('s', '8').replace('e', '2').replace('O', '0').replace('B', '8').replace('o', '0').replace('.', '')\n",
    "    score = sub('[^0-9]', '', score)\n",
    "\n",
    "    try:\n",
    "        score = float(score)\n",
    "\n",
    "    except ValueError:\n",
    "        score = 1.0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you'd like to optimize memory, use the effective X and Y action space, that is, the screen area where the actions really take place.\n",
    "\n",
    "example = plt.imread(\"D:/SerpentAI/datasets/current/bullet_heaven_reduced.png\")\n",
    "plt.imshow(example)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hakisa(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    Hakisa itself, properly optimized to use probability distribution of actions instead of vectors.\n",
    "\n",
    "    In order to avoid pollution, Hakisa will have a single mode.\n",
    "\n",
    "    She will receive as inputs grame frames and the previous cumulative reward and, in the end, will generate 4 outputs:\n",
    "\n",
    "        output 1: a tuple of probability distributions (possible_command_types, possible_actions1, possible_actions2),\n",
    "        each one with sizes (Batch, len(command))\n",
    "\n",
    "        output 2: a tuple of true action, that is, the action selected to be executed (command_type, action1, action2),\n",
    "        each one with sizes (Batch, 1).\n",
    "\n",
    "        output 3: sum of the average of rewards that can be obtained through each action\n",
    "\n",
    "            output3 = avg_reward(command_type) + avg_reward(action1) + avg_reward(action2)\n",
    "\n",
    "            Note that this approach is not mathematically correct and will provide a different result\n",
    "            than it would if the model predicted the reward for every possible option.\n",
    "            The deviation tend to be low when the reward is low, but it gets greater as the reward increases:\n",
    "\n",
    "                27 possible actions, sum of rewards = 344 ---> mean = 12.74\n",
    "                output3 = 12.63\n",
    "                deviation = 0.11\n",
    "\n",
    "                sum of rewards = 34,400,000 ---> mean = 1,274,000\n",
    "                output3 = 1,263,000\n",
    "                deviation = 11,000\n",
    "\n",
    "        output 4: the predicted reward for the true action\n",
    "    '''\n",
    "\n",
    "    def __init__(self, command_types, actions1, actions2, epsilon):\n",
    "\n",
    "        super(Hakisa, self).__init__()\n",
    "\n",
    "        self.command_types = len(command_types)\n",
    "        self.actions1 = len(actions1)\n",
    "        self.actions2 = len(actions2)\n",
    "        self.epsilon = epsilon # Used to determine whether to explore or simply select the best action.\n",
    "        # This method is more used in Q-Learning, but can be used in Actor-Critic as well.\n",
    "\n",
    "        # Sticking to the traditional approach first. We might use Attention Layers if those are indeed effective.\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 100, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(100)\n",
    "        self.conv2 = torch.nn.Conv2d(100, 100, kernel_size=3, stride=1, padding=1, bias=False) # 200x200\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(100)\n",
    "        # Add pool 2x2 ---> 100x100\n",
    "        self.conv3 = torch.nn.Conv2d(100, 200, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(200)\n",
    "        self.conv4 = torch.nn.Conv2d(200, 200, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(200)\n",
    "        # Add pool 2x2 ---> 50x50\n",
    "        self.conv5 = torch.nn.Conv2d(200, 400, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(400)\n",
    "        self.conv6 = torch.nn.Conv2d(400, 400, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm6 = torch.nn.BatchNorm2d(400)\n",
    "        # Add pool 2x2 ---> 25x25\n",
    "        self.conv7 = torch.nn.Conv2d(400, 800, kernel_size=4, stride=1, bias=False) # 22x22\n",
    "        self.batchnorm7 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv8 = torch.nn.Conv2d(800, 800, kernel_size=3, stride=1, bias=False) # 20x20\n",
    "        self.batchnorm8 = torch.nn.BatchNorm2d(800)\n",
    "        # Add pool 2x2 ---> 10x10\n",
    "        self.conv9 = torch.nn.Conv2d(800, 1000, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm9 = torch.nn.BatchNorm2d(1000)\n",
    "        self.conv10 = torch.nn.Conv2d(1000, 1000, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm10 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 5x5\n",
    "\n",
    "        self.neuron_in = torch.nn.Linear(1000*5*5, 100, bias=False) # Bottleneck layer.\n",
    "\n",
    "        if self.command_types > 1:\n",
    "\n",
    "            self.neuron_command_type = torch.nn.Linear(100, self.command_types, bias=False)\n",
    "\n",
    "            # Considering the command type that has been predicted, what should be action1 and action2?\n",
    "\n",
    "            self.neuron_action1 = torch.nn.Linear(100+1, self.actions1, bias=False)\n",
    "            self.neuron_action2 = torch.nn.Linear(100+1, self.actions2, bias=False)\n",
    "\n",
    "        else: # The command type index is always 0\n",
    "\n",
    "            self.neuron_action1 = torch.nn.Linear(100, self.actions1, bias=False)\n",
    "            self.neuron_action2 = torch.nn.Linear(100, self.actions2, bias=False)\n",
    "\n",
    "        self.pred_reward_command_type = torch.nn.Linear(100+1+self.command_types, 1, bias=False)\n",
    "        self.pred_reward_action1 = torch.nn.Linear(100+1+self.actions1, 1, bias=False)\n",
    "        self.pred_reward_action2 = torch.nn.Linear(100+1+self.actions2, 1, bias=False)\n",
    "\n",
    "        self.pool2x2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.LRelu = torch.nn.LeakyReLU(0.25)\n",
    "        self.softmax = torch.nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, input_frame, previous_cumulative_reward):\n",
    "\n",
    "        x = self.conv1(input_frame)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.batchnorm6(x)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = self.batchnorm7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.batchnorm8(x)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv9(x)\n",
    "        x = self.batchnorm9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.batchnorm10(x)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.neuron_in(x) # (Batch, 100). Since we're capturing a single frame at time, our Batch = 1.\n",
    "        # If we were running more game instances in parallel, our Batch would be equal to the number of game instances.\n",
    "\n",
    "        if self.command_types > 1:\n",
    "\n",
    "            possible_command_types = self.neuron_command_type(x) # (Batch, n_command_types)\n",
    "            possible_command_types = self.softmax(possible_command_types)\n",
    "\n",
    "            # Sampling command type to determine the actions\n",
    "            if torch.rand((1,)) < self.epsilon:\n",
    "                one_hot = torch.zeros_like(possible_command_types, device=device)\n",
    "                one_hot[:, torch.multinomial(possible_command_types, 1, replacement=True).item()] = 1.\n",
    "                #idx = torch.randint(0, self.command_types, (possible_command_types.size(0), 1), device=device)\n",
    "                #one_hot = torch.zeros_like(possible_command_types, device=device)\n",
    "                #one_hot[:, idx.item()] = 1.\n",
    "                true_command_type = possible_command_types * one_hot\n",
    "                true_command_type = torch.sum(true_command_type, dim=-1, keepdim=True)\n",
    "            \n",
    "            else: # We can't use .argmax() directly as this detaches the tensor's graphs, since argmax isn't differentiable.\n",
    "\n",
    "                one_hot = torch.zeros_like(possible_command_types, device=device)\n",
    "                one_hot[:, possible_command_types.argmax(-1)] = 1.\n",
    "                true_command_type = possible_command_types * one_hot\n",
    "                true_command_type = torch.sum(true_command_type, dim=-1, keepdim=True)\n",
    "\n",
    "            y = torch.cat((x, true_command_type), -1) # (Batch, 100+1)\n",
    "            possible_actions1 = self.neuron_action1(y)\n",
    "            possible_actions1 = self.softmax(possible_actions1)\n",
    "            possible_actions2 = self.neuron_action2(y)\n",
    "            possible_actions2 = self.softmax(possible_actions2)\n",
    "\n",
    "        else:\n",
    "            possible_command_types = torch.zeros((x.size(0), 1), device=device)\n",
    "            possible_actions1 = self.neuron_action1(x)\n",
    "            possible_actions1 = self.softmax(possible_actions1)\n",
    "            possible_actions2 = self.neuron_action2(x)\n",
    "            possible_actions2 = self.softmax(possible_actions2)\n",
    "\n",
    "        # Calculating possible reward for each command_type, for each action1 and for each action2\n",
    "\n",
    "        expected_reward_ct = torch.zeros_like(possible_command_types, device=device)\n",
    "        expected_reward_a1 = torch.zeros_like(possible_actions1, device=device)\n",
    "        expected_reward_a2 = torch.zeros_like(possible_actions2, device=device)\n",
    "\n",
    "        for batch in range(expected_reward_ct.size(0)):\n",
    "            for action in range(self.command_types):\n",
    "\n",
    "                y = torch.cat((x[batch], previous_cumulative_reward[batch], possible_command_types[batch]), -1) # (100+1+n_commands)\n",
    "                y = self.pred_reward_command_type(y.unsqueeze(0)) # (1, 1)\n",
    "                expected_reward_ct[batch, action] = y\n",
    "\n",
    "                del y\n",
    "        \n",
    "        for batch in range(expected_reward_a1.size(0)):\n",
    "            for action in range(self.actions1):\n",
    "\n",
    "                y = torch.cat((x[batch], previous_cumulative_reward[batch], possible_actions1[batch]), -1)\n",
    "                y = self.pred_reward_action1(y.unsqueeze(0))\n",
    "                expected_reward_a1[batch, action] = y\n",
    "                \n",
    "                del y\n",
    "        \n",
    "        for batch in range(expected_reward_a2.size(0)):\n",
    "            for action in range(self.actions2):\n",
    "\n",
    "                y = torch.cat((x[batch], previous_cumulative_reward[batch], possible_actions2[batch]), -1)\n",
    "                y = self.pred_reward_action2(y.unsqueeze(0))\n",
    "                expected_reward_a2[batch, action] = y\n",
    "                \n",
    "                del y\n",
    "\n",
    "        predicted_reward_ct = one_hot * expected_reward_ct # One-hot has the index of the chosen command_type\n",
    "        predicted_reward_ct = torch.sum(predicted_reward_ct, dim=-1, keepdim=True)\n",
    "\n",
    "        # Now, sampling the actions1 and actions2\n",
    "\n",
    "        if torch.rand((1,)) < self.epsilon:\n",
    "            one_hot = torch.zeros_like(possible_actions1, device=device)\n",
    "            one_hot[:, torch.multinomial(possible_actions1, 1, replacement=True).item()] = 1.\n",
    "            #idx = torch.randint(0, self.actions1, (possible_actions1.size(0), 1), device=device)\n",
    "            #one_hot = torch.zeros_like(possible_actions1, device=device)\n",
    "            #one_hot[:, idx.item()] = 1.\n",
    "            true_action1 = possible_actions1 * one_hot\n",
    "            true_action1 = torch.sum(true_action1, dim=-1, keepdim=True)\n",
    "            \n",
    "        else:\n",
    "            one_hot = torch.zeros_like(possible_actions1, device=device)\n",
    "            one_hot[:, possible_actions1.argmax(-1)] = 1.\n",
    "            true_action1 = possible_actions1 * one_hot\n",
    "            true_action1 = torch.sum(true_action1, dim=-1, keepdim=True)\n",
    "\n",
    "        predicted_reward_a1 = one_hot * expected_reward_a1\n",
    "        predicted_reward_a1 = torch.sum(predicted_reward_a1, dim=-1, keepdim=True)\n",
    "\n",
    "        if torch.rand((1,)) < self.epsilon:\n",
    "            one_hot = torch.zeros_like(possible_actions2, device=device)\n",
    "            one_hot[:, torch.multinomial(possible_actions2, 1, replacement=True).item()] = 1.\n",
    "            #idx = torch.randint(0, self.actions2, (possible_actions2.size(0), 1), device=device)\n",
    "            #one_hot = torch.zeros_like(possible_actions2, device=device)\n",
    "            #one_hot[:, idx.item()] = 1.\n",
    "            true_action2 = possible_actions2 * one_hot\n",
    "            true_action2 = torch.sum(true_action2, dim=-1, keepdim=True)\n",
    "            \n",
    "        else:\n",
    "            one_hot = torch.zeros_like(possible_actions2, device=device)\n",
    "            one_hot[:, possible_actions2.argmax(-1)] = 1.\n",
    "            true_action2 = possible_actions2 * one_hot\n",
    "            true_action2 = torch.sum(true_action2, dim=-1, keepdim=True)\n",
    "\n",
    "        one_hot = torch.zeros_like(possible_actions2, device=device)\n",
    "        one_hot[:, possible_actions2.argmax(-1)] = 1\n",
    "        predicted_reward_a2 = one_hot * expected_reward_a2\n",
    "        predicted_reward_a2 = torch.sum(predicted_reward_a2, dim=-1, keepdim=True)\n",
    "\n",
    "        predicted_reward = predicted_reward_ct + predicted_reward_a1 + predicted_reward_a2\n",
    "\n",
    "        del expected_reward_ct, expected_reward_a1, expected_reward_a2 # Actually not used in PPO\n",
    "\n",
    "        possible_actions = (possible_command_types, possible_actions1, possible_actions2)\n",
    "\n",
    "        true_action = (true_command_type, true_action1, true_action2)\n",
    "\n",
    "        return possible_actions, true_action, predicted_reward\n",
    "\n",
    "    def execute_command(self, command):\n",
    "        '''\n",
    "        Command must be a tuple(command_type, action1, action2), where:\n",
    "\n",
    "            command_type: key(keyboard) or move, rightclick, click(mouse)\n",
    "            action1: Up, Down, press(keyboard), X coordinate(mouse) or None(no mouse movement)\n",
    "            action2: 'a', 'z', 'shift'...(keyboard), Y coordinate(mouse) or None(no mouse movement)\n",
    "\n",
    "        Make sure all key actions(action2) are lowered.\n",
    "\n",
    "        Have in mind that Hakisa might output command_type 'key' and action1 that is equivalent to a mouse action.\n",
    "        In this case, the command is ignored.\n",
    "        '''\n",
    "\n",
    "        if \"key\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                \n",
    "                if \"Up\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        #pyautogui.keyUp(command[2])\n",
    "                        keyboard.release(command[2])\n",
    "                \n",
    "                    except:\n",
    "                        pass # If Hakisa predicts a mouse action for a keyboard command, she won't do anything.\n",
    "\n",
    "                elif \"Down\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        #pyautogui.keyDown(command[2])\n",
    "                        keyboard.press(command[2])\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif \"press\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        keyboard.send(command[2]) # Some games won't work with pyautogui.press(), so use keyboard module, since we'll import it for Play Mode.\n",
    "                    \n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            except:\n",
    "\n",
    "                pass # If Hakisa predicts a keyboard command, but outputs a mouse action, she won't do anything.\n",
    "\n",
    "        elif \"move\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                #pyautogui.moveTo(command[1], command[2], duration=0.19) # Duration = 0.19 seconds to be more realistic\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "\n",
    "            except:\n",
    "                pass # If Hakisa predict a mouse command, but outputs a keyboard action, she won't do anything.\n",
    "\n",
    "        elif \"rightclick\" in command[0]:\n",
    "            \n",
    "            try:\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "                mouse.right_click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif \"click\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                #pyautogui.moveTo(command[1], command[2], duration=0.19)\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "                mouse.click() # Same case as press. Use mouse module.\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError # It was probably you who made a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hakisa = Hakisa(command_types, actions1, actions2, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Initializing Hakisa weights: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/ - Item 2\n",
    "\n",
    "Ruo-Ze Liu didn't use this. My experiments indicate that this may actually sabotage the model rather\n",
    "than helping it. It makes the model more prone to vanishing gradients. In SL, the loss gets more resilient.\n",
    "\n",
    "'''\n",
    "torch.nn.init.orthogonal_(hakisa.neuron_in.weight, np.sqrt([2]).item())\n",
    "torch.nn.init.orthogonal_(hakisa.neuron_command_type.weight, 0.01)\n",
    "torch.nn.init.orthogonal_(hakisa.neuron_action1.weight, 0.01)\n",
    "torch.nn.init.orthogonal_(hakisa.neuron_action2.weight, 0.01)\n",
    "torch.nn.init.orthogonal_(hakisa.pred_reward_command_type.weight, 1)\n",
    "torch.nn.init.orthogonal_(hakisa.pred_reward_action1.weight, 1)\n",
    "torch.nn.init.orthogonal_(hakisa.pred_reward_action2.weight, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for Study Phase\n",
    "# To label your data, consider Supervised Learning + Self-Learning.\n",
    "# For really big datasets, consider using Google Colabs.\n",
    "# Its RAM supports around 42,000 200x200 images (personal experience with Cocogoat dataset)\n",
    "\n",
    "import os\n",
    "\n",
    "images_by_order = []\n",
    "\n",
    "for directory, _, files in os.walk(\"D:/Python/Projects/Hakisa/Hakisa/BH_gameplay\"):\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        file = file.split('.')\n",
    "        file = file[0] # Getting exclusively the number\n",
    "\n",
    "        images_by_order.append(file)\n",
    "\n",
    "images_by_order = sorted([int(x) for x in images_by_order])\n",
    "\n",
    "# Problem: for strings, Python considers that 1000 < 2. Maybe something related to how the string is assembled?\n",
    "\n",
    "images_data = []\n",
    "\n",
    "for i in images_by_order[0:10]: # 10 samples for testing\n",
    "\n",
    "    i = directory + '/' + str(i) + '.png'\n",
    "    image = Image.open(i)\n",
    "    image = image.resize((200, 200))\n",
    "    array = np.array(image, dtype=np.float32)\n",
    "    image.close()\n",
    "    array = array/255 # Note that the data must be within [0, 1] for matplotlib.\n",
    "    images_data.append(array)\n",
    "\n",
    "images_data = np.stack(images_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_data = torch.from_numpy(images_data)\n",
    "images_data = images_data.view(images_data.size(0), images_data.size(3), images_data.size(1), images_data.size(2))\n",
    "print(images_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:/Python/Projects/Hakisa/Preprocessing/commands_05000.pkl\", 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "labels.insert(2, (0, 860, 550))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.create_data(data=images_data[:5000], commands=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study Phase, or Supervised Learning Phase. What matters here is making Hakisa correlate states to commands\n",
    "\n",
    "'''\n",
    "\"In SL training, we found a learning rate of 1e-4 and 10 training epochs achieve the\n",
    "best result. The best model achieves a 0.15 win rate against the level-1 built-in AI. Note\n",
    "that though this result is not as good as that we acquire in the HRL method, the training\n",
    "here faces 564 actions, thus is much difficult. The 1e-4 learning rate is also selected by\n",
    "experiments and is different from the default 1e-3 in the AlphaStar pseudocodes. We find\n",
    "that training more than 10 epochs will easily fall in overfitting, making the agent can't do\n",
    "any meaningful things.\" - Liu, Ruo-Ze et al. On Efficient Reinforcement Learning for Full-length Game of StarCraft II\n",
    "'''\n",
    "\n",
    "# Since our batch size is 1, beware of vanishing gradients and overfitting.\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Using a lr lower than 1e-4 mitigates vanishing gradients. Trick used in Generative Models.\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1e-6, eps=1e-8)\n",
    "# TO CONSIDER: Using weight decay to prevent overfitting.\n",
    "\n",
    "losses = []\n",
    "action_grads = []\n",
    "policy_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "dummy_reward = torch.zeros((1, 1), device=device) # Don't worry about the reward. It won't be used during the Study Phase.\n",
    "\n",
    "grad_clip = None\n",
    "save_path = None\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    for i, (frame, encoded_command_type, encoded_action1, encoded_action2) in enumerate(dataloader):\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        frame = frame.to(device)\n",
    "        encoded_command_type = encoded_command_type.to(device)\n",
    "        encoded_action1 = encoded_action1[:, 1:].to(device) # Removing index 0 ---> Coordinate 0,0 in mouse\n",
    "        encoded_action2 = encoded_action2[:, 1:].to(device) # Removing index 0 ---> Coordinate 0,0 in mouse\n",
    "\n",
    "        possible_actions, true_action, predicted_reward = hakisa(frame, dummy_reward)\n",
    "\n",
    "        # Deleting Agent variables. Remember that our goal is to pretrain the Policy(\"Vectorizer\")\n",
    "        del true_action, predicted_reward\n",
    "        \n",
    "        command_type_loss = policy_loss(possible_actions[0], encoded_command_type)\n",
    "        action1_loss = policy_loss(possible_actions[1], encoded_action1)\n",
    "        action2_loss = policy_loss(possible_actions[2], encoded_action2)\n",
    "\n",
    "        total_loss = command_type_loss + action1_loss + action2_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "        action_grads.append(torch.mean(hakisa.neuron_in.weight.grad))\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        #if i % 100 == 0: # On GTX 1650 Ti, 5000 iterations(1 epoch) = +- 66 minutes.\n",
    "\n",
    "    print(f\"{epoch}/{EPOCHS}\\nCurrent Loss: {total_loss.item()}\\tTotal Epoch Loss: {epoch_loss/(i+1)}\\tGradients Average: {action_grads[-1]}\")\n",
    "    print(f\"Command Type Loss: {command_type_loss.item()}\\nAction 1 Loss: {action1_loss.item()}\\nAction 2 Loss: {action2_loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to use continuous rewards, it might be necessary to use a Reward Model.\n",
    "\n",
    "In this case, you'll have to recreate the Reward Model here.\n",
    "\n",
    "**Remember that Neural Networks can be seen as functions with learning parameters. Thus, they can make good reward functions.**\n",
    "\n",
    "Careful with overfitting and memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, kernel_size, strides=1, padding=1):\n",
    "\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.convA = torch.nn.Conv2d(input_channels, input_channels, kernel_size, strides, padding, bias=False)\n",
    "        self.batchnormA = torch.nn.BatchNorm2d(input_channels)\n",
    "        self.convB = torch.nn.Conv2d(input_channels, input_channels, kernel_size, strides, padding, bias=False)\n",
    "        self.batchnormB = torch.nn.BatchNorm2d(input_channels)\n",
    "\n",
    "        self.PRelu = torch.nn.PReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.convA(input)\n",
    "        x = self.batchnormA(x)\n",
    "        x = self.PRelu(x)\n",
    "        x = self.convB(x)\n",
    "        x = self.batchnormB(x)\n",
    "\n",
    "        output = input + x\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PapezCircuit(torch.nn.Module):\n",
    "    '''\n",
    "    Reward model, based on ResNet architecture\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(PapezCircuit, self).__init__()\n",
    "\n",
    "        self.conv_in = torch.nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "        \n",
    "        self.resblock1 = ResidualBlock(64, 3, 1, 1)\n",
    "        self.resblock2 = ResidualBlock(64, 3, 1, 1)\n",
    "        self.resblock3 = ResidualBlock(64, 3, 1, 1)\n",
    "        self.conv4 = torch.nn.Conv2d(64, 128, 4, 2, 1, bias=False)\n",
    "        self.conv5 = torch.nn.Conv2d(128, 128, 3, 1, 1, bias=False)\n",
    "        self.resblock6 = ResidualBlock(128, 3, 1, 1)\n",
    "        self.resblock7 = ResidualBlock(128, 3, 1, 1)\n",
    "        self.resblock8 = ResidualBlock(128, 3, 1, 1)\n",
    "        self.conv9 = torch.nn.Conv2d(128, 256, 4, 2, 1, bias=False)\n",
    "        self.conv10 = torch.nn.Conv2d(256, 256, 3, 1, 1, bias=False)\n",
    "        self.resblock11 = ResidualBlock(256, 3, 1, 1)\n",
    "        self.resblock12 = ResidualBlock(256, 3, 1, 1)\n",
    "        self.resblock13 = ResidualBlock(256, 3, 1, 1)\n",
    "        self.conv14 = torch.nn.Conv2d(256, 512, 4, 2, 1, bias=False)\n",
    "        self.conv15 = torch.nn.Conv2d(512, 512, 3, 1, 1, bias=False)\n",
    "        self.resblock16 = ResidualBlock(512, 3, 1, 1)\n",
    "        self.resblock17 = ResidualBlock(512, 3, 1, 1)\n",
    "        self.resblock18 = ResidualBlock(512, 3, 1, 1)\n",
    "\n",
    "        self.neuron_out = torch.nn.Linear(18432, 1, bias=False)\n",
    "\n",
    "        self.pool = torch.nn.AvgPool2d(2, 2)\n",
    "        self.dropout = torch.nn.Dropout(0.35)\n",
    "\n",
    "        self.LRelu = torch.nn.LeakyReLU(0.25)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.conv_in(input)\n",
    "        x = self.LRelu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.LRelu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.resblock6(x)\n",
    "        x = self.resblock7(x)\n",
    "        x = self.resblock8(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.LRelu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.resblock11(x)\n",
    "        x = self.resblock12(x)\n",
    "        x = self.resblock13(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv14(x)\n",
    "        x = self.conv15(x)\n",
    "        x = self.LRelu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.resblock16(x)\n",
    "        x = self.resblock17(x)\n",
    "        x = self.resblock18(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        output = self.neuron_out(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sensei(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, save_pathA, save_pathB, save_pathC):\n",
    "\n",
    "        super(Sensei, self).__init__()\n",
    "\n",
    "        # Different metrics can require different reward models architectures.\n",
    "        # Don't be afraid to use simpler and shallow models.\n",
    "\n",
    "        self.score_model = PapezCircuit()\n",
    "        self.power_model = PapezCircuit()\n",
    "        self.life_model = PapezCircuit()\n",
    "\n",
    "        self.score_model.load_state_dict(torch.load(save_pathA))\n",
    "        self.power_model.load_state_dict(torch.load(save_pathB))\n",
    "        self.life_model.load_state_dict(torch.load(save_pathC))\n",
    "\n",
    "    def forward(self, score, power, life):\n",
    "\n",
    "        score_reward = self.score_model(score)\n",
    "        power_reward = self.power_model(power)\n",
    "        life_reward = self.life_model(life)\n",
    "\n",
    "        return score_reward, power_reward, life_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensei = Sensei(\n",
    "    save_pathA=\"D:/Python/Projects/Hakisa/Hakisa/Sensei_JK_Score.tar\",\n",
    "    save_pathB=\"D:/Python/Projects/Hakisa/Hakisa/Sensei_JK_Power.tar\",\n",
    "    save_pathC=\"D:/Python/Projects/Hakisa/Hakisa/Sensei_JK_Life.tar\"\n",
    ").to(device).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score_region = (1640, 790, 1640+280, 790+290)\n",
    "\n",
    "power_region = (1620, 0, 1620+200, 30)\n",
    "\n",
    "life_region = (1115, 915, 1115+210, 915+130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_regions():\n",
    "    # ATTENTION: Remember that scaling is crucial\n",
    "    # PIL.Image only works with uint8 arrays(integers, 0 to 255).\n",
    "    # Matplotlib, for floats(type used in the models), considers 0 to 1.\n",
    "        \n",
    "    with mss() as sct:\n",
    "\n",
    "            score = sct.grab(monitor={\"top\": 0, \"left\": 1620, \"width\": 200, \"height\": 30})\n",
    "            score = Image.frombytes(\"RGB\", score.size, score.bgra, 'raw', 'BGRX')\n",
    "            score = np.array(score, dtype=np.float32)\n",
    "            score = score/255\n",
    "            score = torch.from_numpy(score).unsqueeze(0)\n",
    "            score = score.view(score.size(0), score.size(3), score.size(1), score.size(2))\n",
    "\n",
    "            power = sct.grab(monitor={\"top\": 790, \"left\": 1640, \"width\": 280, \"height\": 290})\n",
    "            power = Image.frombytes(\"RGB\", power.size, power.bgra, 'raw', 'BGRX')\n",
    "            power = np.array(power, dtype=np.float32)\n",
    "            power = power/255\n",
    "            power = torch.from_numpy(power).unsqueeze(0)\n",
    "            power = power.view(power.size(0), power.size(3), power.size(1), power.size(2))\n",
    "\n",
    "            life = sct.grab(monitor={\"top\": 915, \"left\": 1115, \"width\": 210, \"height\": 130})\n",
    "            life = Image.frombytes(\"RGB\", life.size, life.bgra, 'raw', 'BGRX')\n",
    "            life = np.array(life, dtype=np.float32)\n",
    "            life = life/255\n",
    "            life = torch.from_numpy(life).unsqueeze(0)\n",
    "            life = life.view(life.size(0), life.size(3), life.size(1), life.size(2))\n",
    "        \n",
    "    return score, power, life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playthrough Phase, or Reinforcement Learning Phase, where the magic really happens.\n",
    "'''\n",
    "\"The Supervised Learning trained model has a natural “domain shift” to the RL environment.\n",
    "The model of high Supervised Learning accuracy may not behave well in the RL domain\"\n",
    "'''\n",
    "\n",
    "reward = torch.zeros((1, 1), device=device) # Cumulative reward\n",
    "advantage = []\n",
    "steps = 0\n",
    "save_point = 10 # Also optimization point\n",
    "action_grads = []\n",
    "reward_grads = []\n",
    "previous_predicted_reward = None # BEWARE: this will be the basis for backpropagation (TD-Learning)\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1e-4, eps=1e-8)\n",
    "# 1e-4 is a common LR. But 1e-6 is also used by RainbowDQN and it's the best one for HierNet. Change it as needed.\n",
    "# Note: PPO also uses epsilon = 1e-5, but it seems that eps=1e-7 or 1e-8 are better.\n",
    "\n",
    "old_policy = copy.deepcopy(hakisa) # For Surrogate Loss. Creating here to reserve some space in memory\n",
    "old_policy.eval()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.1) # The learning rate should decay linearly until it vanishes\n",
    "\n",
    "value_criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO Implementation based on\n",
    "https://github.com/liuruoze/HierNet-SC2/blob/main/algo/ppo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)\n",
    "\n",
    "\n",
    "while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "    frame = frame/255 # Scaling data, since we did so in the Supervised Learning\n",
    "\n",
    "    reward_input = reward.clone() # To avoid issues with inplace operations(optimizer)\n",
    "\n",
    "    if previous_predicted_reward is None:\n",
    "\n",
    "        _, _, previous_predicted_reward = hakisa(frame, reward_input)\n",
    "\n",
    "    possible_actions, true_action, predicted_reward = hakisa(frame, reward_input)\n",
    "\n",
    "    # For Surrogate Loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        previous_possible_actions, _, _ = old_policy(frame, reward_input)\n",
    "\n",
    "    true_command_type = (possible_actions[0] == true_action[0]).nonzero(as_tuple=True)[1][0].item()\n",
    "    true_action1 = (possible_actions[1] == true_action[1]).nonzero(as_tuple=True)[1][0].item()\n",
    "    true_action2 = (possible_actions[2] == true_action[2]).nonzero(as_tuple=True)[1][0].item()\n",
    "\n",
    "    command = dataset.get_command(true_command_type, true_action1, true_action2)\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    #score = reward_BH2() # Change this function as you'd like.\n",
    "\n",
    "    score, power, life = capture_regions()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        score_reward, power_reward, life_reward = sensei(score.to(device), power.to(device), life.to(device))\n",
    "\n",
    "    scoring = score_reward + (power_reward * 1) + (life_reward * 2.0)\n",
    "\n",
    "    reward += scoring\n",
    "\n",
    "    delta = scoring + (0.9995 * predicted_reward.item()) - previous_predicted_reward.item()\n",
    "    advantage.append(delta)\n",
    "\n",
    "    for t in reversed(range(len(advantage) - 1)):\n",
    "\n",
    "        advantage[t] = advantage[t] + 0.9995 * 0.9995 * advantage[t+1]\n",
    "\n",
    "    one_hot = torch.zeros_like(possible_actions[0], device=device)\n",
    "    one_hot[0, possible_actions[0].argmax(-1)] = 1.\n",
    "    possible_command_type = possible_actions[0] * one_hot\n",
    "    possible_command_type = torch.sum(possible_command_type, dim=-1)\n",
    "\n",
    "    one_hot = torch.zeros_like(previous_possible_actions[0], device=device)\n",
    "    one_hot[0, previous_possible_actions[0].argmax(-1)] = 1.\n",
    "    previous_possible_command_type = previous_possible_actions[0] * one_hot\n",
    "    previous_possible_command_type = torch.sum(previous_possible_command_type, dim=-1)\n",
    "\n",
    "    one_hot = torch.zeros_like(possible_actions[1], device=device)\n",
    "    one_hot[0, possible_actions[1].argmax(-1)] = 1.\n",
    "    possible_action1 = possible_actions[1] * one_hot\n",
    "    possible_action1 = torch.sum(possible_action1, dim=-1)\n",
    "\n",
    "    one_hot = torch.zeros_like(previous_possible_actions[1], device=device)\n",
    "    one_hot[0, previous_possible_actions[1].argmax(-1)] = 1.\n",
    "    previous_possible_action1 = previous_possible_actions[1] * one_hot\n",
    "    previous_possible_action1 = torch.sum(previous_possible_action1, dim=-1)\n",
    "\n",
    "    one_hot = torch.zeros_like(possible_actions[2], device=device)\n",
    "    one_hot[0, possible_actions[2].argmax(-1)] = 1.\n",
    "    possible_action2 = possible_actions[2] * one_hot\n",
    "    possible_action2 = torch.sum(possible_action2, dim=-1)\n",
    "\n",
    "    one_hot = torch.zeros_like(previous_possible_actions[2], device=device)\n",
    "    one_hot[0, previous_possible_actions[2].argmax(-1)] = 1.\n",
    "    previous_possible_action2 = previous_possible_actions[2] * one_hot\n",
    "    previous_possible_action2 = torch.sum(previous_possible_action1, dim=-1)\n",
    "\n",
    "    # Since we're dealing with a probability distribution, using exp(log) is more mathmatically correct(and stable)\n",
    "    # In practice, we're doing a KL-Divergence. To make it less computationally expensive, applying clip.\n",
    "\n",
    "    possible_command_type = torch.clamp(possible_command_type, 1e-10, 1.0)\n",
    "    previous_possible_command_type = torch.clamp(previous_possible_command_type, 1e-10, 1.0)\n",
    "    possible_possible_action1 = torch.clamp(possible_action1, 1e-10, 1.0)\n",
    "    previous_possible_action1 = torch.clamp(previous_possible_action1, 1e-10, 1.0)\n",
    "    possible_possible_action2 = torch.clamp(possible_action2, 1e-10, 1.0)\n",
    "    previous_possible_action2 = torch.clamp(previous_possible_action2, 1e-10, 1.0)\n",
    "\n",
    "    ratio_command_type = torch.exp(torch.log(possible_command_type) - torch.log(previous_possible_command_type))\n",
    "    ratio_action1 = torch.exp(torch.log(possible_action1) - torch.log(previous_possible_action1))\n",
    "    ratio_action2 = torch.exp(torch.log(possible_action2) - torch.log(previous_possible_action2))\n",
    "\n",
    "    clipped_ratio_command_type = torch.clamp(ratio_command_type, min=0.8, max=1.2)\n",
    "    clipped_ratio_action1 = torch.clamp(ratio_action1, min=0.8, max=1.2)\n",
    "    clipped_ratio_action2 = torch.clamp(ratio_action2, min=0.8, max=1.2)\n",
    "\n",
    "    surrogate_loss_command_type = -torch.mean(torch.minimum(torch.mul(ratio_command_type, advantage[-1]), torch.mul(clipped_ratio_command_type, advantage[-1])))\n",
    "    surrogate_loss_action1 = -torch.mean(torch.minimum(torch.mul(ratio_action1, advantage[-1]), torch.mul(clipped_ratio_action1, advantage[-1])))\n",
    "    surrogate_loss_action2 = -torch.mean(torch.minimum(torch.mul(ratio_action2, advantage[-1]), torch.mul(clipped_ratio_action2, advantage[-1])))\n",
    "\n",
    "    total_surrogate_loss = surrogate_loss_command_type + surrogate_loss_action1 + surrogate_loss_action2\n",
    "\n",
    "    # We can use both the actual cumulative (discounted) reward, or the predicted reward for target. Following Ruo-Ze Liu's code.\n",
    "    value_loss = value_criterion(previous_predicted_reward, (scoring + 0.9995 * predicted_reward.detach()))\n",
    "\n",
    "    total_loss = total_surrogate_loss + (value_loss * 0.5)\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Performing a single iteration in order to get previous predicted reward for backpropagation\n",
    "\n",
    "    _, _, previous_predicted_reward = hakisa(frame, reward_input)\n",
    "\n",
    "    del frame, _\n",
    "\n",
    "    try:\n",
    "        action_grads.append(torch.mean(hakisa.neuron_in.weight.grad))\n",
    "        reward_grads.append(torch.mean(hakisa.pred_reward_command_type.weight.grad))\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        old_policy = copy.deepcopy(hakisa)\n",
    "        old_policy.eval()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Current Loss: {total_loss.item()}\")\n",
    "        print(f\"Surrogate Loss: {total_surrogate_loss.item()}\\tValue Loss: {value_loss.item()}\\tAdvantage: {advantage[-1].item()}\")\n",
    "        print(f\"Command Type loss: {surrogate_loss_command_type.item()}\\tRatio: {ratio_command_type.item()}\")\n",
    "        print(f\"Action1 loss: {surrogate_loss_action1.item()}\\tRatio: {ratio_action1.item()}\")\n",
    "        print(f\"Action2 loss: {surrogate_loss_action2.item()}\\tRatio: {ratio_action2.item()}\")\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward.item()}\")\n",
    "        print(f\"Score Reward: {score_reward.item()}\\tPower: {power_reward.item()}\\tLife: {life_reward.item()}\")\n",
    "        print(command)\n",
    "\n",
    "        # Avoid saving your model during gameplay, \n",
    "        # as this process takes too much time (some seconds).\n",
    "\n",
    "        '''torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': hakisa.state_dict(),\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")'''\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)\n",
    "\n",
    "        # Performing a single iteration in order to get previous predicted reward for backpropagation\n",
    "\n",
    "        frame = dataset._grab_frame()\n",
    "        frame = frame/255 # Scaling data, since we did so in the Supervised Learning\n",
    "\n",
    "        reward_input = reward.clone()\n",
    "\n",
    "        _, _, previous_predicted_reward = hakisa(frame, reward_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Generalist Advantage Estimative(GAE)\n",
    "\n",
    "plt.plot(advantage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing how the grads behave\n",
    "\n",
    "_, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].plot(action_grads[:100])\n",
    "ax[1].plot(reward_grads[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Hakisa params\n",
    "\n",
    "torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': hakisa.state_dict(),\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, \"D:/Python/Projects/Hakisa/Hakisa/Hakisa_checkpoint.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Hakisa params\n",
    "\n",
    "params = torch.load(\"D:/Python/Projects/Hakisa/Hakisa/Hakisa_checkpoint.tar\")\n",
    "\n",
    "steps = params['Steps']\n",
    "hakisa.load_state_dict(params['Hakisa_params'])\n",
    "print(f\"Last LR: {params['Hakisa_LR']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

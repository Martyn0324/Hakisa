{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "#import pyautogui\n",
    "import keyboard\n",
    "import mouse\n",
    "import pytesseract\n",
    "from time import sleep\n",
    "import winsound\n",
    "from re import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Generates commands for Hakisa. Based on NLP/Classic RL approach.\n",
    "\n",
    "        Commands = list of all possible commands(command type + action1 + action2).\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        commands=None,\n",
    "        top=0,\n",
    "        left=0,\n",
    "        width=1920,\n",
    "        height=1080,\n",
    "        resize=None\n",
    "    ):\n",
    "\n",
    "        # Window resolutions for the screen grabber\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.resize = resize # For reducing the images. Must be a tuple (Height, Width)\n",
    "\n",
    "        self.commands = commands\n",
    "\n",
    "\n",
    "    # Pytorch's Dataset functions will only be used in Studying mode\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        inputs = self.data[idx]\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def _grab_frame(self):\n",
    "        # Unfortunately, this whole operation takes about 0.6 seconds, so we'll probably have to deal with a single frame each 1~3 seconds.\n",
    "        with mss() as sct:\n",
    "            frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "            frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if self.resize:\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame = np.array(frame, dtype=np.float32)\n",
    "\n",
    "            frame = torch.from_numpy(frame)\n",
    "        \n",
    "        frame = frame.view(1, frame.size(2), frame.size(0), frame.size(1)).to(device) # (Batch, Channels, Height, Width)\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def get_command(self, idx):\n",
    "        '''\n",
    "        Model output = LogSoftmax Output\n",
    "        '''\n",
    "        command = self.commands[idx]\n",
    "\n",
    "        return command\n",
    "\n",
    "    def get_consequences(self, top, left, width, height, togray=False, threshold=False, thresh_gauss=171, thresh_C=13, tesseract_config='--psm 8'):\n",
    "        '''\n",
    "        Used after Hakisa performed an input, in order to get its consequences(ex: score change, bombs, kills, deaths...).\n",
    "        Returns a string according to Tesseract's OCR.\n",
    "        '''\n",
    "\n",
    "        with mss() as sct:\n",
    "            consequence = sct.grab(monitor={\"top\": top, \"left\": left, \"width\": width, \"height\": height})\n",
    "\n",
    "            consequence = Image.frombytes(\"RGB\", consequence.size, consequence.bgra, 'raw', 'BGRX')\n",
    "\n",
    "        if togray is True:\n",
    "\n",
    "            consequence = consequence.convert(\"P\") # Sometimes, simply converting to grayscale is enough\n",
    "\n",
    "            if threshold is True:\n",
    "                if \"ADAPTIVE_THRESH_GAUSSIAN_C\" and \"adaptiveThreshold\" and \"THRESH_BINARY\" not in dir():\n",
    "                    from cv2 import adaptiveThreshold, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY\n",
    "\n",
    "                consequence = adaptiveThreshold(np.array(consequence),255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,thresh_gauss,thresh_C)\n",
    "                consequence = Image.fromarray(consequence)\n",
    "        \n",
    "        consequence = pytesseract.image_to_string(consequence, config=tesseract_config) \n",
    "\n",
    "        # OCR adds some strange characters(even with the whitelist function). Let's remove them.\n",
    "\n",
    "        consequence = sub('[^A-Za-z0-9/.]', '', consequence) # Attention: 0, 1 and 8 can be seen as O, l and B.\n",
    "\n",
    "        return consequence\n",
    "\n",
    "    def record_gameplay(self, number_of_screenshots, screenshot_delay, grayscale=False, resize=False, path=None):\n",
    "\n",
    "        # Resizing and grayscaling isn't really necessary here, but can save you some time later.\n",
    "        # Both saving you from writing more code and from making your hardware having to process more and more data at once.\n",
    "\n",
    "        print(f\"Ok. Screenshot capture will begin in 5 seconds\")\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "        for i in range(number_of_screenshots):\n",
    "\n",
    "            with mss() as sct:\n",
    "\n",
    "                frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "                frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if grayscale:\n",
    "\n",
    "                frame = frame.convert('L')\n",
    "\n",
    "            if resize:\n",
    "\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame.save(f\"{path}/{i+2000}.png\")\n",
    "\n",
    "            sleep(screenshot_delay)\n",
    "        \n",
    "        print(\"Screenshot capture finished!\")\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = [\n",
    "    'down_up', 'down_down', 'down_left', 'down_right', 'down_z', 'down_shift',\n",
    "    'up_up', 'up_down', 'up_left', 'up_right', 'up_z', 'up_shift', 'press_x'\n",
    "]\n",
    "\n",
    "dataset = Dataset(commands, resize=(200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down_up', 'down_down', 'down_left', 'down_right', 'down_z', 'down_shift', 'up_up', 'up_down', 'up_left', 'up_right', 'up_z', 'up_shift', 'press_x']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Jigoku(score):\n",
    "    # For the game Jigoku Kisetsukan: Sense of the Seasons\n",
    "\n",
    "    score = score.replace('S', '5').replace('s', '8').replace('e', '2').replace('O', '0').replace('B', '8').replace('o', '4').replace('b', '4')\n",
    "    score = score.replace('I', '1').replace('l', '1').replace('.', '')\n",
    "\n",
    "    try:\n",
    "        score = float(score)\n",
    "\n",
    "    except ValueError:\n",
    "            score = 1.0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Approach for Reinforcement Learning using Policy-based method ---> Actor-Critic.\n",
    "\n",
    "There's 2 traditional approaches for RL: Off-Policy and On-Policy. Off-Policy is normally about Q-Learning algorithms, such as RainbowDQN.\n",
    "Those, however, are too costly, as those requires memory so it can map states --> values(reward) and make a backpropagation...or even not use neural networks at all.\n",
    "\n",
    "On-Policy are more used in uncertain environments and is based on Temporal-Difference Learning, not relying on memory.\n",
    "On-Policy algorithms can be 2 algorithms: the policy algorithm, which tries to map a given state to possible actions; and the agent algorithm, which executes\n",
    "an action and predicts an expected reward.\n",
    "\n",
    "The Policy algorithm can be a Neural Network. In this case, it's quite similar to NLP, receiving an input and trying to extract a context(or actions) related\n",
    "to that input. We could think of if as a Vectorizer, making the work analogue to an Embedding Matrix:\n",
    "\n",
    "    Sequence of words(integers) -----------> Embedding Layer ------------> Context(Words vectors, floats)\n",
    "\n",
    "    Game frame(RGB or grayscale image, float) ------> Vectorizer --------> Context(Actions vectors, floats)\n",
    "\n",
    "    Such comparison is more explicit in Liu Ruo-Ze et al.'s HierNet, where Embedding, Transformers and LSTMs have been used.\n",
    "\n",
    "The Agent algorithm can also be a Neural Network, executing a simple, more mundane Regression task and trying to predict the reward to be obtained.\n",
    "\n",
    "\n",
    "Instead of using 2 neural networks, however, we'll be using a single one, combining the policy and the agent ---> Actor-Critic.\n",
    "This method is more similar to the human brain(and cerebellum?), where the Broadmann's Areas 17, 18, 19 receives the game frames, extracts features and\n",
    "send them to the Area 9, where the possible actions are calculated, with the expected consequence for each action, and the true action is decided by Areas 11, 12.\n",
    "If our action generates the consequence we were expecting(usually a good one), the limbic system awards us with a reward.\n",
    "The higher the difference between the expected consequence and the actual consequence, the lower our reward, which can be negative(sadness, angriness):\n",
    "\n",
    "    Actor Critic: Receives current state ----> Predicts both probability of actions and predicts expected cumulative reward\n",
    "\n",
    "This is the method used by OpenAI's PPO. However, PPO uses Assynchronous Advantage Actor-Critic(A3C), running multiple agents in parallel\n",
    "in order to make the process faster.\n",
    "\n",
    "Can be used in simple games to check if our strategy is running as planned.\n",
    "\n",
    "There's no Exploration nor Study Mode. Everything goes in real-time.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Model receives current state(game frame) ------> Feature Extraction\n",
    "\n",
    "Features -------> Vectorizer -------> Possible Actions\n",
    "\n",
    "Possible Actions -------> \"Dead\" layer(no optimization) ----> True Action(action to be executed)\n",
    "\n",
    "Features + True Action -------> Predicted Reward (Calculating Predicted reward only for the true action)\n",
    "\n",
    "    The action to be executed is sampled from the actions probability, which provides certain variety of actions(we won't be always dealing with the same action),\n",
    "    and the actions loss function is given by Cross-Entropy Loss(Possible Actions, True Action), where True Action is the action that was executed.\n",
    "\n",
    "    The predicted reward is compared with the actual cumulative reward ---> MSE or MAE(HuberLoss) ---> Backpropagation\n",
    "    \n",
    "    *An alternative to the MSE(predicted_reward, cumulative_reward) is actually using the advantage, which is given by:\n",
    "        advantage = predicted_reward - previous_reward\n",
    "\n",
    "    However, if the previous assumptions about the human decision process are correct and we were to mymetize this in a RL algorithm,\n",
    "    our True Action would be a weighted sample from the possible actions where the weights are the expected rewards for each action.\n",
    "    The result would be:\n",
    "\n",
    "    Features + Possible Actions -------> Expected Reward for each Action\n",
    "\n",
    "    (Possible Actions * Expected Reward per action) ---------> True Action(action to be executed)\n",
    "\n",
    "    The expected reward per action could compensate the softmax tendency of always providing the same action as \"best action\".\n",
    "\n",
    "    But then, let's start with the classic first.\n",
    "\n",
    "**PS: This approach is actually used to calculate the advantage for the surrogate loss in PPO.**\n",
    "\n",
    "\n",
    "This process combines TD-Learning and a Policy and it's OpenAI's default technique, being used in ChatGPT\n",
    "(and probably in Tesla cars, since the CEO's the same).\n",
    "It's also more similar to NLP, where the Vectorizer makes the work analogue to an Embedding Matrix:\n",
    "\n",
    "    Sequence of words(integers) -----------> Embedding Layer ------------> Context(Words vectors, floats)\n",
    "\n",
    "    Game frame(RGB or grayscale image, float) ------> Vectorizer --------> Context(Actions vectors, floats)\n",
    "\n",
    "    Such comparison is more explicit in Liu Ruo-Ze et al.'s HierNet, where Embedding, Transformers and LSTMs have been used.\n",
    "\n",
    "According to ChatGPT:\n",
    "\"In policy-based reinforcement learning,\n",
    "the goal is to learn a policy that directly maps states to actions,\n",
    "rather than learning a value function that predicts the expected return for each action.\n",
    "The agent's policy is typically represented by a probability distribution over the action space,\n",
    "which specifies the probability of selecting each possible action given a particular state.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLNutshell(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    Traditional Approach for Reinforcement Learning using Policy-based method ---> Actor-Critic.\n",
    "    \n",
    "    Can be used in simple games to check if our strategy is running as planned.\n",
    "\n",
    "    There's no Exploration nor Study Mode. Everything goes in real-time.\n",
    "\n",
    "    This model includes Softmax Activation Function, which limits the viability of this approach to simple keyboard games in order to avoid bottlenecks\n",
    "\n",
    "    This approach might not be viable to mouse games(since the screen coordinates leads to matrices with dimensions that are too great),\n",
    "    but sometimes we have to take a step back in order to leap forward.\n",
    "\n",
    "    In Gym and using PPO, there are multiple environments running at once.\n",
    "    We can't do this here, but we could generate multiple outputs and concatenate then in a tuple (possible_actions, predicted_reward)\n",
    "    and select the one with lower loss. This, however, is more expensive and we won't try this for now.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, commands, epsilon):\n",
    "\n",
    "        super(RLNutshell, self).__init__()\n",
    "\n",
    "        self.commands = len(commands)\n",
    "        self.epsilon = epsilon # Used to determine whether to explore or simply select the best action.\n",
    "        # This method is more used in Q-Learning, but can be used in Actor-Critic as well.\n",
    "\n",
    "        # Sticking to the traditional approach first. We might use Attention Layers if those are indeed effective.\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 100, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(100)\n",
    "        self.conv2 = torch.nn.Conv2d(100, 100, kernel_size=3, stride=1, padding=1, bias=False) # 200x200\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(100)\n",
    "        # Add pool 2x2 ---> 100x100\n",
    "        self.conv3 = torch.nn.Conv2d(100, 200, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(200)\n",
    "        self.conv4 = torch.nn.Conv2d(200, 200, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(200)\n",
    "        # Add pool 2x2 ---> 50x50\n",
    "        self.conv5 = torch.nn.Conv2d(200, 400, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(400)\n",
    "        self.conv6 = torch.nn.Conv2d(400, 400, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm6 = torch.nn.BatchNorm2d(400)\n",
    "        # Add pool 2x2 ---> 25x25\n",
    "        self.conv7 = torch.nn.Conv2d(400, 800, kernel_size=4, stride=1, bias=False) # 22x22\n",
    "        self.batchnorm7 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv8 = torch.nn.Conv2d(800, 800, kernel_size=3, stride=1, bias=False) # 20x20\n",
    "        self.batchnorm8 = torch.nn.BatchNorm2d(800)\n",
    "        # Add pool 2x2 ---> 10x10\n",
    "        self.conv9 = torch.nn.Conv2d(800, 1000, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm9 = torch.nn.BatchNorm2d(1000)\n",
    "        self.conv10 = torch.nn.Conv2d(1000, 1000, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm10 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 5x5\n",
    "\n",
    "        self.neuron_in = torch.nn.Linear(1000*5*5, 100, bias=False)\n",
    "\n",
    "        self.neuron_vectorizer = torch.nn.Linear(100, len(commands), bias=False)\n",
    "\n",
    "        #self.select_action = torch.nn.Linear(len(commands), 1, bias=False) # To test with weighted sampling. Outputs must be [0, len(commands)], so beware.\n",
    "\n",
    "        self.pred_reward = torch.nn.Linear(100+1, 1, bias=False)\n",
    "\n",
    "        self.pool2x2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.LRelu = torch.nn.LeakyReLU(0.25)\n",
    "        #self.softmax = torch.nn.LogSoftmax(-1)\n",
    "        self.softmax = torch.nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, input_frame, previous_cumulative_reward):\n",
    "\n",
    "        x = self.conv1(input_frame)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.batchnorm6(x)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = self.batchnorm7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.batchnorm8(x)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv9(x)\n",
    "        x = self.batchnorm9(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.batchnorm10(x)\n",
    "        x = self.LRelu(x)\n",
    "\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.neuron_in(x) # (Batch, 100). Since we're capturing a single frame at time, our Batch = 1.\n",
    "        # If we were running more game instances in parallel, our Batch would be equal to the number of game instances.\n",
    "\n",
    "        possible_actions = self.neuron_vectorizer(x) # (Batch, len(commands))\n",
    "        possible_actions = self.softmax(possible_actions) # The probability distribution of possible actions, the output of an Optimal Stochastic Policy Neural Network.\n",
    "\n",
    "        # Calculating possible reward for each action\n",
    "\n",
    "        expected_reward = torch.zeros_like(possible_actions, device=device)\n",
    "\n",
    "        for action in range(self.commands):\n",
    "\n",
    "            y = torch.cat((x, possible_actions[0, action].unsqueeze(0).unsqueeze(0)), -1)\n",
    "            y = self.pred_reward(y)\n",
    "            expected_reward[0, action] = y.squeeze(0)\n",
    "\n",
    "        del y\n",
    "\n",
    "        #expected_reward = torch.mean(expected_reward)\n",
    "\n",
    "        if torch.rand((1,)) < self.epsilon: # The higher the epsilon, the more random our model will be. The lower, the more deterministic.\n",
    "            # The expected reward can also be used for sampling, but let's stick to epsilon for now.\n",
    "\n",
    "            true_action = torch.randint(0, self.commands, (1, 1), device=device)\n",
    "        \n",
    "        else: # We can't use .argmax() directly as this detaches the tensor's graphs, since argmax isn't differentiable.\n",
    "\n",
    "            one_hot = torch.zeros_like(possible_actions, device=device)\n",
    "            one_hot[0, possible_actions.argmax()] = 1.\n",
    "            true_action = possible_actions * one_hot\n",
    "            true_action = torch.sum(true_action, dim=-1, keepdim=True)\n",
    "\n",
    "        one_hot = torch.zeros_like(possible_actions, device=device)\n",
    "        one_hot[0, possible_actions.argmax()] = 1.\n",
    "        predicted_reward = expected_reward * one_hot\n",
    "        predicted_reward = torch.sum(predicted_reward, dim=-1, keepdim=True)\n",
    "\n",
    "        expected_reward = torch.mean(expected_reward)\n",
    "\n",
    "        return possible_actions, true_action, predicted_reward, expected_reward\n",
    "\n",
    "    def execute_command(self, command):\n",
    "        '''\n",
    "        Command must be an item from the commands list. Make sure all commands are lowered words.\n",
    "        For now, we'll also suppose that all commands are keyboard commands.\n",
    "        '''\n",
    "\n",
    "        command = command.split(\"_\")\n",
    "\n",
    "        if \"down\" in command:\n",
    "\n",
    "            keyboard.press(command[1])\n",
    "\n",
    "        elif \"up\" in command:\n",
    "\n",
    "            keyboard.release(command[1])\n",
    "\n",
    "        else: # Press and Release\n",
    "\n",
    "            keyboard.send(command[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RLNutshell(commands, epsilon=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giova\\AppData\\Local\\Temp/ipykernel_19280/3443294517.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reward = torch.tensor(reward, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 10\n",
      "Current Loss: 52.12706756591797\n",
      "Action Loss: 2.549668788909912\tReward Loss: 49.57740020751953\n",
      "Predicted Reward: 0.047055453062057495\tCurrent Reward: 10.0\n",
      "up_left\n",
      "Current step: 20\n",
      "Current Loss: 7.34737491607666\n",
      "Action Loss: 2.5853519439697266\tReward Loss: 4.762022972106934\n",
      "Predicted Reward: 18.793216705322266\tCurrent Reward: 20.0\n",
      "down_down\n",
      "Current step: 30\n",
      "Current Loss: 108.53346252441406\n",
      "Action Loss: 2.663149356842041\tReward Loss: 105.87031555175781\n",
      "Predicted Reward: 14.942988395690918\tCurrent Reward: 28.0\n",
      "down_left\n",
      "Current step: 40\n",
      "Current Loss: 48.827980041503906\n",
      "Action Loss: 1.770707130432129\tReward Loss: 47.057273864746094\n",
      "Predicted Reward: 31.443038940429688\tCurrent Reward: 38.0\n",
      "up_up\n",
      "Current step: 50\n",
      "Current Loss: 56.66971206665039\n",
      "Action Loss: 2.6858973503112793\tReward Loss: 53.98381423950195\n",
      "Predicted Reward: 41.78805923461914\tCurrent Reward: 48.0\n",
      "up_down\n",
      "Current step: 60\n",
      "Current Loss: 45.13782501220703\n",
      "Action Loss: 1.6972062587738037\tReward Loss: 43.44062042236328\n",
      "Predicted Reward: 54.0877685546875\tCurrent Reward: 58.0\n",
      "up_up\n",
      "Current step: 70\n",
      "Current Loss: 31.675329208374023\n",
      "Action Loss: 1.6923083066940308\tReward Loss: 29.983020782470703\n",
      "Predicted Reward: 70.826416015625\tCurrent Reward: 56.0\n",
      "up_up\n",
      "Current step: 80\n",
      "Current Loss: 2.4120523929595947\n",
      "Action Loss: 1.691254734992981\tReward Loss: 0.7207976579666138\n",
      "Predicted Reward: 70.22296142578125\tCurrent Reward: 62.0\n",
      "up_up\n",
      "Current step: 90\n",
      "Current Loss: 134.3931121826172\n",
      "Action Loss: 1.6933233737945557\tReward Loss: 132.6997833251953\n",
      "Predicted Reward: 61.8987922668457\tCurrent Reward: 72.0\n",
      "up_up\n",
      "Current step: 100\n",
      "Current Loss: 163.5012664794922\n",
      "Action Loss: 1.6905885934829712\tReward Loss: 161.81068420410156\n",
      "Predicted Reward: 71.122802734375\tCurrent Reward: 82.0\n",
      "up_up\n",
      "Current step: 110\n",
      "Current Loss: 137.73828125\n",
      "Action Loss: 1.6894330978393555\tReward Loss: 136.04884338378906\n",
      "Predicted Reward: 83.8940200805664\tCurrent Reward: 92.0\n",
      "up_up\n",
      "Current step: 120\n",
      "Current Loss: 121.33812713623047\n",
      "Action Loss: 1.6892274618148804\tReward Loss: 119.6489028930664\n",
      "Predicted Reward: 96.14527893066406\tCurrent Reward: 102.0\n",
      "up_up\n",
      "Current step: 130\n",
      "Current Loss: 94.62663269042969\n",
      "Action Loss: 1.689135193824768\tReward Loss: 92.9375\n",
      "Predicted Reward: 109.29600524902344\tCurrent Reward: 112.0\n",
      "up_up\n",
      "Current step: 140\n",
      "Current Loss: 52.338096618652344\n",
      "Action Loss: 1.6891067028045654\tReward Loss: 50.648990631103516\n",
      "Predicted Reward: 122.15034484863281\tCurrent Reward: 120.0\n",
      "up_up\n",
      "Current step: 150\n",
      "Current Loss: 45.92377471923828\n",
      "Action Loss: 1.689094066619873\tReward Loss: 44.23468017578125\n",
      "Predicted Reward: 131.7713165283203\tCurrent Reward: 128.0\n",
      "up_up\n",
      "Current step: 160\n",
      "Current Loss: 2.524679660797119\n",
      "Action Loss: 1.689091444015503\tReward Loss: 0.8355881571769714\n",
      "Predicted Reward: 152.5474853515625\tCurrent Reward: 136.0\n",
      "up_up\n",
      "Current step: 170\n",
      "Current Loss: 3.3136038780212402\n",
      "Action Loss: 1.6890909671783447\tReward Loss: 1.624513030052185\n",
      "Predicted Reward: 157.99722290039062\tCurrent Reward: 144.0\n",
      "up_up\n",
      "Current step: 180\n",
      "Current Loss: 14.65392017364502\n",
      "Action Loss: 1.6890907287597656\tReward Loss: 12.964829444885254\n",
      "Predicted Reward: 161.00875854492188\tCurrent Reward: 150.0\n",
      "up_up\n",
      "Current step: 190\n",
      "Current Loss: 76.78408813476562\n",
      "Action Loss: 1.689090609550476\tReward Loss: 75.09500122070312\n",
      "Predicted Reward: 164.160888671875\tCurrent Reward: 160.0\n",
      "up_up\n",
      "Current step: 200\n",
      "Current Loss: 30.43303680419922\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 28.743946075439453\n",
      "Predicted Reward: 180.46435546875\tCurrent Reward: 170.0\n",
      "up_up\n",
      "Current step: 210\n",
      "Current Loss: 37.21356964111328\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 34.524478912353516\n",
      "Predicted Reward: 190.76715087890625\tCurrent Reward: 180.0\n",
      "down_shift\n",
      "Current step: 220\n",
      "Current Loss: 33.53485870361328\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 31.845767974853516\n",
      "Predicted Reward: 202.24366760253906\tCurrent Reward: 190.0\n",
      "up_up\n",
      "Current step: 230\n",
      "Current Loss: 156.4765625\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 154.7874755859375\n",
      "Predicted Reward: 200.45028686523438\tCurrent Reward: 198.0\n",
      "up_up\n",
      "Current step: 240\n",
      "Current Loss: 6.605386734008789\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 4.916296005249023\n",
      "Predicted Reward: 227.62701416015625\tCurrent Reward: 208.0\n",
      "up_up\n",
      "Current step: 250\n",
      "Current Loss: 2.900940418243408\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 1.2118500471115112\n",
      "Predicted Reward: 238.27020263671875\tCurrent Reward: 216.0\n",
      "up_up\n",
      "Current step: 260\n",
      "Current Loss: 3.096410036087036\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 1.4073195457458496\n",
      "Predicted Reward: 244.08633422851562\tCurrent Reward: 218.0\n",
      "up_up\n",
      "Current step: 270\n",
      "Current Loss: 17.645305633544922\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 15.956215858459473\n",
      "Predicted Reward: 247.05654907226562\tCurrent Reward: 228.0\n",
      "up_up\n",
      "Current step: 280\n",
      "Current Loss: 68.46864318847656\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 66.77955627441406\n",
      "Predicted Reward: 251.6035919189453\tCurrent Reward: 238.0\n",
      "up_up\n",
      "Current step: 290\n",
      "Current Loss: 17.83104705810547\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 15.14195728302002\n",
      "Predicted Reward: 262.77435302734375\tCurrent Reward: 242.0\n",
      "down_down\n",
      "Current step: 300\n",
      "Current Loss: 7.590372085571289\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 5.901281356811523\n",
      "Predicted Reward: 277.1505432128906\tCurrent Reward: 246.0\n",
      "up_up\n",
      "Current step: 310\n",
      "Current Loss: 12.01898193359375\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 10.329891204833984\n",
      "Predicted Reward: 282.828125\tCurrent Reward: 250.0\n",
      "up_up\n",
      "Current step: 320\n",
      "Current Loss: 7.0688371658325195\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 5.379746437072754\n",
      "Predicted Reward: 270.3113098144531\tCurrent Reward: 240.0\n",
      "up_up\n",
      "Current step: 330\n",
      "Current Loss: 48.538448333740234\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 46.84935760498047\n",
      "Predicted Reward: 266.3109130859375\tCurrent Reward: 230.0\n",
      "up_up\n",
      "Current step: 340\n",
      "Current Loss: 9.985027313232422\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 7.295936584472656\n",
      "Predicted Reward: 262.42230224609375\tCurrent Reward: 240.0\n",
      "up_z\n",
      "Current step: 350\n",
      "Current Loss: 1037.9981689453125\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 1036.30908203125\n",
      "Predicted Reward: 227.1933135986328\tCurrent Reward: 250.0\n",
      "up_up\n",
      "Current step: 360\n",
      "Current Loss: 133.30335998535156\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 131.61427307128906\n",
      "Predicted Reward: 270.86187744140625\tCurrent Reward: 260.0\n",
      "up_up\n",
      "Current step: 370\n",
      "Current Loss: 110.19727325439453\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 108.50818634033203\n",
      "Predicted Reward: 281.40948486328125\tCurrent Reward: 268.0\n",
      "up_up\n",
      "Current step: 380\n",
      "Current Loss: 134.2818145751953\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 132.5927276611328\n",
      "Predicted Reward: 290.79498291015625\tCurrent Reward: 278.0\n",
      "up_up\n",
      "Current step: 390\n",
      "Current Loss: 254.5340118408203\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 252.8449249267578\n",
      "Predicted Reward: 295.01385498046875\tCurrent Reward: 288.0\n",
      "up_up\n",
      "Current step: 400\n",
      "Current Loss: 81.54324340820312\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 78.85415649414062\n",
      "Predicted Reward: 314.9353332519531\tCurrent Reward: 296.0\n",
      "down_down\n",
      "Current step: 410\n",
      "Current Loss: 64.21489715576172\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 62.52580642700195\n",
      "Predicted Reward: 327.5748596191406\tCurrent Reward: 306.0\n",
      "up_up\n",
      "Current step: 420\n",
      "Current Loss: 13.060685157775879\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 11.371594429016113\n",
      "Predicted Reward: 345.812255859375\tCurrent Reward: 316.0\n",
      "up_up\n",
      "Current step: 430\n",
      "Current Loss: 3.0738816261291504\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 0.3847910761833191\n",
      "Predicted Reward: 358.75250244140625\tCurrent Reward: 322.0\n",
      "down_shift\n",
      "Current step: 440\n",
      "Current Loss: 6.162214279174805\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 4.473123550415039\n",
      "Predicted Reward: 361.12109375\tCurrent Reward: 328.0\n",
      "up_up\n",
      "Current step: 450\n",
      "Current Loss: 55.32402420043945\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 53.63493347167969\n",
      "Predicted Reward: 364.04766845703125\tCurrent Reward: 338.0\n",
      "up_up\n",
      "Current step: 460\n",
      "Current Loss: 38.35435104370117\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 36.665260314941406\n",
      "Predicted Reward: 377.15185546875\tCurrent Reward: 348.0\n",
      "up_up\n",
      "Current step: 470\n",
      "Current Loss: 32.278472900390625\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 30.589384078979492\n",
      "Predicted Reward: 389.0870361328125\tCurrent Reward: 358.0\n",
      "up_up\n",
      "Current step: 480\n",
      "Current Loss: 176.58738708496094\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 174.89830017089844\n",
      "Predicted Reward: 388.10797119140625\tCurrent Reward: 368.0\n",
      "up_up\n",
      "Current step: 490\n",
      "Current Loss: 6.366028785705566\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 4.676938056945801\n",
      "Predicted Reward: 416.6017761230469\tCurrent Reward: 378.0\n",
      "up_up\n",
      "Current step: 500\n",
      "Current Loss: 27.431285858154297\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 25.74219512939453\n",
      "Predicted Reward: 423.13861083984375\tCurrent Reward: 388.0\n",
      "up_up\n",
      "Current step: 510\n",
      "Current Loss: 28.27032470703125\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 26.581233978271484\n",
      "Predicted Reward: 427.45416259765625\tCurrent Reward: 392.0\n",
      "up_up\n",
      "Current step: 520\n",
      "Current Loss: 1.9812941551208496\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 0.2922036349773407\n",
      "Predicted Reward: 441.3728332519531\tCurrent Reward: 398.0\n",
      "up_up\n",
      "Current step: 530\n",
      "Current Loss: 2.0962300300598145\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 0.4071396589279175\n",
      "Predicted Reward: 452.11376953125\tCurrent Reward: 406.0\n",
      "up_up\n",
      "Current step: 540\n",
      "Current Loss: 9.952577590942383\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 8.263486862182617\n",
      "Predicted Reward: 457.7052001953125\tCurrent Reward: 416.0\n",
      "up_up\n",
      "Current step: 550\n",
      "Current Loss: 30.446805953979492\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 28.757715225219727\n",
      "Predicted Reward: 464.90679931640625\tCurrent Reward: 426.0\n",
      "up_up\n",
      "Current step: 560\n",
      "Current Loss: 83.96047973632812\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 82.27139282226562\n",
      "Predicted Reward: 470.1917724609375\tCurrent Reward: 436.0\n",
      "up_up\n",
      "Current step: 570\n",
      "Current Loss: 29.42497444152832\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 27.735883712768555\n",
      "Predicted Reward: 487.28009033203125\tCurrent Reward: 446.0\n",
      "up_up\n",
      "Current step: 580\n",
      "Current Loss: 28.85099983215332\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 26.161909103393555\n",
      "Predicted Reward: 498.6294250488281\tCurrent Reward: 456.0\n",
      "down_right\n",
      "Current step: 590\n",
      "Current Loss: 4.421100616455078\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 2.7320101261138916\n",
      "Predicted Reward: 515.1805419921875\tCurrent Reward: 466.0\n",
      "up_up\n",
      "Current step: 600\n",
      "Current Loss: 2.3409266471862793\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 0.651836097240448\n",
      "Predicted Reward: 527.6202392578125\tCurrent Reward: 476.0\n",
      "up_up\n",
      "Current step: 610\n",
      "Current Loss: 6.657397270202637\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 3.9683070182800293\n",
      "Predicted Reward: 534.6475830078125\tCurrent Reward: 484.0\n",
      "up_z\n",
      "Current step: 620\n",
      "Current Loss: 4.729225158691406\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 3.040134906768799\n",
      "Predicted Reward: 546.1491088867188\tCurrent Reward: 494.0\n",
      "up_up\n",
      "Current step: 630\n",
      "Current Loss: 130.7018280029297\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 129.0127410888672\n",
      "Predicted Reward: 539.9298095703125\tCurrent Reward: 502.0\n",
      "up_up\n",
      "Current step: 640\n",
      "Current Loss: 1.7194466590881348\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 0.030356215313076973\n",
      "Predicted Reward: 569.1626586914062\tCurrent Reward: 512.0\n",
      "up_up\n",
      "Current step: 650\n",
      "Current Loss: 15.72324275970459\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 13.034152030944824\n",
      "Predicted Reward: 579.00634765625\tCurrent Reward: 516.0\n",
      "up_down\n",
      "Current step: 660\n",
      "Current Loss: 2.2978291511535645\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 0.6087386608123779\n",
      "Predicted Reward: 576.5518188476562\tCurrent Reward: 520.0\n",
      "up_up\n",
      "Current step: 670\n",
      "Current Loss: 26.370363235473633\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 24.681272506713867\n",
      "Predicted Reward: 565.52685546875\tCurrent Reward: 516.0\n",
      "up_up\n",
      "Current step: 680\n",
      "Current Loss: 193.4397735595703\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 191.7506866455078\n",
      "Predicted Reward: 583.9813232421875\tCurrent Reward: 506.0\n",
      "up_up\n",
      "Current step: 690\n",
      "Current Loss: 173.63597106933594\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 171.94688415527344\n",
      "Predicted Reward: 580.6048583984375\tCurrent Reward: 504.0\n",
      "up_up\n",
      "Current step: 700\n",
      "Current Loss: 139.6359100341797\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 137.9468231201172\n",
      "Predicted Reward: 552.655517578125\tCurrent Reward: 514.0\n",
      "up_up\n",
      "Current step: 710\n",
      "Current Loss: 627.0950927734375\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 625.406005859375\n",
      "Predicted Reward: 542.92578125\tCurrent Reward: 524.0\n",
      "up_up\n",
      "Current step: 720\n",
      "Current Loss: 120.91349029541016\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 118.22440338134766\n",
      "Predicted Reward: 576.2479248046875\tCurrent Reward: 534.0\n",
      "up_z\n",
      "Current step: 730\n",
      "Current Loss: 32.53034210205078\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 30.841251373291016\n",
      "Predicted Reward: 595.718017578125\tCurrent Reward: 544.0\n",
      "up_up\n",
      "Current step: 740\n",
      "Current Loss: 76.01274108886719\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 73.32365417480469\n",
      "Predicted Reward: 602.1002197265625\tCurrent Reward: 554.0\n",
      "up_down\n",
      "Current step: 750\n",
      "Current Loss: 49.68803787231445\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 47.99894714355469\n",
      "Predicted Reward: 615.7801513671875\tCurrent Reward: 564.0\n",
      "up_up\n",
      "Current step: 760\n",
      "Current Loss: 63.475440979003906\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 60.78635025024414\n",
      "Predicted Reward: 618.8599853515625\tCurrent Reward: 568.0\n",
      "down_left\n",
      "Current step: 770\n",
      "Current Loss: 12.6348295211792\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 10.945738792419434\n",
      "Predicted Reward: 637.0235595703125\tCurrent Reward: 578.0\n",
      "up_up\n",
      "Current step: 780\n",
      "Current Loss: 30.64642333984375\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 28.957332611083984\n",
      "Predicted Reward: 661.7890625\tCurrent Reward: 588.0\n",
      "up_up\n",
      "Current step: 790\n",
      "Current Loss: 28.4566707611084\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 26.767580032348633\n",
      "Predicted Reward: 656.314697265625\tCurrent Reward: 598.0\n",
      "up_up\n",
      "Current step: 800\n",
      "Current Loss: 123.49822998046875\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 121.80914306640625\n",
      "Predicted Reward: 658.2130737304688\tCurrent Reward: 608.0\n",
      "up_up\n",
      "Current step: 810\n",
      "Current Loss: 9.866893768310547\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 7.1778035163879395\n",
      "Predicted Reward: 682.456787109375\tCurrent Reward: 618.0\n",
      "up_right\n",
      "Current step: 820\n",
      "Current Loss: 14.429510116577148\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 11.740419387817383\n",
      "Predicted Reward: 692.3936767578125\tCurrent Reward: 628.0\n",
      "down_right\n",
      "Current step: 830\n",
      "Current Loss: 5.952062606811523\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 4.262971878051758\n",
      "Predicted Reward: 703.4223022460938\tCurrent Reward: 636.0\n",
      "up_up\n",
      "Current step: 840\n",
      "Current Loss: 26.672962188720703\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 24.983871459960938\n",
      "Predicted Reward: 707.7013549804688\tCurrent Reward: 644.0\n",
      "up_up\n",
      "Current step: 850\n",
      "Current Loss: 48.83098602294922\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 46.14189529418945\n",
      "Predicted Reward: 713.7706298828125\tCurrent Reward: 652.0\n",
      "down_z\n",
      "Current step: 860\n",
      "Current Loss: 3.3702802658081055\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 1.6811896562576294\n",
      "Predicted Reward: 731.2958984375\tCurrent Reward: 660.0\n",
      "up_up\n",
      "Current step: 870\n",
      "Current Loss: 13.362468719482422\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 11.673377990722656\n",
      "Predicted Reward: 747.5909423828125\tCurrent Reward: 668.0\n",
      "up_up\n",
      "Current step: 880\n",
      "Current Loss: 43.808876037597656\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 41.11978530883789\n",
      "Predicted Reward: 741.034912109375\tCurrent Reward: 676.0\n",
      "down_z\n",
      "Current step: 890\n",
      "Current Loss: 1594.6224365234375\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 1592.933349609375\n",
      "Predicted Reward: 699.5072631835938\tCurrent Reward: 686.0\n",
      "up_up\n",
      "Current step: 900\n",
      "Current Loss: 13.22016716003418\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 11.531076431274414\n",
      "Predicted Reward: 778.6692504882812\tCurrent Reward: 696.0\n",
      "up_up\n",
      "Current step: 910\n",
      "Current Loss: 5.721404075622559\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 3.032313823699951\n",
      "Predicted Reward: 781.7081909179688\tCurrent Reward: 706.0\n",
      "up_left\n",
      "Current step: 920\n",
      "Current Loss: 1004.489501953125\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 1002.8004150390625\n",
      "Predicted Reward: 741.3511962890625\tCurrent Reward: 712.0\n",
      "up_up\n",
      "Current step: 930\n",
      "Current Loss: 48.311851501464844\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 46.62276077270508\n",
      "Predicted Reward: 801.8404541015625\tCurrent Reward: 712.0\n",
      "up_up\n",
      "Current step: 940\n",
      "Current Loss: 163.2136993408203\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 160.5246124267578\n",
      "Predicted Reward: 822.1309814453125\tCurrent Reward: 722.0\n",
      "down_z\n",
      "Current step: 950\n",
      "Current Loss: 153.14492797851562\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 151.45584106445312\n",
      "Predicted Reward: 791.77294921875\tCurrent Reward: 730.0\n",
      "up_up\n",
      "Current step: 960\n",
      "Current Loss: 14.152266502380371\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 11.463175773620605\n",
      "Predicted Reward: 816.902099609375\tCurrent Reward: 740.0\n",
      "up_shift\n",
      "Current step: 970\n",
      "Current Loss: 121.29621124267578\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 119.60712432861328\n",
      "Predicted Reward: 816.1483154296875\tCurrent Reward: 750.0\n",
      "up_up\n",
      "Current step: 980\n",
      "Current Loss: 122.21693420410156\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 119.52784729003906\n",
      "Predicted Reward: 827.26513671875\tCurrent Reward: 760.0\n",
      "down_shift\n",
      "Current step: 990\n",
      "Current Loss: 9.965514183044434\n",
      "Action Loss: 2.6890904903411865\tReward Loss: 7.276423454284668\n",
      "Predicted Reward: 851.31689453125\tCurrent Reward: 770.0\n",
      "up_down\n",
      "Current step: 1000\n",
      "Current Loss: 6.018796920776367\n",
      "Action Loss: 1.6890904903411865\tReward Loss: 4.329706192016602\n",
      "Predicted Reward: 867.714111328125\tCurrent Reward: 778.0\n",
      "up_up\n"
     ]
    }
   ],
   "source": [
    "reward = torch.zeros((1, 1), device=device) # Cumulative reward\n",
    "reward_input = reward.clone()\n",
    "previous_value = torch.zeros((1, 1), device=device) # Initial value function Weighted Moving Average\n",
    "steps = 0\n",
    "save_point = 10 # Also optimization point\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.1)\n",
    "grads = []\n",
    "\n",
    "reward_loss = torch.nn.HuberLoss(delta=100)\n",
    "actions_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)\n",
    "\n",
    "\n",
    "while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "\n",
    "    possible_actions, true_action, predicted_reward, _ = model(frame, reward_input)\n",
    "\n",
    "    del frame\n",
    "\n",
    "    command = dataset.get_command(true_action.detach().cpu().item())\n",
    "\n",
    "    model.execute_command(command)\n",
    "\n",
    "    life = dataset.get_consequences(849, 400, 498-400, 904-849, tesseract_config='--psm 8')\n",
    "\n",
    "    life = preprocess_Jigoku(life)\n",
    "\n",
    "    if life <= 1:\n",
    "\n",
    "        reward -= 1.\n",
    "\n",
    "    else:\n",
    "\n",
    "        reward += 1.\n",
    "\n",
    "    del life\n",
    "\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    # PPO updates the policy based on the previous policy parameters, which can also be seen as the previous policy outputs\n",
    "    # Traditionally, however, we use the possible actions(policy output) and the true action(agent output)\n",
    "\n",
    "    command_loss = actions_loss(possible_actions, true_action.detach().squeeze(0))\n",
    "    value_loss = reward_loss(predicted_reward, reward)\n",
    "\n",
    "    total_loss = command_loss + value_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    try:\n",
    "\n",
    "        grads.append(torch.mean(model.neuron_vectorizer.weight.grad))\n",
    "\n",
    "    except:\n",
    "\n",
    "        pass\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    reward_input = reward.clone()\n",
    "    \n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        #print(f\"Current Loss: {total_loss.item()}\\tCurrent LR: {scheduler.get_last_lr()[0]}\")\n",
    "        print(f\"Current Loss: {total_loss.item()}\")\n",
    "        print(f\"Action Loss: {command_loss.item()}\\tReward Loss: {value_loss.item()}\")\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward.item()}\")\n",
    "        print(command)\n",
    "\n",
    "        '''torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")'''\n",
    "\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For PPO:\n",
    "\n",
    "https://github.com/liuruoze/HierNet-SC2/blob/main/algo/ppo.py\n",
    "\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "\n",
    "expected_reward = torch.mean(reward_for_each_action)\n",
    "\n",
    "advantage = predicted_reward - expected_reward\n",
    "\n",
    "surrogate_loss = (possible_actions.argmax()/previous_possible_actions.argmax()) * advantage # Add clip (ratio can't be greater than 0.2)\n",
    "\n",
    "\n",
    "predicted_value = predicted_reward + discount_factor*previous_reward\n",
    "\n",
    "current_value = reward + discount_factor*previous_reward\n",
    "\n",
    "value_loss = mse(predicted_value, current_value) ---------> value_loss = mse(predicted_reward, reward)\n",
    "\n",
    "\n",
    "total_loss = surrogate_loss + value_loss*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RLNutshell(commands, epsilon=0.2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-5.1686e-02,  8.5438e-02, -3.6007e-02,  1.8787e-02,  4.3484e-02,\n",
       "          1.0793e-01, -1.2112e-01,  1.2513e-02, -1.8009e-01, -7.2568e-02,\n",
       "          1.8884e-01, -3.5154e-02,  6.1086e-03,  1.9923e-02,  1.9526e-02,\n",
       "          6.6890e-02,  1.0109e-01,  1.4790e-01, -4.5165e-03, -1.3757e-01,\n",
       "         -3.3153e-02, -2.5010e-03,  1.2422e-01, -1.8540e-02, -1.4943e-02,\n",
       "          1.5786e-01, -3.9742e-01, -1.2863e-01, -6.4086e-02,  1.3487e-02,\n",
       "          1.7874e-01,  5.5828e-02,  5.1097e-02, -4.2842e-02, -5.7433e-02,\n",
       "         -7.9712e-02, -9.6630e-02,  4.9805e-02, -7.0034e-02, -7.9895e-02,\n",
       "          2.8797e-02, -1.1362e-01,  1.1717e-01, -3.6348e-02, -7.2291e-02,\n",
       "          6.6278e-02, -3.9130e-02, -1.1846e-01,  5.9452e-02,  1.1040e-02,\n",
       "         -6.3735e-02,  5.5863e-03, -1.9960e-01,  3.8771e-02,  5.5197e-02,\n",
       "         -6.5724e-02, -1.6580e-01,  3.9302e-02,  6.0213e-02,  2.5666e-02,\n",
       "          1.9644e-02,  9.5550e-02,  1.2468e-01,  5.4121e-02,  5.0882e-02,\n",
       "         -2.8105e-01,  1.5258e-01, -3.4509e-02, -7.7804e-02,  5.5780e-02,\n",
       "          3.7767e-02,  1.4255e-02, -4.0618e-02, -2.8933e-02,  1.2386e-01,\n",
       "         -6.2557e-02, -1.2396e-01,  9.3734e-02,  8.4185e-02, -1.9621e-01,\n",
       "         -1.1776e-02,  4.0108e-02, -9.6395e-02,  5.9368e-02,  8.7614e-02,\n",
       "         -1.0372e-01, -8.9316e-02,  1.2931e-01,  1.1306e-01,  8.6528e-02,\n",
       "         -2.7854e-02,  7.2104e-03,  2.1055e-02,  1.3617e-01,  7.0610e-02,\n",
       "         -1.4901e-02,  2.0872e-02,  1.6401e-01, -3.2337e-04,  7.2860e-02,\n",
       "         -1.4255e-01]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the model weights: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/ - Item 2\n",
    "import math\n",
    "\n",
    "torch.nn.init.orthogonal_(model.neuron_in.weight, math.sqrt(2))\n",
    "torch.nn.init.orthogonal_(model.neuron_vectorizer.weight, 0.01)\n",
    "torch.nn.init.orthogonal_(model.pred_reward.weight, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giova\\AppData\\Local\\Temp/ipykernel_11776/3356857927.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reward = torch.tensor(reward, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 10\n",
      "Current Loss: 23.693843841552734\n",
      "Action Loss: -2.568960189819336e-05\tAdvantage: -0.0001284480094909668\tRatio: 0.20000000298023224\tReward Loss: 47.38773727416992\n",
      "Predicted Reward: 0.2647300362586975\tCurrent Reward: 10.0\n",
      "down_up\n",
      "Current step: 20\n",
      "Current Loss: 324.6003112792969\n",
      "Action Loss: -0.0002471923944540322\tAdvantage: -0.0012359619140625\tRatio: 0.20000000298023224\tReward Loss: 649.2011108398438\n",
      "Predicted Reward: 56.033348083496094\tCurrent Reward: 20.0\n",
      "down_down\n",
      "Current step: 30\n",
      "Current Loss: 133.67982482910156\n",
      "Action Loss: -7.190704491222277e-05\tAdvantage: -0.00035953521728515625\tRatio: 0.20000000298023224\tReward Loss: 267.35980224609375\n",
      "Predicted Reward: 4.875994682312012\tCurrent Reward: 28.0\n",
      "down_up\n",
      "Current step: 40\n",
      "Current Loss: 300.2787780761719\n",
      "Action Loss: -8.325577073264867e-05\tAdvantage: -0.0004162788391113281\tRatio: 0.20000000298023224\tReward Loss: 600.5577392578125\n",
      "Predicted Reward: 3.3428871631622314\tCurrent Reward: 38.0\n",
      "down_up\n",
      "Current step: 50\n",
      "Current Loss: 127.9990005493164\n",
      "Action Loss: -0.0002849579032044858\tAdvantage: -0.0014247894287109375\tRatio: 0.20000000298023224\tReward Loss: 255.99856567382812\n",
      "Predicted Reward: 25.37264633178711\tCurrent Reward: 48.0\n",
      "down_up\n",
      "Current step: 60\n",
      "Current Loss: 282.51837158203125\n",
      "Action Loss: -0.0003269195731263608\tAdvantage: -0.0016345977783203125\tRatio: 0.20000000298023224\tReward Loss: 565.0374145507812\n",
      "Predicted Reward: 24.383413314819336\tCurrent Reward: 58.0\n",
      "up_up\n",
      "Current step: 70\n",
      "Current Loss: 49.22332000732422\n",
      "Action Loss: -0.00067901611328125\tAdvantage: -0.00339508056640625\tRatio: 0.20000000298023224\tReward Loss: 98.447998046875\n",
      "Predicted Reward: 53.96803665161133\tCurrent Reward: 68.0\n",
      "down_up\n",
      "Current step: 80\n",
      "Current Loss: 43.49833679199219\n",
      "Action Loss: -0.0008346557733602822\tAdvantage: -0.00417327880859375\tRatio: 0.20000000298023224\tReward Loss: 86.99834442138672\n",
      "Predicted Reward: 64.80921936035156\tCurrent Reward: 78.0\n",
      "down_up\n",
      "Current step: 90\n",
      "Current Loss: 7.389976978302002\n",
      "Action Loss: -0.0010437011951580644\tAdvantage: -0.005218505859375\tRatio: 0.20000000298023224\tReward Loss: 14.782041549682617\n",
      "Predicted Reward: 82.56271362304688\tCurrent Reward: 88.0\n",
      "down_left\n",
      "Current step: 100\n",
      "Current Loss: 73.00511169433594\n",
      "Action Loss: -0.0009918212890625\tAdvantage: -0.0049591064453125\tRatio: 0.20000000298023224\tReward Loss: 146.01220703125\n",
      "Predicted Reward: 80.9112777709961\tCurrent Reward: 98.0\n",
      "down_left\n",
      "Current step: 110\n",
      "Current Loss: 240.9096221923828\n",
      "Action Loss: -0.0008041381952352822\tAdvantage: -0.00402069091796875\tRatio: 0.20000000298023224\tReward Loss: 481.82086181640625\n",
      "Predicted Reward: 76.9574203491211\tCurrent Reward: 108.0\n",
      "down_up\n",
      "Current step: 120\n",
      "Current Loss: 35.859527587890625\n",
      "Action Loss: -0.0008560181013308465\tAdvantage: -0.00428009033203125\tRatio: 0.20000000298023224\tReward Loss: 71.72076416015625\n",
      "Predicted Reward: 106.0232925415039\tCurrent Reward: 118.0\n",
      "down_up\n",
      "Current step: 130\n",
      "Current Loss: 30.38999366760254\n",
      "Action Loss: -0.00057220458984375\tAdvantage: -0.00286102294921875\tRatio: 0.20000000298023224\tReward Loss: 60.781131744384766\n",
      "Predicted Reward: 116.97447204589844\tCurrent Reward: 128.0\n",
      "down_up\n",
      "Current step: 140\n",
      "Current Loss: 37.515235900878906\n",
      "Action Loss: -0.0002990722714457661\tAdvantage: -0.001495361328125\tRatio: 0.20000000298023224\tReward Loss: 75.03106689453125\n",
      "Predicted Reward: 125.75001525878906\tCurrent Reward: 138.0\n",
      "down_up\n",
      "Current step: 150\n",
      "Current Loss: 12.098746299743652\n",
      "Action Loss: -0.0003692627069540322\tAdvantage: -0.0018463134765625\tRatio: 0.20000000298023224\tReward Loss: 24.198230743408203\n",
      "Predicted Reward: 135.04324340820312\tCurrent Reward: 142.0\n",
      "down_up\n",
      "Current step: 160\n",
      "Current Loss: 1.5123374462127686\n",
      "Action Loss: -0.0004394531424622983\tAdvantage: -0.002197265625\tRatio: 0.20000000298023224\tReward Loss: 3.0255537033081055\n",
      "Predicted Reward: 152.45989990234375\tCurrent Reward: 150.0\n",
      "down_up\n",
      "Current step: 170\n",
      "Current Loss: 22.918493270874023\n",
      "Action Loss: -0.0006256103515625\tAdvantage: -0.0031280517578125\tRatio: 0.20000000298023224\tReward Loss: 45.83823776245117\n",
      "Predicted Reward: 157.5747833251953\tCurrent Reward: 148.0\n",
      "down_up\n",
      "Current step: 180\n",
      "Current Loss: 14.71251392364502\n",
      "Action Loss: -0.0008483887067995965\tAdvantage: -0.004241943359375\tRatio: 0.20000000298023224\tReward Loss: 29.426725387573242\n",
      "Predicted Reward: 150.32839965820312\tCurrent Reward: 158.0\n",
      "down_up\n",
      "Current step: 190\n",
      "Current Loss: 116.4882583618164\n",
      "Action Loss: -0.0012359619140625\tAdvantage: -0.0061798095703125\tRatio: 0.20000000298023224\tReward Loss: 232.97898864746094\n",
      "Predicted Reward: 146.4139404296875\tCurrent Reward: 168.0\n",
      "down_up\n",
      "Current step: 200\n",
      "Current Loss: 76.72584533691406\n",
      "Action Loss: -0.0010986328125\tAdvantage: -0.0054931640625\tRatio: 0.20000000298023224\tReward Loss: 153.45388793945312\n",
      "Predicted Reward: 158.48121643066406\tCurrent Reward: 176.0\n",
      "down_up\n",
      "Current step: 210\n",
      "Current Loss: 102.6310806274414\n",
      "Action Loss: -0.0007568359724245965\tAdvantage: -0.0037841796875\tRatio: 0.20000000298023224\tReward Loss: 205.263671875\n",
      "Predicted Reward: 165.738525390625\tCurrent Reward: 186.0\n",
      "down_up\n",
      "Current step: 220\n",
      "Current Loss: 0.010656509548425674\n",
      "Action Loss: -0.0006286621210165322\tAdvantage: -0.003143310546875\tRatio: 0.20000000298023224\tReward Loss: 0.02257034368813038\n",
      "Predicted Reward: 196.21246337890625\tCurrent Reward: 196.0\n",
      "down_up\n",
      "Current step: 230\n",
      "Current Loss: 15.579611778259277\n",
      "Action Loss: -0.0008728027460165322\tAdvantage: -0.004364013671875\tRatio: 0.20000000298023224\tReward Loss: 31.160968780517578\n",
      "Predicted Reward: 196.10557556152344\tCurrent Reward: 204.0\n",
      "down_up\n",
      "Current step: 240\n",
      "Current Loss: 27.155000686645508\n",
      "Action Loss: -0.0009368896717205644\tAdvantage: -0.0046844482421875\tRatio: 0.20000000298023224\tReward Loss: 54.31187438964844\n",
      "Predicted Reward: 201.57772827148438\tCurrent Reward: 212.0\n",
      "down_shift\n",
      "Current step: 250\n",
      "Current Loss: 33.27222442626953\n",
      "Action Loss: -0.0008300781482830644\tAdvantage: -0.004150390625\tRatio: 0.20000000298023224\tReward Loss: 66.54611206054688\n",
      "Predicted Reward: 206.46343994140625\tCurrent Reward: 218.0\n",
      "down_up\n",
      "Current step: 260\n",
      "Current Loss: 28.169525146484375\n",
      "Action Loss: -0.0009521484607830644\tAdvantage: -0.0047607421875\tRatio: 0.20000000298023224\tReward Loss: 56.3409538269043\n",
      "Predicted Reward: 215.38482666015625\tCurrent Reward: 226.0\n",
      "down_up\n",
      "Current step: 270\n",
      "Current Loss: 1.6326700448989868\n",
      "Action Loss: -0.00115966796875\tAdvantage: -0.00579833984375\tRatio: 0.20000000298023224\tReward Loss: 3.2676594257354736\n",
      "Predicted Reward: 236.55642700195312\tCurrent Reward: 234.0\n",
      "down_up\n",
      "Current step: 280\n",
      "Current Loss: 13.402430534362793\n",
      "Action Loss: -0.001574707101099193\tAdvantage: -0.00787353515625\tRatio: 0.20000000298023224\tReward Loss: 26.80801010131836\n",
      "Predicted Reward: 236.67770385742188\tCurrent Reward: 244.0\n",
      "down_up\n",
      "Current step: 290\n",
      "Current Loss: 12.93490219116211\n",
      "Action Loss: -0.0019714355003088713\tAdvantage: -0.009857177734375\tRatio: 0.20000000298023224\tReward Loss: 25.873746871948242\n",
      "Predicted Reward: 246.80642700195312\tCurrent Reward: 254.0\n",
      "down_up\n",
      "Current step: 300\n",
      "Current Loss: 29.635536193847656\n",
      "Action Loss: -0.0018524170154705644\tAdvantage: -0.0092620849609375\tRatio: 0.20000000298023224\tReward Loss: 59.274776458740234\n",
      "Predicted Reward: 253.11195373535156\tCurrent Reward: 264.0\n",
      "down_up\n",
      "Current step: 310\n",
      "Current Loss: 20.165727615356445\n",
      "Action Loss: -0.0016845703357830644\tAdvantage: -0.0084228515625\tRatio: 0.20000000298023224\tReward Loss: 40.33482360839844\n",
      "Predicted Reward: 265.01837158203125\tCurrent Reward: 274.0\n",
      "down_up\n",
      "Current step: 320\n",
      "Current Loss: 10.362966537475586\n",
      "Action Loss: -0.0019042969215661287\tAdvantage: -0.009521484375\tRatio: 0.20000000298023224\tReward Loss: 20.7297420501709\n",
      "Predicted Reward: 277.56109619140625\tCurrent Reward: 284.0\n",
      "down_up\n",
      "Current step: 330\n",
      "Current Loss: 29.93744659423828\n",
      "Action Loss: -0.002166748046875\tAdvantage: -0.010833740234375\tRatio: 0.20000000298023224\tReward Loss: 59.87922668457031\n",
      "Predicted Reward: 283.05657958984375\tCurrent Reward: 294.0\n",
      "down_up\n",
      "Current step: 340\n",
      "Current Loss: 1.9042251110076904\n",
      "Action Loss: -0.002716064453125\tAdvantage: -0.013580322265625\tRatio: 0.20000000298023224\tReward Loss: 3.813882350921631\n",
      "Predicted Reward: 297.2381591796875\tCurrent Reward: 300.0\n",
      "down_right\n",
      "Current step: 350\n",
      "Current Loss: 1.457297444343567\n",
      "Action Loss: -0.0034973144065588713\tAdvantage: -0.017486572265625\tRatio: 0.20000000298023224\tReward Loss: 2.9215896129608154\n",
      "Predicted Reward: 303.5827331542969\tCurrent Reward: 306.0\n",
      "down_up\n",
      "Current step: 360\n",
      "Current Loss: 2.4386773109436035\n",
      "Action Loss: -0.00506591796875\tAdvantage: -0.02532958984375\tRatio: 0.20000000298023224\tReward Loss: 4.887486457824707\n",
      "Predicted Reward: 319.1264953613281\tCurrent Reward: 316.0\n",
      "down_up\n",
      "Current step: 370\n",
      "Current Loss: 4.620698928833008\n",
      "Action Loss: -0.006500244140625\tAdvantage: -0.032501220703125\tRatio: 0.20000000298023224\tReward Loss: 9.254398345947266\n",
      "Predicted Reward: 319.69781494140625\tCurrent Reward: 324.0\n",
      "down_up\n",
      "Current step: 380\n",
      "Current Loss: 13.88819694519043\n",
      "Action Loss: -0.006805419921875\tAdvantage: -0.034027099609375\tRatio: 0.20000000298023224\tReward Loss: 27.79000473022461\n",
      "Predicted Reward: 324.5447998046875\tCurrent Reward: 332.0\n",
      "press_x\n",
      "Current step: 390\n",
      "Current Loss: 51.086875915527344\n",
      "Action Loss: -0.0052124024368822575\tAdvantage: -0.02606201171875\tRatio: 0.20000000298023224\tReward Loss: 102.18417358398438\n",
      "Predicted Reward: 325.7042541503906\tCurrent Reward: 340.0\n",
      "down_up\n",
      "Current step: 400\n",
      "Current Loss: 1.5757979154586792\n",
      "Action Loss: -0.0065246582962572575\tAdvantage: -0.032623291015625\tRatio: 0.20000000298023224\tReward Loss: 3.164645195007324\n",
      "Predicted Reward: 345.48419189453125\tCurrent Reward: 348.0\n",
      "down_up\n",
      "Current step: 410\n",
      "Current Loss: -0.0036429716274142265\n",
      "Action Loss: -0.008929443545639515\tAdvantage: -0.044647216796875\tRatio: 0.20000000298023224\tReward Loss: 0.010572943836450577\n",
      "Predicted Reward: 356.1454162597656\tCurrent Reward: 356.0\n",
      "down_up\n",
      "Current step: 420\n",
      "Current Loss: -0.0037884563207626343\n",
      "Action Loss: -0.012237548828125\tAdvantage: -0.061187744140625\tRatio: 0.20000000298023224\tReward Loss: 0.01689818501472473\n",
      "Predicted Reward: 359.816162109375\tCurrent Reward: 360.0\n",
      "down_up\n",
      "Current step: 430\n",
      "Current Loss: 4.079324722290039\n",
      "Action Loss: -0.01633300818502903\tAdvantage: -0.0816650390625\tRatio: 0.20000000298023224\tReward Loss: 8.191315650939941\n",
      "Predicted Reward: 361.95245361328125\tCurrent Reward: 366.0\n",
      "down_up\n",
      "Current step: 440\n",
      "Current Loss: 10.320272445678711\n",
      "Action Loss: -0.01887207105755806\tAdvantage: -0.0943603515625\tRatio: 0.20000000298023224\tReward Loss: 20.67828941345215\n",
      "Predicted Reward: 365.569091796875\tCurrent Reward: 372.0\n",
      "up_left\n",
      "Current step: 450\n",
      "Current Loss: -0.014022961258888245\n",
      "Action Loss: -0.01905517652630806\tAdvantage: -0.09527587890625\tRatio: 0.20000000298023224\tReward Loss: 0.01006443053483963\n",
      "Predicted Reward: 377.8581237792969\tCurrent Reward: 378.0\n",
      "down_up\n",
      "Current step: 460\n",
      "Current Loss: 0.2685389518737793\n",
      "Action Loss: -0.01871948316693306\tAdvantage: -0.093597412109375\tRatio: 0.20000000298023224\tReward Loss: 0.5745168924331665\n",
      "Predicted Reward: 385.0719299316406\tCurrent Reward: 384.0\n",
      "down_up\n",
      "Current step: 470\n",
      "Current Loss: 6.420444965362549\n",
      "Action Loss: -0.01942138746380806\tAdvantage: -0.09710693359375\tRatio: 0.20000000298023224\tReward Loss: 12.879733085632324\n",
      "Predicted Reward: 388.92462158203125\tCurrent Reward: 394.0\n",
      "up_down\n",
      "Current step: 480\n",
      "Current Loss: 107.64077758789062\n",
      "Action Loss: -0.020111083984375\tAdvantage: -0.100555419921875\tRatio: 0.20000000298023224\tReward Loss: 215.32177734375\n",
      "Predicted Reward: 383.248046875\tCurrent Reward: 404.0\n",
      "down_up\n",
      "Current step: 490\n",
      "Current Loss: 19.467453002929688\n",
      "Action Loss: -0.01945800893008709\tAdvantage: -0.0972900390625\tRatio: 0.20000000298023224\tReward Loss: 38.97382354736328\n",
      "Predicted Reward: 405.17120361328125\tCurrent Reward: 414.0\n",
      "down_up\n",
      "Current step: 500\n",
      "Current Loss: 5.0659332275390625\n",
      "Action Loss: -0.01746215857565403\tAdvantage: -0.087310791015625\tRatio: 0.20000000298023224\tReward Loss: 10.166790962219238\n",
      "Predicted Reward: 417.49072265625\tCurrent Reward: 422.0\n",
      "down_up\n",
      "Current step: 510\n",
      "Current Loss: -0.002389276400208473\n",
      "Action Loss: -0.01483764685690403\tAdvantage: -0.074188232421875\tRatio: 0.20000000298023224\tReward Loss: 0.024896740913391113\n",
      "Predicted Reward: 427.77685546875\tCurrent Reward: 428.0\n",
      "down_up\n",
      "Current step: 520\n",
      "Current Loss: 9.535734176635742\n",
      "Action Loss: -0.013946533203125\tAdvantage: -0.069732666015625\tRatio: 0.20000000298023224\tReward Loss: 19.099361419677734\n",
      "Predicted Reward: 431.8194885253906\tCurrent Reward: 438.0\n",
      "down_up\n",
      "Current step: 530\n",
      "Current Loss: 22.33641242980957\n",
      "Action Loss: -0.012347412295639515\tAdvantage: -0.061737060546875\tRatio: 0.20000000298023224\tReward Loss: 44.6975212097168\n",
      "Predicted Reward: 436.54510498046875\tCurrent Reward: 446.0\n",
      "down_up\n",
      "Current step: 540\n",
      "Current Loss: 12.634506225585938\n",
      "Action Loss: -0.011712647043168545\tAdvantage: -0.058563232421875\tRatio: 0.20000000298023224\tReward Loss: 25.292438507080078\n",
      "Predicted Reward: 448.8876953125\tCurrent Reward: 456.0\n",
      "down_up\n",
      "Current step: 550\n",
      "Current Loss: 11.825079917907715\n",
      "Action Loss: -0.01336669921875\tAdvantage: -0.06683349609375\tRatio: 0.20000000298023224\tReward Loss: 23.67689323425293\n",
      "Predicted Reward: 459.11859130859375\tCurrent Reward: 466.0\n",
      "down_right\n",
      "Current step: 560\n",
      "Current Loss: 22.369001388549805\n",
      "Action Loss: -0.013531493954360485\tAdvantage: -0.067657470703125\tRatio: 0.20000000298023224\tReward Loss: 44.76506423950195\n",
      "Predicted Reward: 466.5379638671875\tCurrent Reward: 476.0\n",
      "down_up\n",
      "Current step: 570\n",
      "Current Loss: 1.196927547454834\n",
      "Action Loss: -0.0128173828125\tAdvantage: -0.0640869140625\tRatio: 0.20000000298023224\tReward Loss: 2.419489860534668\n",
      "Predicted Reward: 479.80023193359375\tCurrent Reward: 482.0\n",
      "down_up\n",
      "Current step: 580\n",
      "Current Loss: 1.17194402217865\n",
      "Action Loss: -0.012292481027543545\tAdvantage: -0.06146240234375\tRatio: 0.20000000298023224\tReward Loss: 2.3684730529785156\n",
      "Predicted Reward: 489.82354736328125\tCurrent Reward: 492.0\n",
      "up_z\n",
      "Current step: 590\n",
      "Current Loss: 39.40936279296875\n",
      "Action Loss: -0.01407470740377903\tAdvantage: -0.07037353515625\tRatio: 0.20000000298023224\tReward Loss: 78.84687805175781\n",
      "Predicted Reward: 489.4423828125\tCurrent Reward: 502.0\n",
      "down_up\n",
      "Current step: 600\n",
      "Current Loss: 18.12763214111328\n",
      "Action Loss: -0.013031005859375\tAdvantage: -0.065155029296875\tRatio: 0.20000000298023224\tReward Loss: 36.28132629394531\n",
      "Predicted Reward: 503.48162841796875\tCurrent Reward: 512.0\n",
      "down_up\n",
      "Current step: 610\n",
      "Current Loss: 169.93316650390625\n",
      "Action Loss: -0.009179688058793545\tAdvantage: -0.0458984375\tRatio: 0.20000000298023224\tReward Loss: 339.88470458984375\n",
      "Predicted Reward: 495.9276123046875\tCurrent Reward: 522.0\n",
      "press_x\n",
      "Current step: 620\n",
      "Current Loss: 0.30287033319473267\n",
      "Action Loss: -0.009838867001235485\tAdvantage: -0.0491943359375\tRatio: 0.20000000298023224\tReward Loss: 0.6254184246063232\n",
      "Predicted Reward: 531.118408203125\tCurrent Reward: 530.0\n",
      "down_up\n",
      "Current step: 630\n",
      "Current Loss: 8.321714401245117\n",
      "Action Loss: -0.015307617373764515\tAdvantage: -0.0765380859375\tRatio: 0.20000000298023224\tReward Loss: 16.674043655395508\n",
      "Predicted Reward: 534.2252197265625\tCurrent Reward: 540.0\n",
      "down_up\n",
      "Current step: 640\n",
      "Current Loss: 31.59595489501953\n",
      "Action Loss: -0.01755371131002903\tAdvantage: -0.0877685546875\tRatio: 0.20000000298023224\tReward Loss: 63.22701644897461\n",
      "Predicted Reward: 538.7548217773438\tCurrent Reward: 550.0\n",
      "down_left\n",
      "Current step: 650\n",
      "Current Loss: 5.749119281768799\n",
      "Action Loss: -0.015100098215043545\tAdvantage: -0.07550048828125\tRatio: 0.20000000298023224\tReward Loss: 11.528438568115234\n",
      "Predicted Reward: 555.1982421875\tCurrent Reward: 560.0\n",
      "down_up\n",
      "Current step: 660\n",
      "Current Loss: 0.027821682393550873\n",
      "Action Loss: -0.009448242373764515\tAdvantage: -0.0472412109375\tRatio: 0.20000000298023224\tReward Loss: 0.07453984767198563\n",
      "Predicted Reward: 570.3861083984375\tCurrent Reward: 570.0\n",
      "down_up\n",
      "Current step: 670\n",
      "Current Loss: 7.450896739959717\n",
      "Action Loss: -0.01123046875\tAdvantage: -0.05615234375\tRatio: 0.20000000298023224\tReward Loss: 14.924254417419434\n",
      "Predicted Reward: 574.53662109375\tCurrent Reward: 580.0\n",
      "down_up\n",
      "Current step: 680\n",
      "Current Loss: 108.92862701416016\n",
      "Action Loss: -0.01214599609375\tAdvantage: -0.06072998046875\tRatio: 0.20000000298023224\tReward Loss: 217.8815460205078\n",
      "Predicted Reward: 569.1250610351562\tCurrent Reward: 590.0\n",
      "down_up\n",
      "Current step: 690\n",
      "Current Loss: 0.04751749336719513\n",
      "Action Loss: -0.009289550594985485\tAdvantage: -0.04644775390625\tRatio: 0.20000000298023224\tReward Loss: 0.11361408978700638\n",
      "Predicted Reward: 598.4766845703125\tCurrent Reward: 598.0\n",
      "down_up\n",
      "Current step: 700\n",
      "Current Loss: 0.2542468011379242\n",
      "Action Loss: -0.015222168527543545\tAdvantage: -0.07611083984375\tRatio: 0.20000000298023224\tReward Loss: 0.5389379262924194\n",
      "Predicted Reward: 604.9617919921875\tCurrent Reward: 606.0\n",
      "down_right\n",
      "Current step: 710\n",
      "Current Loss: 59.69671630859375\n",
      "Action Loss: -0.02003173902630806\tAdvantage: -0.10015869140625\tRatio: 0.20000000298023224\tReward Loss: 119.4334945678711\n",
      "Predicted Reward: 600.544677734375\tCurrent Reward: 616.0\n",
      "down_up\n",
      "Current step: 720\n",
      "Current Loss: 17.42094612121582\n",
      "Action Loss: -0.02104492299258709\tAdvantage: -0.105224609375\tRatio: 0.20000000298023224\tReward Loss: 34.88398361206055\n",
      "Predicted Reward: 617.6472778320312\tCurrent Reward: 626.0\n",
      "down_up\n",
      "Current step: 730\n",
      "Current Loss: 0.02363281138241291\n",
      "Action Loss: -0.02086181752383709\tAdvantage: -0.10430908203125\tRatio: 0.20000000298023224\tReward Loss: 0.0889892578125\n",
      "Predicted Reward: 634.421875\tCurrent Reward: 634.0\n",
      "down_up\n",
      "Current step: 740\n",
      "Current Loss: 2.44380784034729\n",
      "Action Loss: -0.02159423939883709\tAdvantage: -0.10797119140625\tRatio: 0.20000000298023224\tReward Loss: 4.930804252624512\n",
      "Predicted Reward: 636.8596801757812\tCurrent Reward: 640.0\n",
      "down_up\n",
      "Current step: 750\n",
      "Current Loss: 304.6773681640625\n",
      "Action Loss: -0.02156982384622097\tAdvantage: -0.10784912109375\tRatio: 0.20000000298023224\tReward Loss: 609.3978881835938\n",
      "Predicted Reward: 611.0887451171875\tCurrent Reward: 646.0\n",
      "down_up\n",
      "Current step: 760\n",
      "Current Loss: 9.81672477722168\n",
      "Action Loss: -0.02055664174258709\tAdvantage: -0.102783203125\tRatio: 0.20000000298023224\tReward Loss: 19.674562454223633\n",
      "Predicted Reward: 660.2728881835938\tCurrent Reward: 654.0\n",
      "down_up\n",
      "Current step: 770\n",
      "Current Loss: -0.011767014861106873\n",
      "Action Loss: -0.02142333984375\tAdvantage: -0.10711669921875\tRatio: 0.20000000298023224\tReward Loss: 0.019312649965286255\n",
      "Predicted Reward: 660.196533203125\tCurrent Reward: 660.0\n",
      "press_x\n",
      "Current step: 780\n",
      "Current Loss: 65.86133575439453\n",
      "Action Loss: -0.02244873158633709\tAdvantage: -0.11224365234375\tRatio: 0.20000000298023224\tReward Loss: 131.76756286621094\n",
      "Predicted Reward: 653.7662353515625\tCurrent Reward: 670.0\n",
      "down_up\n",
      "Current step: 790\n",
      "Current Loss: 22.319232940673828\n",
      "Action Loss: -0.02231445349752903\tAdvantage: -0.111572265625\tRatio: 0.20000000298023224\tReward Loss: 44.6830940246582\n",
      "Predicted Reward: 670.546630859375\tCurrent Reward: 680.0\n",
      "down_up\n",
      "Current step: 800\n",
      "Current Loss: 0.06579075008630753\n",
      "Action Loss: -0.02176513709127903\tAdvantage: -0.10882568359375\tRatio: 0.20000000298023224\tReward Loss: 0.1751117706298828\n",
      "Predicted Reward: 689.408203125\tCurrent Reward: 690.0\n",
      "down_up\n",
      "Current step: 810\n",
      "Current Loss: 5.622712135314941\n",
      "Action Loss: -0.02197265625\tAdvantage: -0.10986328125\tRatio: 0.20000000298023224\tReward Loss: 11.289369583129883\n",
      "Predicted Reward: 693.248291015625\tCurrent Reward: 698.0\n",
      "down_up\n",
      "Current step: 820\n",
      "Current Loss: 16.994293212890625\n",
      "Action Loss: -0.0223388671875\tAdvantage: -0.1116943359375\tRatio: 0.20000000298023224\tReward Loss: 34.03326416015625\n",
      "Predicted Reward: 699.749755859375\tCurrent Reward: 708.0\n",
      "down_up\n",
      "Current step: 830\n",
      "Current Loss: 646.1959228515625\n",
      "Action Loss: -0.02150878868997097\tAdvantage: -0.1075439453125\tRatio: 0.20000000298023224\tReward Loss: 1292.434814453125\n",
      "Predicted Reward: 667.1583862304688\tCurrent Reward: 718.0\n",
      "down_up\n",
      "Current step: 840\n",
      "Current Loss: 24.484987258911133\n",
      "Action Loss: -0.01823730580508709\tAdvantage: -0.0911865234375\tRatio: 0.20000000298023224\tReward Loss: 49.00645065307617\n",
      "Predicted Reward: 737.900146484375\tCurrent Reward: 728.0\n",
      "down_up\n",
      "Current step: 850\n",
      "Current Loss: 23.95095443725586\n",
      "Action Loss: -0.01862793043255806\tAdvantage: -0.0931396484375\tRatio: 0.20000000298023224\tReward Loss: 47.93916320800781\n",
      "Predicted Reward: 728.208251953125\tCurrent Reward: 738.0\n",
      "down_up\n",
      "Current step: 860\n",
      "Current Loss: 69.23760986328125\n",
      "Action Loss: -0.01690673828125\tAdvantage: -0.08453369140625\tRatio: 0.20000000298023224\tReward Loss: 138.509033203125\n",
      "Predicted Reward: 729.3561401367188\tCurrent Reward: 746.0\n",
      "down_up\n",
      "Current step: 870\n",
      "Current Loss: 18.81548500061035\n",
      "Action Loss: -0.011608886532485485\tAdvantage: -0.05804443359375\tRatio: 0.20000000298023224\tReward Loss: 37.6541862487793\n",
      "Predicted Reward: 764.6780395507812\tCurrent Reward: 756.0\n",
      "down_up\n",
      "Current step: 880\n",
      "Current Loss: 0.029418373480439186\n",
      "Action Loss: -0.02059326134622097\tAdvantage: -0.10296630859375\tRatio: 0.20000000298023224\tReward Loss: 0.10002326965332031\n",
      "Predicted Reward: 763.552734375\tCurrent Reward: 764.0\n",
      "down_up\n",
      "Current step: 890\n",
      "Current Loss: 168.6135711669922\n",
      "Action Loss: -0.02402343787252903\tAdvantage: -0.1201171875\tRatio: 0.20000000298023224\tReward Loss: 337.2751770019531\n",
      "Predicted Reward: 748.0278930664062\tCurrent Reward: 774.0\n",
      "down_up\n",
      "Current step: 900\n",
      "Current Loss: 0.19108796119689941\n",
      "Action Loss: -0.02496337890625\tAdvantage: -0.12481689453125\tRatio: 0.20000000298023224\tReward Loss: 0.43210268020629883\n",
      "Predicted Reward: 781.0703735351562\tCurrent Reward: 782.0\n",
      "down_up\n",
      "Current step: 910\n",
      "Current Loss: 3.657785415649414\n",
      "Action Loss: -0.0252685546875\tAdvantage: -0.1263427734375\tRatio: 0.20000000298023224\tReward Loss: 7.366107940673828\n",
      "Predicted Reward: 795.8382568359375\tCurrent Reward: 792.0\n",
      "down_up\n",
      "Current step: 920\n",
      "Current Loss: 28.5302734375\n",
      "Action Loss: -0.025390625\tAdvantage: -0.126953125\tRatio: 0.20000000298023224\tReward Loss: 57.111328125\n",
      "Predicted Reward: 791.3125\tCurrent Reward: 802.0\n",
      "down_right\n",
      "Current step: 930\n",
      "Current Loss: 62.942134857177734\n",
      "Action Loss: -0.02543945424258709\tAdvantage: -0.127197265625\tRatio: 0.20000000298023224\tReward Loss: 125.93515014648438\n",
      "Predicted Reward: 796.1295776367188\tCurrent Reward: 812.0\n",
      "down_shift\n",
      "Current step: 940\n",
      "Current Loss: 8.012195587158203\n",
      "Action Loss: -0.02550048939883709\tAdvantage: -0.12750244140625\tRatio: 0.20000000298023224\tReward Loss: 16.07539176940918\n",
      "Predicted Reward: 827.670166015625\tCurrent Reward: 822.0\n",
      "down_up\n",
      "Current step: 950\n",
      "Current Loss: 1.8811877965927124\n",
      "Action Loss: -0.02550048939883709\tAdvantage: -0.12750244140625\tRatio: 0.20000000298023224\tReward Loss: 3.8133766651153564\n",
      "Predicted Reward: 827.2383422851562\tCurrent Reward: 830.0\n",
      "down_up\n",
      "Current step: 960\n",
      "Current Loss: 87.27159881591797\n",
      "Action Loss: -0.0255126953125\tAdvantage: -0.1275634765625\tRatio: 0.20000000298023224\tReward Loss: 174.59422302246094\n",
      "Predicted Reward: 821.3134155273438\tCurrent Reward: 840.0\n",
      "down_up\n",
      "Current step: 970\n",
      "Current Loss: 2.6004538536071777\n",
      "Action Loss: -0.0255126953125\tAdvantage: -0.1275634765625\tRatio: 0.20000000298023224\tReward Loss: 5.2519330978393555\n",
      "Predicted Reward: 846.759033203125\tCurrent Reward: 850.0\n",
      "down_up\n",
      "Current step: 980\n",
      "Current Loss: 1.760031819343567\n",
      "Action Loss: -0.0255126953125\tAdvantage: -0.1275634765625\tRatio: 0.20000000298023224\tReward Loss: 3.571089029312134\n",
      "Predicted Reward: 858.6724853515625\tCurrent Reward: 856.0\n",
      "down_up\n",
      "Current step: 990\n",
      "Current Loss: 23.199310302734375\n",
      "Action Loss: -0.0255126953125\tAdvantage: -0.1275634765625\tRatio: 0.20000000298023224\tReward Loss: 46.44964599609375\n",
      "Predicted Reward: 852.361572265625\tCurrent Reward: 862.0\n",
      "down_up\n",
      "Current step: 1000\n",
      "Current Loss: 113.28311920166016\n",
      "Action Loss: -0.02552490308880806\tAdvantage: -0.12762451171875\tRatio: 0.20000000298023224\tReward Loss: 226.61729431152344\n",
      "Predicted Reward: 848.710693359375\tCurrent Reward: 870.0\n",
      "up_up\n"
     ]
    }
   ],
   "source": [
    "reward = torch.zeros((1, 1), device=device) # Cumulative reward\n",
    "steps = 0\n",
    "save_point = 10 # Also optimization point\n",
    "#uncertainty_factor = 0.9 # Also known as gamma or discount factor\n",
    "action_grads = []\n",
    "reward_grads = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, eps=1e-8) # 1e-4 is a common LR. But 1e-6 is also used by RainbowDQN and it's the best one for HierNet\n",
    "# Note: PPO also uses epsilon = 1e-5, but it seems that eps=1e-7 or 1e-8 are better.\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, gamma=0.1) # The learning rate should decay linearly until it vanishes\n",
    "\n",
    "reward_loss = torch.nn.HuberLoss(delta=100)\n",
    "actions_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)\n",
    "\n",
    "\n",
    "#while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "while steps < 1000:\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "\n",
    "    reward_input = reward.clone() # To avoid issues with inplace operations\n",
    "\n",
    "    possible_actions, true_action, predicted_reward, expected_reward = model(frame, reward_input)\n",
    "\n",
    "    del frame\n",
    "\n",
    "    command = dataset.get_command(int(true_action.detach().cpu().item()))\n",
    "\n",
    "    model.execute_command(command)\n",
    "\n",
    "    life = dataset.get_consequences(849, 400, 498-400, 904-849, tesseract_config='--psm 8')\n",
    "\n",
    "    life = preprocess_Jigoku(life)\n",
    "\n",
    "    if life <= 1:\n",
    "\n",
    "        reward -= 1.\n",
    "\n",
    "    else:\n",
    "\n",
    "        reward += 1.\n",
    "\n",
    "    del life\n",
    "\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    advantage = predicted_reward - expected_reward\n",
    "    \n",
    "    try:\n",
    "        one_hot = torch.zeros_like(possible_actions, device=device)\n",
    "        one_hot[0, possible_actions.argmax()] = 1.\n",
    "        possible_actions = possible_actions * one_hot\n",
    "        possible_actions = torch.sum(possible_actions, dim=-1)\n",
    "        \n",
    "        ratio = possible_actions/previous_possible_actions\n",
    "\n",
    "        ratio = torch.clamp(ratio, min=0.2, max=0.2)\n",
    "\n",
    "        surrogate_loss = ratio * advantage\n",
    "\n",
    "        value_loss = reward_loss(predicted_reward, reward) # Consider computing the advantage and the imediate reward\n",
    "\n",
    "        total_loss = surrogate_loss + (value_loss * 0.5)\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        action_grads.append(torch.mean(model.neuron_vectorizer.weight.grad))\n",
    "        reward_grads.append(torch.mean(model.pred_reward.weight.grad))\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    previous_possible_actions = possible_actions.detach()\n",
    "\n",
    "    steps += 1\n",
    "    \n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Current Loss: {total_loss.item()}\")\n",
    "        print(f\"Action Loss: {surrogate_loss.item()}\\tAdvantage: {advantage.item()}\\tRatio: {ratio.item()}\\tReward Loss: {value_loss.item()}\")\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward.item()}\")\n",
    "        print(command)\n",
    "\n",
    "        '''torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")'''\n",
    "\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(model.pred_reward.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[113.2831]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9995], device='cuda:0')\n",
      "tensor([0.9995], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(previous_possible_actions)\n",
    "print(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000], device='cuda:0', grad_fn=<ClampBackward1>)\n",
      "tensor([[-0.1276]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor([[-0.0255]], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(ratio)\n",
    "print(advantage)\n",
    "print(surrogate_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].plot(action_grads)\n",
    "ax[1].plot(reward_grads)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

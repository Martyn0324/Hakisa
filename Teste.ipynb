{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "#import pyautogui\n",
    "import keyboard\n",
    "import mouse\n",
    "import pytesseract\n",
    "from time import sleep\n",
    "import winsound\n",
    "from re import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory efficient loop\n",
    "\n",
    "with mss() as sct:\n",
    "    #filename = sct.shot()\n",
    "    monitor = {\"top\": 40, \"left\": 0, \"width\": 800, \"height\": 640} # Memory efficiency.\n",
    "    data = sct.grab(monitor)\n",
    "    data = Image.frombytes(\"RGB\", data.size, data.bgra, 'raw', 'BGRX')\n",
    "    data = data.convert(\"P\")\n",
    "    data = np.array(data)\n",
    "    #data = torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sleep(3)\n",
    "\n",
    "#\"Score\": (180, 1, 213, 249)\n",
    "\n",
    "with mss() as sct:\n",
    "    monitor = {\"top\": 180, \"left\": 1, \"width\": 249-1, \"height\": 213-180} # Memory efficiency.\n",
    "    data = sct.grab(monitor)\n",
    "    data = Image.frombytes(\"RGB\", data.size, data.bgra, 'raw', 'BGRX')\n",
    "    data = data.convert(\"P\")\n",
    "    data = np.array(data)\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "plt.imshow(data)\n",
    "plt.show()\n",
    "\n",
    "consequence = pytesseract.image_to_string(data, config='--psm 6')\n",
    "print(consequence)\n",
    "\n",
    "consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence)\n",
    "\n",
    "print(consequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing Screen region capture and OCR\n",
    "\n",
    "from cv2 import adaptiveThreshold, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY\n",
    "\n",
    "sleep(3)\n",
    "\n",
    "with mss() as sct:\n",
    "    monitor = {\"top\": 180, \"left\": 1, \"width\": 249-1, \"height\": 213-180} # Memory efficiency.\n",
    "    data = sct.grab(monitor)\n",
    "    data = Image.frombytes(\"RGB\", data.size, data.bgra, 'raw', 'BGRX')\n",
    "    data = data.convert(\"P\")\n",
    "    data = np.array(data)\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "plt.imshow(data)\n",
    "plt.show()\n",
    "\n",
    "consequence = pytesseract.image_to_string(data, config='--psm 6')\n",
    "print(consequence)\n",
    "\n",
    "consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence)\n",
    "\n",
    "print(consequence)\n",
    "\n",
    "datathresh = adaptiveThreshold(data,255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,63,20)\n",
    "\n",
    "consequence = pytesseract.image_to_string(datathresh, config='--psm 6')\n",
    "print(consequence)\n",
    "\n",
    "#replace w --> 1, f --> / ---> y ---> 1 ---> e --> 2\n",
    "consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence)\n",
    "\n",
    "print(consequence)\n",
    "\n",
    "plt.imshow(datathresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 45800.\n",
      "\n",
      "45800.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "datathresh = adaptiveThreshold(data,255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,11,5) # 11, 5\n",
    "\n",
    "consequence = pytesseract.image_to_string(datathresh, config='--psm 8')\n",
    "print(consequence)\n",
    "\n",
    "#replace w --> 1, f --> / ---> y ---> 1 ---> e --> 2\n",
    "consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence)\n",
    "\n",
    "print(consequence)\n",
    "\n",
    "plt.imshow(datathresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.3) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-sn_xpupm\\opencv\\modules\\imgproc\\src\\thresh.cpp:1676: error: (-215:Assertion failed) src.type() == CV_8UC1 in function 'cv::adaptiveThreshold'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2372/2661678619.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcv2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madaptiveThreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mADAPTIVE_THRESH_GAUSSIAN_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTHRESH_BINARY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mthresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madaptiveThreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mADAPTIVE_THRESH_GAUSSIAN_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTHRESH_BINARY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m171\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.3) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-sn_xpupm\\opencv\\modules\\imgproc\\src\\thresh.cpp:1676: error: (-215:Assertion failed) src.type() == CV_8UC1 in function 'cv::adaptiveThreshold'\n"
     ]
    }
   ],
   "source": [
    "from cv2 import adaptiveThreshold, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY\n",
    "\n",
    "thresh = adaptiveThreshold(data,255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,171,13)\n",
    "\n",
    "plt.imshow(thresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 800, 4)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape) # (Height, Width, Channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a single dictionary for every single possible command causes us to deal with too many data at once\n",
    "# A simple game which uses the mouse extensively, like Bullet Heaven 2, can have around 6 million itens in its input mapping dict.\n",
    "# We had a similar problem in SerpentAI...and we'll solve this in the same way as we did with Serpent...or quite.\n",
    "\n",
    "# In Serpent for Bullet Heaven, we made the agent get, first, the coordinate X where it would move the mouse and, then, another iteration to get Y.\n",
    "# We could do a similar thing with Hakisa, but, instead of making another entire iteration,\n",
    "# we could simply use a Linear Layer which would generate 2 outputs. Embeddings could also be an option, but I don't see any advantage here.\n",
    "\n",
    "# In this paper https://arxiv.org/pdf/2209.11553.pdf , in MiniAlphaStar, they categorize each eaction between types and, in Supervised Learning stage,\n",
    "# they pass those types into a Cross Entropy Function. They also use AutoRegressive Embedding...which I don't know exactly how we can do it.\n",
    "\n",
    "# Using this same idea, we could classify our commands into keyboard press command, keyboard up command, move(mouse) command, click command and right click command, etc.\n",
    "# and also classify the actions, which would be the keys to be pressed and the window coordinates.\n",
    "\n",
    "# Hakisa would have to correctly predict the command type she must use and, given the predicted command type, predict the action she must perform.\n",
    "# So, Hakisa will now have 3 outputs: command type, action1, action2, where:\n",
    "# command type: keyboard, move(mouse), click, right click\n",
    "# action1: keyUp, keyDown, press(keyboard), X(mouse)\n",
    "# action2: 'a', 'z', 'shift'(keyboard), Y(mouse)\n",
    "\n",
    "# Since command type will have a small dimension, Cross Entropy will be no problem.\n",
    "\n",
    "# As for Embedding Layers: they could be used to relate a command to a specific context, given by the input frame.\n",
    "# Embedding Layers take an input with N_possibilities size, and outputs a vector which can have 1 dimension, 2, 3...\n",
    "# In NLP, N_possibilities is the size of our vocabulary, and the vector will determine the location of that input in the vector matrix.\n",
    "# Usually, in NLP, vector have the same size as the context. When analyzing 3 words, vector will have 3 dimensions.\n",
    "# Since Hakisa is basically a Feature Extractor net, the context can be the result of such extraction.\n",
    "# Image frame(input) ------------> Features ----------> Embedding ----> Context ---------> Output.\n",
    "# Simpler games will have fewer features and fewer possibilities of context, while those more complex will have more possible contexts.\n",
    "# Ex: Jigoku Kisetsukan = beginning of level and no enemies, window full of projectiles, being hit... //\n",
    "# Starcraft 2 = Resources lacking and have to collect, need reconaissance, being attacked, plan an attack, counterattack...\n",
    "# However, an Embedding layer will only accept encoded inputs(in Pytorch, index-encoded), so only integers. So use torch.round()\n",
    "# This could also cause the problem that features that are subtly different will be treated as equal. Maybe backpropagation can change this with time?\n",
    "\n",
    "\n",
    "# The new Study loss would now be: Cross Entropy Loss between command types + MSELoss action1 + MSELoss action 2\n",
    "\n",
    "# With those modifications, an input map which had 6 million itens will now be distributed between 3 input maps with 3, 1920 and 1080 items respectively.\n",
    "# The number of possible commands is still 6 million, but now those possibilities are somehow \"chunked\". We don't have a single dictionary with 6 million items anymore.\n",
    "# and now will be much easier for Hakisa to get the correct value for a dict with scaled values, generating\n",
    "# a wider range of outputs during exploration and maybe helping in the Play Mode.\n",
    "# It'll also be easier to fit KNN into dictionaries, avoiding the necessity of using PaperSpace or Google Colabs just to fit KNN and download it.\n",
    "\n",
    "# Since we'll use cross entropy for command types, we don't need to convert it to a dictionary. Thus, KNN can be discarded here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When encoding to vector, it might be useful to use a separate model that will vectorizer the input mappings according to the current game state,\n",
    "that is, a model that will extract features on the input frames, concatenate it into the one-hot encoded input mapping and pass it through 1 or 2 linear layers to\n",
    "generate our vector.\n",
    "\n",
    "However, this might only be useful when we're dealing with ready-made data, that is: input frames that are correctly attached to the \"correct\" input mappings(both will be used as inputs).\n",
    "If we don't have that data available, Hakisa will generate random input mappings, which means the input frames(the context) will be attached to wrong responses.\n",
    "\n",
    "Since vectorization is usually more related to NLP, I suppose that we could compare this situation to training a vectorizer model on phrases that makes no sense. If the vectorizer receives the sentence \"School bright keyboard Europe\", it'll make the mistake of relating \"school\" to \"bright\", which is nonsense. This would be the case if we train a vectorizer model on Hakisa's random input mappings.\n",
    "\n",
    "However, if we have a ready-made data, which could be the sentence \"At school, we had computing class, but my computer keyboard was broken\", our vectorizer would correctly relate the word \"keyboard\" to \"computer\", or \"broken\". It could relate \"keyboard\" to \"school\", but it'll also relate it to \"computing\" and \"class\".\n",
    "I presume this would be the case if we used a ready made data where in the state A, the command must be \"Jump\".\n",
    "\n",
    "Also, in the beginning, I was dealing with vectors with a single dimension. This might not be effective, depending on how many types of actions you have.\n",
    "\n",
    "If we're playing Jigoku Kisetsukan, which has just keyboard actions and movement and combat actions, we could think of our embedding matrix as being just a single x-axis,\n",
    "where the right side stores movement actions, and the left stores combat actions. Positive actions = movement; Negative actions = combat.\n",
    "However, if we're playing, for example, Dark Souls 3, we'll have mouse actions and keyboard actions, and some mouse actions aren't related directly to combat\n",
    "(healing, applying magical buffs), while some keyboard actions are also related to combat(strong attack, estus?). Some commands even would require a combination of a mouse\n",
    "command and a keyboard command(strong attacks = shift + click). In this case, we would need an embedding matrix with much more axis, more dimensions. An axis could be \n",
    "the combat actions for mouse, another would be the combat actions for keyboard, a third would be the mix of both keyboard and mouse, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Generates input maps and commands for Hakisa.\n",
    "\n",
    "    Remember: command_types = list of strings, actions1 and 2 = list of strings(keyboard), X coordinates or None(mouse)\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        command_types = None,\n",
    "        actions1 = None,\n",
    "        actions2 = None,\n",
    "        explore_train_steps=1000,\n",
    "        memory_size=100,\n",
    "        top=0,\n",
    "        left=0,\n",
    "        width=1920,\n",
    "        height=1080,\n",
    "        resize=None\n",
    "    ):\n",
    "\n",
    "        # Window resolutions for the screen grabber\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.resize = resize # For reducing the images. Must be a tuple (Height, Width)\n",
    "\n",
    "        self.steps = explore_train_steps\n",
    "\n",
    "        self.data = None # This will be created during training. However, it's possible to load a ready-made data for training.\n",
    "\n",
    "        # Initially, we'll be using lists. After our vector embedding has been properly trained, we'll create a dictionary\n",
    "        # of input mappings with it.\n",
    "\n",
    "        self.command_type = command_types\n",
    "        self.actions1 = actions1\n",
    "        self.actions2 = actions2\n",
    "\n",
    "        self.encoded_command_type = to_categorical(np.arange(0, len(command_types)), len(command_types))\n",
    "        self.encoded_command_type = torch.from_numpy(self.encoded_command_type) # Will be used by Hakisa, as it can be used with softmax without generating big outputs.\n",
    "        \n",
    "        #self.encoded_actions1 = self._encode4vec(actions1) # Will be used to train the vector embedding.\n",
    "        #self.encoded_actions2 = self._encode4vec(actions2)\n",
    "\n",
    "        self.key_actions1 = None # Vectors for each action1\n",
    "        self.key_actions2 = None # Vectors for each action2\n",
    "        # command_type doesn't need a vector library, as it can be used with softmax in Hakisa without generating big outputs.\n",
    "\n",
    "        #self.command_type = command_types\n",
    "        #self.actions1 = self._create_commands_dictionary(input_maps=actions1)\n",
    "        #self.actions2 = self._create_commands_dictionary(input_maps=actions2)\n",
    "\n",
    "        self.knn_actions1 = None # Where we'll store our fitted KNN\n",
    "        self.knn_actions2 = None\n",
    "\n",
    "        #self.knn_actions1 = self._fit_knn(self.actions1)\n",
    "        #print(\"KNN fitted in actions 1\")\n",
    "        #self.knn_actions2 = self._fit_knn(self.actions2)\n",
    "        #print(\"KNN fitted in actions 2\\nAll action maps have been properly fitted by their respective KNN algorithm\")\n",
    "\n",
    "        #self.key_actions1 = actions1 # For efficiency in each step\n",
    "        #self.key_actions2 = actions2\n",
    "\n",
    "        self.labels = None # Used for studying\n",
    "        self.rewards = None # Also used for studying.\n",
    "\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = []\n",
    "\n",
    "\n",
    "    # Pytorch's Dataset functions will only be used in Studying mode\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        inputs = self.data[idx]\n",
    "        encoded_command_type = self.encoded_command_type[idx]\n",
    "        encoded_actions1 = self.encoded_actions1[idx]\n",
    "        encoded_actions2 = self.encoded_actions2[idx]\n",
    "        #labels = self.labels[idx]\n",
    "        #rewards = self.rewards[idx]\n",
    "\n",
    "        #return encoded_actions, inputs, labels, rewards\n",
    "        return inputs, encoded_command_type, encoded_actions1, encoded_actions2\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def _grab_frame(self):\n",
    "        # Unfortunately, this whole operation takes about 0.6 seconds, so we'll probably have to deal with a single frame each 1~3 seconds.\n",
    "        with mss() as sct:\n",
    "            frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "            frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if self.resize:\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame = np.array(frame, dtype=np.float32)\n",
    "\n",
    "            frame = torch.from_numpy(frame)\n",
    "        \n",
    "        frame = frame.view(1, frame.size(2), frame.size(0), frame.size(1)).to(device) # (Batch, Channels, Height, Width)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def create_commands_dictionary(self, map2vec_model):\n",
    "\n",
    "        map2vec_model.evaluate = True\n",
    "\n",
    "        # I don't really know how we could handle vector dimensions\n",
    "\n",
    "        dictionary_actions1 = {}\n",
    "        dictionary_actions2 = {}\n",
    "\n",
    "        empty1 = torch.empty_like(self.encoded_actions1, device=device) # The vectorizer demands both actions as input, but they're vectorized independently.\n",
    "        empty2 = torch.empty_like(self.encoded_actions2, device=device)\n",
    "\n",
    "        empty_frame = torch.empty((1, 400*5*5), device=device) # The vectorizer requires a context as input, but for evaluation this isn't necessary.\n",
    "\n",
    "        empty_type = torch.empty_like(self.encoded_command_type, device=device)\n",
    "\n",
    "        for i in range(len(self.actions1)):\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                output, _ = map2vec_model(empty_frame, empty_type[0].unsqueeze(0), self.encoded_actions1[i].unsqueeze(0), empty2[0].unsqueeze(0))\n",
    "\n",
    "                output = output.view(-1)\n",
    "\n",
    "                vector = output[torch.argmax(output)].item()\n",
    "            \n",
    "            dictionary_actions1[self.actions1[i]] = vector\n",
    "\n",
    "        self.key_actions1 = dictionary_actions1\n",
    "\n",
    "        del dictionary_actions1\n",
    "\n",
    "        for i in range(len(self.actions2)):\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                _, output = map2vec_model(empty_frame, empty_type[0].unsqueeze(0), empty1[0].unsqueeze(0), self.encoded_actions2[i].unsqueeze(0))\n",
    "\n",
    "                output = output.view(-1)\n",
    "\n",
    "                vector = output[torch.argmax(output)].item()\n",
    "\n",
    "            dictionary_actions2[self.actions2[i]] = vector\n",
    "\n",
    "        self.key_actions2 = dictionary_actions2\n",
    "\n",
    "        del dictionary_actions2\n",
    "\n",
    "        print(f\"Dict input maps created successfully!\\nActions 1 dict length: {len(self.key_actions1)}\\nActions 2 dict length: {len(self.key_actions2)}\")\n",
    "\n",
    "        self.knn_actions1 = self._fit_knn(self.key_actions1)\n",
    "        self.knn_actions2 = self._fit_knn(self.key_actions2)\n",
    "\n",
    "        print(\"All action maps have been properly fitted by their respective KNN algorithm\")\n",
    "\n",
    "    def _fit_knn(self, dictionary):\n",
    "        \n",
    "        values = list(dictionary.values())\n",
    "\n",
    "        values = np.array(values).reshape(-1,1)\n",
    "\n",
    "        knn = KNN(n_neighbors=1, algorithm='kd_tree').fit(values)\n",
    "\n",
    "        del values\n",
    "\n",
    "        return knn\n",
    "        \n",
    "\n",
    "    def get_command(self, cmd_type, action1, action2, exploration=False):\n",
    "        '''\n",
    "        Hakisa's output: (command_type, action1, action2) ----> (key, Down, z) or (click, 100, 60)\n",
    "        command_type is the argmax output from a logsoftmax function and will be used as index for its respective list.\n",
    "        action1 and action 2 are both floats and will be passed through KNN in order to get the proper command.\n",
    "        '''\n",
    "\n",
    "        if exploration:\n",
    "\n",
    "            cmd_type = self.command_type[cmd_type.item()]\n",
    "            action1 = self.actions1[action1.item()]\n",
    "            action2 = self.actions2[action2.item()]\n",
    "\n",
    "            command = (cmd_type, action1, action2)\n",
    "\n",
    "            return command\n",
    "\n",
    "        if cmd_type.ndim > 1:\n",
    "\n",
    "            cmd_type = np.argmax(cmd_type, 1).item()\n",
    "        \n",
    "        else:\n",
    "            cmd_type = cmd_type.astype(int).item()\n",
    "\n",
    "        cmd_type = self.command_type[cmd_type]\n",
    "\n",
    "        _, index = self.knn_actions1.kneighbors(action1)\n",
    "\n",
    "        for subarray in index:\n",
    "            for i in subarray:\n",
    "                action1 = self.actions1[i]\n",
    "\n",
    "        _, index = self.knn_actions2.kneighbors(action2)\n",
    "\n",
    "        for subarray in index:\n",
    "            for i in subarray:\n",
    "                action2 = self.actions2[i]\n",
    "        \n",
    "        del index, subarray, i\n",
    "\n",
    "        command = (cmd_type, action1, action2)\n",
    "\n",
    "        del cmd_type, action1, action2\n",
    "\n",
    "        return command\n",
    "\n",
    "    def get_consequences(self, top, left, width, height, togray=False, threshold=False, thresh_gauss=171, thresh_C=13, tesseract_config='--psm 8'):\n",
    "        '''\n",
    "        Used after Hakisa performed an input, in order to get its consequences(ex: score change, bombs, kills, deaths...).\n",
    "        Returns a string according to Tesseract's OCR.\n",
    "        '''\n",
    "\n",
    "        with mss() as sct:\n",
    "            consequence = sct.grab(monitor={\"top\": top, \"left\": left, \"width\": width, \"height\": height})\n",
    "\n",
    "            consequence = Image.frombytes(\"RGB\", consequence.size, consequence.bgra, 'raw', 'BGRX')\n",
    "\n",
    "        if togray is True:\n",
    "\n",
    "            consequence = consequence.convert(\"P\") # Sometimes, simply converting to grayscale is enough\n",
    "\n",
    "            if threshold is True:\n",
    "                if \"ADAPTIVE_THRESH_GAUSSIAN_C\" and \"adaptiveThreshold\" and \"THRESH_BINARY\" not in dir():\n",
    "                    from cv2 import adaptiveThreshold, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY\n",
    "\n",
    "                consequence = adaptiveThreshold(np.array(consequence),255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,thresh_gauss,thresh_C)\n",
    "                consequence = Image.fromarray(consequence)\n",
    "        \n",
    "        consequence = pytesseract.image_to_string(consequence, config=tesseract_config) \n",
    "\n",
    "        # OCR adds some strange characters(even with the whitelist function). Let's remove them.\n",
    "\n",
    "        consequence = sub('[^A-Za-z0-9/.]', '', consequence) # Attention: 0, 1 and 8 can be seen as O, l and B.\n",
    "\n",
    "        return consequence\n",
    "\n",
    "    def create_memory(self, frame, keys, values, reward):\n",
    "        '''\n",
    "        Saves data in the memory list.\n",
    "        Memory is saved in the format (frame, (command_type, action1_key, action2_key), (command_index, action1_value, action2_value), reward)\n",
    "\n",
    "        During study mode, frame will be used as input during training. The tuple of values and reward, as labels.\n",
    "        The tuple of keys is used for visualization, and reward also works as weights(helps discarding bad decisions and saving good ones).\n",
    "        \n",
    "        Use cumulative rewards.\n",
    "\n",
    "        Memory will only be changed once it reaches its full size.\n",
    "        '''\n",
    "\n",
    "        reward = reward # Beta can be a constant value, like 1e-5. This just to avoid big numbers.\n",
    "\n",
    "\n",
    "        memory = (frame, keys, values, reward) # A tuple makes each item in the list iterable...and its easier to visualize than lists of lists.\n",
    "\n",
    "\n",
    "        if len(self.memory) < self.memory_size:\n",
    "\n",
    "            self.memory.append(memory)\n",
    "        \n",
    "        else:\n",
    "            self.memory = sorted(self.memory, key=lambda x: x[3]) # Sorting list according to rewards values.\n",
    "            self.memory.pop(0) # Removing the item with lowest reward value\n",
    "            \n",
    "            self.memory.append(memory)\n",
    "        \n",
    "        del memory\n",
    "\n",
    "    def create_data_for_study(self):\n",
    "\n",
    "        # Creating dataset for studying\n",
    "\n",
    "        inputs = [i[0].cpu() for i in self.memory] # game frames. Using cpu to avoid CudaMemory errors.\n",
    "        labels = [i[2] for i in self.memory] # (command_type index, action1 value, action2 value)\n",
    "        actions1 = [i[1][1] for i in self.memory] # action1 key for one-hot encoding\n",
    "        actions2 = [i[1][2] for i in self.memory] # action2 key\n",
    "        rewards = [i[3] for i in self.memory] # Reward got in that step.\n",
    "\n",
    "        inputs = torch.cat(inputs, 0)\n",
    "\n",
    "        labels = np.stack(labels, 0).astype(np.float32) # Now converting to float here to avoid numpy.dtype == object\n",
    "        encoded_actions1 = to_categorical(np.arange(0, len(actions1)), len(self.actions1))\n",
    "        encoded_actions2 = to_categorical(np.arange(0, len(actions2)), len(self.actions2))\n",
    "        rewards = np.stack(rewards, 0).astype(np.float32)\n",
    "\n",
    "        labels = torch.from_numpy(labels)\n",
    "        encoded_actions1 = torch.from_numpy(encoded_actions1)\n",
    "        encoded_actions2 = torch.from_numpy(encoded_actions2)\n",
    "        rewards = torch.from_numpy(rewards)\n",
    "\n",
    "        self.data = inputs\n",
    "        self.labels = labels.to(device)\n",
    "        self.encoded_actions1 = encoded_actions1.to(device)\n",
    "        self.encoded_actions2 = encoded_actions2.to(device)\n",
    "        self.rewards = rewards.to(device)\n",
    "\n",
    "        del inputs, labels, rewards\n",
    "\n",
    "    def record_gameplay(self, number_of_screenshots, screenshot_delay, grayscale=False, resize=False, path=None):\n",
    "\n",
    "        # Resizing and grayscaling isn't really necessary here, but can save you some time later.\n",
    "        # Both saving you from writing more code and from making your hardware having to process more and more data at once.\n",
    "\n",
    "        print(f\"Ok. Screenshot capture will begin in 5 seconds\")\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "        for i in range(number_of_screenshots):\n",
    "\n",
    "            with mss() as sct:\n",
    "\n",
    "                frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "                frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if grayscale:\n",
    "\n",
    "                frame = frame.convert('L')\n",
    "\n",
    "            if resize:\n",
    "\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame.save(f\"{path}/{i+2000}.png\")\n",
    "\n",
    "            sleep(screenshot_delay)\n",
    "        \n",
    "        print(\"Screenshot capture finished!\")\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "\n",
    "    def use_readymade_data(self, data, commands):\n",
    "        '''\n",
    "        data: a tensor of size (N_Samples, Channels, Height, Width) containing the game frames. The pixels values must be within range [0., 255.].\n",
    "        commands: a list of tuples with length (N_samples), with each sample being a tuple composed of (command_type, action1, action2), where:\n",
    "\n",
    "            command_type: a tensor the action command type index-encoded with indices within range [0, len(command_types)].\n",
    "            action1: the action1 index-encoded with indices within range [0, len(actions1)].\n",
    "            action2: the action2 index-encoded with indices within range [0, len(action2)].\n",
    "        '''\n",
    "\n",
    "        # We aren't using data in time_steps mode, like we do for gifs, time series and forecasting in general.\n",
    "        # I thought it might be a good idea to also train Hakisa with that.\n",
    "        # This might also be the best way to train her in frames forecasting, as the process is probably too slow to be made while playing.\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        encoded_command_type = []\n",
    "        encoded_actions1 = []\n",
    "        encoded_actions2 = []\n",
    "\n",
    "        for sample in commands:\n",
    "\n",
    "            command_type = to_categorical(sample[0], len(self.command_type))\n",
    "            command_type = torch.from_numpy(command_type)\n",
    "            command_type = command_type.unsqueeze(0).to(device) # So you don't have to use [number] for your commands tuple to get a command_type with shape [N_samples, 1]\n",
    "            encoded_command_type.append(command_type)\n",
    "\n",
    "            encoded_action1 = to_categorical(sample[1], len(self.actions1))\n",
    "            encoded_action1 = torch.from_numpy(encoded_action1)\n",
    "            encoded_action1 = encoded_action1.unsqueeze(0).to(device)\n",
    "            encoded_actions1.append(encoded_action1)\n",
    "\n",
    "            encoded_action2 = to_categorical(sample[2], len(self.actions2))\n",
    "            encoded_action2 = torch.from_numpy(encoded_action2)\n",
    "            encoded_action2 = encoded_action2.unsqueeze(0).to(device)\n",
    "            encoded_actions2.append(encoded_action2)\n",
    "\n",
    "        encoded_command_type = torch.cat(encoded_command_type, 0)\n",
    "        encoded_actions1 = torch.cat(encoded_actions1, 0)\n",
    "        encoded_actions2 = torch.cat(encoded_actions2, 0)\n",
    "\n",
    "        #encoded_commands = np.stack(encoded_commands, 0).astype(np.int32) # (N_samples)\n",
    "\n",
    "        #encoded_commands = torch.from_numpy(encoded_commands)\n",
    "        #encoded_commands = encoded_commands.to(device)\n",
    "\n",
    "        #self.encoded_commands = encoded_commands\n",
    "\n",
    "        self.encoded_command_type = encoded_command_type\n",
    "        self.encoded_actions1 = encoded_actions1\n",
    "        self.encoded_actions2 = encoded_actions2\n",
    "\n",
    "        print(\"All done! Train the vectorizer and then use it to generate the input mapping dictionary\")\n",
    "\n",
    "\n",
    "    def save_memory(self, memory_name):\n",
    "\n",
    "        import pickle\n",
    "\n",
    "        with open(f'Hakisa_memory_{memory_name}.pkl', 'wb') as f:\n",
    "            pickle.dump(self.memory, f)\n",
    "        \n",
    "        print(f\"Memory saved! You can load it again with\")\n",
    "        print(f\"open('Hakisa_memory_{memory_name}.pkl', 'rb') as f:\\n\\tdataset.memory = pickle.load(f)\")\n",
    "        print(\"Don't forget to close the file!\")\n",
    "\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2out(input, kernel, stride, padding):\n",
    "    x = 2*padding\n",
    "    y = 1*(kernel-1)\n",
    "    z = (input + x - y - 1)/stride\n",
    "\n",
    "    output = z + 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    }
   ],
   "source": [
    "print(conv2out(200, 3, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv2out(160, 2, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer Hakisa prever no apenas o comando que ela deve realizar, mas tambm prever a recompensa daquela ao (Actor-critic) --> Study e Gameplay\n",
    "# Talvez seja interessante tentar prever os features do prximo estado, tambm, mas isso ser muito pesado.\n",
    "# Tambm pode ser til prever se a ao foi boa(1), neutra(0) ou ruim(-1) ---> com base na recompensa ou no output?\n",
    "# Usar recompensas como input? Recompensa condiciona output ----> concatenao ou linear neuron paralelo?\n",
    "\n",
    "\n",
    "# quanto menor for o valor da recompensa prevista em relao  recompensa real, menor o gradiente de backpropagation (MSE Loss) ----> Gameplay mode\n",
    "# Mas, como fazer o agente buscar a maior recompensa, em vez de apenas prever a prxima recompensa? A Hakisa poderia apenas\n",
    "# prever recompensa 0 e se esforar para que a recompensa seja 0.\n",
    "\n",
    "# Probabilidade de a ao ser boa, neutra ou ruim ---> quanto menor o erro na previso, menor o gradiente.\n",
    "# Porm, o que a impediria de realizar aes ruins ou neutras de modo a acertar na sua previso?\n",
    "\n",
    "# Ao boa = recompensa maior. Se Hakisa prever que a ao foi boa, ela vai prever uma recompensa positiva ou maior.\n",
    "# Com isso, a recompensa prevista  diretamente atrelada  probabilidade de a ao ter sido boa.\n",
    "# Logo, se ela prev uma ao ruim, mas uma recompensa alta = maior loss --> backpropagation.\n",
    "# Porm, se ela prev uma ao boa, mas uma recompensa baixa = maior loss --> backpropagation\n",
    "# Mas e se a ao  boa, mas a recompensa de fato no  alta?\n",
    "\n",
    "# RECOMPENSA CUMULATIVA: Hakisa deve prever qual  o valor atual da recompensa(cumulativa) ---> Usar fator de desconto/incerteza(nosso beta) TD-LEARNING\n",
    "# Hakisa deve prever qual ser o valor da recompensa obtida ----> Recompensa ser extrada no prximo estado (backpropagation no prximo estado apenas)\n",
    "# Ao ruim, mas prev recompensa alta = maior loss\n",
    "# Ao boa, mas recompensa baixa = maior loss\n",
    "# Ao boa, recompensa alta = menor loss (ok)\n",
    "# Mas e se ela prev uma ao ruim, recompensa ruim e se sabota para ter uma recompensa baixa?\n",
    "\n",
    "# Hakisa vai classificar a ao que ela decidiu tomar como sendo boa, neutra ou ruim ---> softmax (probabilidade de ser ruim, neutra ou boa)\n",
    "# Isso usando como base a ao tomada, apenas.\n",
    "# No prximo estado, ela receber a ao tomada anteriormente e ir reclassific-la como boa, neutra ou ruim ---> softmax\n",
    "# Isso usando como base a recompensa obtida\n",
    "\n",
    "# O valor de uma ao deve condicionar o valor da prxima ao ---> concatenao? Soma de inputs?\n",
    "# Usar valores da ao anterior como input da Hakisa\n",
    "# Ao ruim, neutra ou boa -----> Classificao da ao anterior para, ento, obter a prxima ao?\n",
    "\n",
    "# LOSS: RECOMPENSA PREVISTA - RECOMPENSA ATUAL (MSE LOSS)\n",
    "\n",
    "# Possvel instabilidade devido a constante optimizao -----> otimizao a cada X steps?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NEW ARCHITECTURE!\n",
    "\n",
    "Hakisa will now receive many inputs and generate many outputs, but not as much as traditional algorithms:\n",
    "    Input A: Game Frame (state) -------> Feature Extraction --------> command type, action1, action2(output A) -----> Probability of being a good decision + Predicted Reward\n",
    "    Input B: Previous action output -------> was that a good decision? -----> added to the main network\n",
    "    Input C: Previous Reward --------------> Added to the network\n",
    "\n",
    "\n",
    "Input A will be the \"main input\", while B and C will be conditioners, being concatenated into input A.\n",
    "Input B will pass through a small preprocessing net in order to classify such action, being concatenated right after that.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "    Output A: (command_type, action1, action2)\n",
    "    Output B: Tensor of size (Batch, 3), classifying probability of Output A being bad(0), neutral(1) or good(2)\n",
    "    Output C: Tensor of size (Batch, 1), predicting the reward to be obtained for that step. Will be multiplied by an uncertainty factor between 0 and 1.[TD-Learning+Actor Critic]\n",
    "\n",
    "\n",
    "Exploration Mode\n",
    "\n",
    "    Now Hakisa will simply select random action values rather than passing them through her network.\n",
    "\n",
    "    This mode won't use output B nor C.\n",
    "\n",
    "Study Mode\n",
    "\n",
    "    Each iteration using frames and actions will generate output C.\n",
    "    Output C is the reward predicted, which will have, as label, the reward stored in memory.\n",
    "\n",
    "    Using more loss functions:\n",
    "\n",
    "        command_type will pass through a NLLLoss ----> command_loss = NLLLoss(predicted_command, command_label)\n",
    "        action1 and action2 will pass, each, through a MSELoss.\n",
    "        predicted reward and actual reward will be passed through a MSELoss ---> reward_loss = MSELoss(predicted_reward, actual_reward)\n",
    "\n",
    "        The Study Loss will be the sum of all those losses.\n",
    "\n",
    "Play Mode\n",
    "    Discarding custom GameplayLoss.\n",
    "\n",
    "    Now, the outputs from one iteration will serve as labels to outputs from previous iteration.\n",
    "\n",
    "        Output B and C from iteration 5 will serve as labels for Outputs B and C from iteration 4.\n",
    "\n",
    "    Using more common losses:\n",
    "\n",
    "        action_quality_loss = NLLLoss(previous_outputB, current_outputB)\n",
    "        reward_loss = MSELoss(predicted_reward, actual_reward)\n",
    "\n",
    "        gameplay_loss = action_quality_loss + reward_loss\n",
    "\n",
    "    Backpropagation will occur after N steps, not after every step ----> avoid instability issues.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hakisa(torch.nn.Module):\n",
    "    '''\n",
    "    Hakisa itself.\n",
    "\n",
    "    She has 2 ways of acting, according to her current mode:\n",
    "\n",
    "        if mode = 'Explore', her inputs can be None, and will generate a random integer according to the input mapping, playing randomly.\n",
    "\n",
    "        if mode = 'Study', she'll receive game frames as inputs, extract the most relevant features and,\n",
    "        in the end, will generate 2 outputs:\n",
    "\n",
    "            output 1: a tuple of commands (command_type, action1, action2). Sizes (Batch, 1)\n",
    "            output 2: prediction of the reward to be obtained through that action. Size (Batch, 1)\n",
    "\n",
    "        if mode = 'Play', she'll receive game frames, previous commands, previous reward and previous action quality, generating:\n",
    "\n",
    "            output 1: a tuple of commands (command_type, action1, action2). Sizes (Batch, 1)\n",
    "            output 2: prediction of the reward to be obtained through that action. Size (Batch, 1)\n",
    "            output 3: a prediction of how good her action was(0 = bad, 1 = ok, 2 = good).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, command_types, actions1, actions2, mode='Explore'):\n",
    "\n",
    "        super(Hakisa, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.n_command_types = len(command_types) # For initialization, the lengths is what actually matters.\n",
    "        self.actions1 = len(actions1)\n",
    "        self.actions2 = len(actions2) # just for exploration mode\n",
    "\n",
    "        # This structure must be changed with the input size...unless you'd like to use adaptive pooling\n",
    "\n",
    "        # Let's begin supposing that we're gonna use 200x200 RGB images ---> (3, 200, 200)\n",
    "\n",
    "        # 200x200\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 100, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(100)\n",
    "        self.conv2 = torch.nn.Conv2d(100, 100, kernel_size=3, stride=1, padding=1, bias=False) # 200x200\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(100)\n",
    "        self.pool2x2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # 100x100\n",
    "        self.conv3 = torch.nn.Conv2d(100, 200, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(200)\n",
    "        self.conv4 = torch.nn.Conv2d(200, 400, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(400)\n",
    "        # Add pool 2x2 ---> 50x50\n",
    "        self.conv5 = torch.nn.Conv2d(400, 800, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv6 = torch.nn.Conv2d(800, 1000, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm6 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 25x25\n",
    "        self.conv7 = torch.nn.Conv2d(1000, 1000, kernel_size=4, stride=1, bias=False) # 22x22\n",
    "        self.batchnorm7 = torch.nn.BatchNorm2d(1000)\n",
    "        self.conv8 = torch.nn.Conv2d(1000, 1000, kernel_size=3, stride=1, bias=False) # 20x20\n",
    "        self.batchnorm8 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 10x10\n",
    "        self.conv9 = torch.nn.Conv2d(1000, 800, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm9 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv10 = torch.nn.Conv2d(800, 400, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm10 = torch.nn.BatchNorm2d(400)\n",
    "        # Add pool 2x2 ---> 5x5\n",
    "        self.neuron1 = torch.nn.Linear(400*5*5, 200*2*2, bias=False)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(200*2*2)\n",
    "\n",
    "        if self.n_command_types > 1:\n",
    "\n",
    "            self.neuron_command_study = torch.nn.Linear(200*2*2, self.n_command_types, bias=False)\n",
    "            self.neuron_command_play = torch.nn.Linear(1200, self.n_command_types, bias=False)\n",
    "\n",
    "            # Considering the command_type that has been predicted, what should be the action1 and action2?\n",
    "\n",
    "            self.neuron2 = torch.nn.Linear(self.n_command_types, 100*2*2, bias=False)\n",
    "\n",
    "        else:\n",
    "        \n",
    "            self.neuron2_study = torch.nn.Linear(200*2*2, 100*2*2, bias=False)\n",
    "            self.neuron2_play = torch.nn.Linear(1200, 100*2*2, bias=False)\n",
    "\n",
    "        self.neuron_quality = torch.nn.Linear(3, 200, bias=False)\n",
    "\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(100*2*2)\n",
    "        self.neuron_action1 = torch.nn.Linear(100*2*2, 1, bias=False)\n",
    "        self.neuron_action2 = torch.nn.Linear(100*2*2, 1, bias=False)\n",
    "\n",
    "        self.neuron_reward = torch.nn.Linear(1, 200, bias=False)\n",
    "        self.layer_normcat = torch.nn.LayerNorm(1200)\n",
    "        self.neuron_predquality = torch.nn.Linear(2+self.n_command_types, 3, bias=False)\n",
    "        self.neuron_predreward1 = torch.nn.Linear(2+self.n_command_types, 1000, bias=False)\n",
    "        self.neuron_predreward2 = torch.nn.Linear(1000, 1, bias=False)\n",
    "\n",
    "        self.PRelu = torch.nn.PReLU(1)\n",
    "\n",
    "        self.softmax = torch.nn.LogSoftmax(-1) # Since we're using softmax here, use NLLLoss during study and play mode.\n",
    "    \n",
    "\n",
    "    def forward(self, input=None, frame_sequence=None, previous_action=None, previous_reward=None):\n",
    "\n",
    "        if self.mode == \"Explore\":\n",
    "            # Reinventing the wheel didn't work. Now, in exploration mode, Hakisa will simply generate random numbers.\n",
    "            \n",
    "            command_type = torch.randint(0, self.n_command_types, size=(1,), device=device)\n",
    "\n",
    "            # Remember that, in exploration mode, we'll be generating data for our vectorizer model, so we'll be dealing exclusively with\n",
    "            # integers(indices for our input mapping lists)\n",
    "\n",
    "            action1 = torch.randint(0, self.actions1, size=(1,), device=device)\n",
    "            action2 = torch.randint(0, self.actions2, size=(1,), device=device)\n",
    "                \n",
    "            #action1 = torch.normal(0, max(dataset.actions1.values()), size=(1, 1), device=device)\n",
    "            #action2 = torch.normal(0, max(dataset.actions2.values()), size=(1, 1), device=device)\n",
    "\n",
    "            return (command_type, action1, action2)\n",
    "\n",
    "\n",
    "        elif self.mode == 'Study':\n",
    "\n",
    "            x = self.conv1(input)\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv3(x)\n",
    "            x = self.batchnorm3(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv4(x)\n",
    "            x = self.batchnorm4(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv5(x)\n",
    "            x = self.batchnorm5(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv6(x)\n",
    "            x = self.batchnorm6(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv7(x)\n",
    "            x = self.batchnorm7(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv8(x)\n",
    "            x = self.batchnorm8(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv9(x)\n",
    "            x = self.batchnorm9(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv10(x)\n",
    "            x = self.batchnorm10(x)\n",
    "            x = self.PRelu(x)\n",
    "            \n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = x.view(x.size(0), -1) # (batch, 400*5*5)\n",
    "\n",
    "            x = self.neuron1(x) # (batch, 200*2*2)\n",
    "            x = self.layer_norm1(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            if self.n_command_types > 1:\n",
    "\n",
    "                command_type = self.neuron_command_study(x)\n",
    "                command_type = self.softmax(command_type) # (Batch, n_commands)\n",
    "\n",
    "                x = self.neuron2(command_type)\n",
    "\n",
    "            else:\n",
    "                command_type = torch.ones((input.size(0), 1), device=device)\n",
    "\n",
    "                x = self.neuron2_study(x)\n",
    "\n",
    "            x = self.layer_norm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            action1 = self.neuron_action1(x) # (Batch, 1)\n",
    "            action2 = self.neuron_action2(x) # (Batch, 1)\n",
    "\n",
    "            x = torch.cat((command_type.detach(), action1.detach(), action2.detach()), 1) # (Batch, 1+1+n_commands)\n",
    "            # Using .detach() for concatenation to prevent reward prediction backpropagation from interferring on actions prediction\n",
    "\n",
    "            x = self.neuron_predreward1(x)\n",
    "            predicted_reward = self.neuron_predreward2(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "            return (command_type, action1, action2), predicted_reward\n",
    "\n",
    "        else:\n",
    "\n",
    "            x = self.conv1(input)\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv3(x)\n",
    "            x = self.batchnorm3(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv4(x)\n",
    "            x = self.batchnorm4(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv5(x)\n",
    "            x = self.batchnorm5(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv6(x)\n",
    "            x = self.batchnorm6(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv7(x)\n",
    "            x = self.batchnorm7(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv8(x)\n",
    "            x = self.batchnorm8(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv9(x)\n",
    "            x = self.batchnorm9(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv10(x)\n",
    "            x = self.batchnorm10(x)\n",
    "            x = self.PRelu(x)\n",
    "            \n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = x.view(x.size(0), -1) # (batch, 400*5*5)\n",
    "\n",
    "            x = self.neuron1(x) # (batch, 200*2*2)\n",
    "\n",
    "            if previous_action==None and previous_reward==None: # For first iteration\n",
    "\n",
    "                previous_action = (torch.zeros((1, 3), device=device), torch.zeros(1, device=device), torch.zeros(1, device=device))\n",
    "                previous_reward = torch.zeros((input.size(0), 1), device=device)\n",
    "\n",
    "            a, b, c = previous_action\n",
    "            previous_action = a + b + c\n",
    "\n",
    "            del a, b, c\n",
    "\n",
    "            previous_action = self.neuron_quality(previous_action) # (batch, 200)\n",
    "            previous_reward = self.neuron_reward(previous_reward) # (batch, 200)\n",
    "\n",
    "            x = torch.cat((x, previous_action, previous_reward), 1) # (batch, 1200)\n",
    "\n",
    "            x = self.layer_normcat(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            if self.n_command_types > 1:\n",
    "\n",
    "                command_type = self.neuron_command_play(x)\n",
    "                command_type = self.softmax(command_type) # (Batch, n_commands)\n",
    "\n",
    "                x = self.neuron2(command_type)\n",
    "\n",
    "            else:\n",
    "                command_type = torch.zeros((input.size(0), 1), device=device)\n",
    "\n",
    "                x = self.neuron2_play(x) # (Batch, 1)\n",
    "\n",
    "            x = self.layer_norm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            action1 = self.neuron_action1(x) # (Batch, 1)\n",
    "            action2 = self.neuron_action2(x) # (Batch, 1)\n",
    "\n",
    "            x = torch.cat((command_type, action1, action2), 1) # (Batch, 1+1+n_commands)\n",
    "\n",
    "            # Attention: .detach() here causes all the previous layers to be excluded from backpropagation.\n",
    "            # This happens because the backpropagation is based on predicted_reward.\n",
    "\n",
    "            command_quality = self.neuron_predquality(x) # (Batch, 3)\n",
    "            command_quality = self.softmax(command_quality)\n",
    "\n",
    "            x = self.neuron_predreward1(x)\n",
    "            predicted_reward = self.neuron_predreward2(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "            return (command_type, action1, action2), command_quality, predicted_reward\n",
    "\n",
    "\n",
    "    def execute_command(self, command):\n",
    "        '''\n",
    "        Command must be a tuple(command_type, action1, action2), where:\n",
    "\n",
    "            command_type: key(keyboard) or move, rightClick, click(mouse)\n",
    "            action1: Up, Down, press(keyboard), X coordinate(mouse) or None(no mouse movement)\n",
    "            action2: 'a', 'z', 'shift'...(keyboard), Y coordinate(mouse) or None(no mouse movement)\n",
    "\n",
    "        Make sure all key actions(action2) are lowered.\n",
    "\n",
    "        Have in mind that Hakisa might output command_type 'key' and action1 that is equivalent to a mouse action.\n",
    "        '''\n",
    "\n",
    "        if \"key\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                \n",
    "                if \"Up\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        #pyautogui.keyUp(command[2])\n",
    "                        keyboard.release(command[2])\n",
    "                \n",
    "                    except:\n",
    "                        pass # If Hakisa predicts a mouse action for a keyboard command, she won't do anything.\n",
    "\n",
    "                elif \"Down\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        #pyautogui.keyDown(command[2])\n",
    "                        keyboard.press(command[2])\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif \"press\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        keyboard.send(command[2]) # Some games won't work with pyautogui.press(), so use keyboard module, since we'll import it for Play Mode.\n",
    "                    \n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            except:\n",
    "\n",
    "                pass # If Hakisa predicts a keyboard command, but outputs a mouse action, she won't do anything.\n",
    "\n",
    "        elif \"move\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                #pyautogui.moveTo(command[1], command[2], duration=0.19) # Duration = 0.19 seconds to be more realistic\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "\n",
    "            except:\n",
    "                pass # If Hakisa predict a mouse command, but outputs a keyboard action, she won't do anything.\n",
    "\n",
    "        elif \"rightclick\" in command[0]:\n",
    "            \n",
    "            try:\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "                mouse.right_click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif \"click\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                #pyautogui.moveTo(command[1], command[2], duration=0.19)\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "                mouse.click() # Same case as press. Use mouse module.\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError # It was probably you who made a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jigoku Kisetsukan\n",
    "\n",
    "command_type = ['key']\n",
    "\n",
    "actions1 = ['Down', 'Up']\n",
    "\n",
    "actions2 = ['up', 'down', 'left', 'right', 'z', 'x', 'shift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(command_types=command_type, actions1=actions1, actions2=actions2, explore_train_steps=100, memory_size=10, resize=(200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['key']\n",
      "['Down', 'Up']\n",
      "['up', 'down', 'left', 'right', 'z', 'x', 'shift']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.command_type)\n",
    "print(dataset.actions1)\n",
    "print(dataset.actions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.encoded_command_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok. Screenshot capture will begin in 5 seconds\n",
      "Screenshot capture finished!\n"
     ]
    }
   ],
   "source": [
    "dataset.record_gameplay(2000, 1, grayscale=False, resize=False, path=\"Hakisa/JK_gameplay/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok. Screenshot capture will begin in 5 seconds\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Hakisa/LoL_ARAM//0.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3036/1025504469.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_gameplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Hakisa/LoL_ARAM/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3036/2488941921.py\u001b[0m in \u001b[0;36mrecord_gameplay\u001b[1;34m(self, number_of_screenshots, screenshot_delay, grayscale, resize, path)\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m             \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{path}/{i}.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreenshot_delay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2167\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2168\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2169\u001b[1;33m                 \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2171\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Hakisa/LoL_ARAM//0.png'"
     ]
    }
   ],
   "source": [
    "dataset.record_gameplay(3000, 0, grayscale=False, resize=False, path=\"Hakisa/LoL_gameplay2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "images_by_order = []\n",
    "\n",
    "for directory, _, files in os.walk(\"Hakisa/LoL_gameplay/\"):\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        file = file.split('.')\n",
    "        file = file[0] # Getting exclusively the number\n",
    "\n",
    "        images_by_order.append(file)\n",
    "\n",
    "images_by_order = sorted([int(x) for x in images_by_order])\n",
    "\n",
    "# Problem: for strings, Python considers that 1000 < 2. Maybe something related to how the string is assembled?\n",
    "\n",
    "images_data = []\n",
    "\n",
    "for i in images_by_order:\n",
    "\n",
    "    i = directory + '/' + str(i) + '.png'\n",
    "    image = Image.open(i)\n",
    "    image = image.resize((200, 200))\n",
    "    array = np.array(image, dtype=np.float32)\n",
    "    image.close()\n",
    "    images_data.append(array)\n",
    "\n",
    "images_data = np.stack(images_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hakisa/LoL_gameplay//0.png', 'Hakisa/LoL_gameplay//1.png', 'Hakisa/LoL_gameplay//2.png', 'Hakisa/LoL_gameplay//3.png', 'Hakisa/LoL_gameplay//4.png', 'Hakisa/LoL_gameplay//5.png', 'Hakisa/LoL_gameplay//6.png', 'Hakisa/LoL_gameplay//7.png', 'Hakisa/LoL_gameplay//8.png', 'Hakisa/LoL_gameplay//9.png']\n"
     ]
    }
   ],
   "source": [
    "print(images_path[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "print(images_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['click', 'rightclick', 'key']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.command_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['key']\n",
    "['Down', 'Up']\n",
    "['up', 'down', 'left', 'right', 'z', 'x', 'shift']\n",
    "   0,    1,       2,       3,    4,    5,    6\n",
    "\n",
    "# Careful not to get confused with the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(start, end):\n",
    "...     for i in range(start, end):\n",
    "...             image = images_by_order[i]\n",
    "...             image = directory + \"/\" + str(image) + \".png\"\n",
    "...             image = Image.open(image)\n",
    "...             array = np.array(image, dtype=np.float32)\n",
    "...             image.close()\n",
    "...             array = array/255\n",
    "...             plt.imshow(array)\n",
    "...             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_type = ['click', 'rightclick', 'key']\n",
    "                    0,        1,        2\n",
    "actions1 = 1~1919 + keyboard press()\n",
    "           0~1918 + 1919\n",
    "actions2 = 1~1079 + ['q', 'w', 'e', 'r', 'd', 'f', 'b', '1', '2', '3', '4', '5', '6', '7', 'space']\n",
    "            0~1078 + 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = [\n",
    "    (0, 663, 226), (0, 670, 230), (0, 660, 224), (0, 665, 225), (0, 1227, 1064), (1, 657, 424), (1, 661, 520), (1, 488, 261), (1, 1384, 431), (1, 1838, 1048),\n",
    "    (0, 756, 926), (0, 765, 930), (0, 756, 927), (0, 758, 925), (0, 750, 920), (0, 1840, 1049), (0, 1837, 1046), (0, 1842, 1045), (0, 1844, 1041), (0, 1836, 1048),\n",
    "    (1, 1357, 368), (1, 1027, 362), (1, 730, 366), (1, 1110, 638), (1, 493, 437), (1, 801, 715), (1, 190, 654), (1, 978, 414), (1, 993, 588), (1, 901, 644),\n",
    "    (1, 661, 511), (1, 706, 409), (1, 974, 625), (1, 1027, 642), (1, 948, 679), (1, 656, 513), (1, 944, 640), (1, 1008, 452), (1, 551, 479), (1, 521, 438),\n",
    "    (1, 1027, 665), (1, 760, 464), (1, 1123, 625), (1, 1280, 597), (1, 1363, 332), (1, 1039, 360), (1, 1289, 515), (1, 950, 210), (1, 1167, 230), (1, 1134, 248),\n",
    "    (1, 805, 202), (1, 1147, 342), (1, 873, 664), (1, 857, 691), (1, 1101, 356), (2, 1919, 1079), (0, 993, 330), (1, 639, 660), (1, 762, 439), (1, 1097, 336),\n",
    "    (1, 1022, 319), (2, 1919, 1079), (0, 1175, 314), (1, 1164, 499), (1, 670, 408), (1, 1006, 632), (1, 616, 461), (2, 1919, 1079), (0, 860, 237), (1, 752, 600),\n",
    "    (1, 814, 811), (1, 793, 692), (1, 732, 714), (1, 980, 260), (1, 1022, 373), (1, 1132, 517), (2, 1919, 1079), (1, 605, 748), (1, 1188, 565), (1, 726, 656),\n",
    "    (1, 846, 237), (1, 1112, 530), (1,1155, 449), (1, 849, 264), (1, 1142, 375), (1, 1071, 448), (1, 1164, 718), (1, 870, 787), (1, 551, 614), (1, 1048, 338),\n",
    "    (1, 989, 760), (1, 777, 705), (1, 466, 498), (2, 1919, 1079), (0, 909, 383), (0, 818, 917), (0, 967, 174), (1, 622, 504), (1, 998, 634), (1, 806, 409), # 100\n",
    "    (1, 913, 219), (1, 1088, 588), (1, 1138, 632), (1, 1171, 454), (1, 1117, 454), (1, 831, 492), (1, 832, 350), (1, 1062, 429), (1, 778, 280), (1, 728, 289),\n",
    "    (1, 561, 610), (1, 771, 483), (1, 948, 319), (0, 851, 302), (1, 724, 420), (2, 1919, 1080), (1, 983, 755), (1, 1039, 532), (2, 1919, 1079), (0, 587, 390),\n",
    "    (1, 797, 208), (1, 799, 377), (1, 1114, 537), (1, 1058, 248), (1, 1009, 278), (1, 1034, 287), (1, 918, 839), (2, 1919, 1079), (2, 1919, 1079), (2, 1919, 1079),\n",
    "    (1, 726, 366), (1, 715, 509), (1, 736, 559), (1, 724, 418), (1, 678, 548), (1, 650, 740), (0, 898, 913), (1, 764, 552), (2, 1919, 1079), (1, 1034, 295),\n",
    "    (1, 967, 763), (1, 886, 815), (1, 752, 720), (1, 1008, 539), (1, 775, 846), (2, 1919, 1079), (1, 860, 804), (2, 1919, 1092), (0, 1242, 489), (1, 676, 805),\n",
    "    (1, 885, 317), (1, 592, 414), (1, 950, 667), (1, 1069, 420), (1, 911, 345), (1, 933, 574), (0, 775, 293), (2, 1919, 1080), (1, 683, 692), (1, 534, 646),\n",
    "    (1, 719, 735), (1, 840, 343), (1, 1071, 489), (1, 1294, 675), (1, 1300, 448), (1, 896, 686), (1, 855, 714), (1, 825, 573), (1, 1060, 554), (1, 656, 591),\n",
    "    (1, 629, 576), (2, 1919, 1092), (0, 589, 323), (1, 993, 653), (0, 1008, 241), (2, 1919, 1080), (1, 825, 317), (2, 1919, 1081), (1, 724, 442), (1, 1183, 528),\n",
    "    (1, 600, 589), (1, 1019, 420), (1, 737, 494), (2, 1919, 1079), (2, 1919, 1079), (1, 756, 442), (1, 1136, 355), (1, 1028, 213), (1, 996, 252), (1, 961, 250),\n",
    "    (1, 1009, 261), (1, 980, 261), (1, 1013, 248), (1, 1034, 258), (1, 1050, 260), (1, 741, 345), (1, 872, 381), (0, 765, 921), (1, 1035, 321), (1, 857, 262), # 200\n",
    "    (1, 896, 239), (1, 901, 753), (1, 903, 275), (1, 864, 269), (1, 950, 267), (1, 896, 275), (1, 905, 260), (1, 622, 606), (1, 879, 256), (1, 1002, 336),\n",
    "    (1, 853, 798), (1, 1097, 558), (1, 983, 694), (1, 814, 252), (1, 823, 299), (1, 749, 552), (1, 872, 228), (1, 793, 807), (1, 717, 828), (1, 700, 837),\n",
    "    (1, 698, 668), (1, 482, 899), (2, 1919, 1079), (2, 1919, 1080), (0, 683, 442), (1, 805, 858), (1, 575, 785), (1, 587, 746), (1, 555, 699), (1, 628, 627),\n",
    "    (1, 551, 565), (2, 1919, 1080), (1, 1144, 377), (0, 873, 502), (1, 1000, 254), (2, 1919, 1087), (1, 1035, 354), (1, 685, 656), (2, 1919, 1080), (1, 1080, 155),\n",
    "    (1, 1019, 321), (1, 1136, 297), (0, 890, 504), (1, 767, 826), (1, 737, 815), (1, 648, 625), (1, 665, 746), (2, 1919, 1087), (1, 888, 805), (1, 1013, 846),\n",
    "    (1, 1216, 608), (1, 754, 487), (1, 855, 334), (1, 670, 757), (1, 1037, 776), (1, 922, 837), (1, 1080, 774), (1, 1067, 712), (1, 803, 545), (1, 1054, 247),\n",
    "    (1, 577, 612), (1, 592, 641), (1, 676, 619), (1, 1002, 746), (1, 1067, 742), (2, 1919, 1079), (1, 1015, 343), (0, 771, 923), (2, 1919, 1079), (2, 1919, 1080),\n",
    "    (2, 1919, 1080), (1, 933, 319), (1, 1009, 790), (1, 758, 420), (1, 866, 830), (1, 844, 647), (1, 742, 422), (2, 1919, 1079), (1, 879, 247), (1, 832, 599),\n",
    "    (1, 626, 392), (2, 1919, 1079), (1, 873, 356), (1, 1153, 315), (1, 1090, 159), (1, 980, 245), (1, 995, 273), (2, 1919, 1079), (2, 1919, 1079), (2, 1919, 1080),\n",
    "    (1, 561, 608), (1, 840, 820), (1, 862, 774), (1, 955, 776), (1, 983, 822), (1, 873, 858), (1, 842, 818), (1, 939, 831), (1, 888, 805), (1, 859, 861), # 300\n",
    "    (1, 823, 858), (1, 872, 869), (1, 726, 804), (1, 726, 852), (1, 1049, 729), (1, 853, 254), (1, 918, 273), (1, 1034, 345), (2, 1919, 1079), (1, 900, 692),\n",
    "    (0, 793, 278), (1, 905, 878), (1, 931, 852), (0, 894, 526), (1, 905, 843), (1, 853, 878), (1, 879, 794), (1, 806, 714), (1, 1034, 751), (1, 767, 722),\n",
    "    (1, 711, 818), (1, 624, 554), (1, 825, 817), (1, 1088, 614), (2, 1919, 1079), (0, 1002, 153), (2, 1919, 1085), (0, 1231, 1057), (0, 1229, 1055), (0, 836, 442),\n",
    "    (1, 1464, 371), (1, 1455, 366), (1, 1460, 371), (1, 1460, 375), (1, 1464, 383), (1, 1464, 371), (1, 492, 533), (1, 486, 530), (1, 756, 625), (1, 765, 425),\n",
    "    (1, 836, 651), (1, 786, 662), (1, 1831, 1046), (2, 1919, 1080), (1, 1833, 1044), (1, 1836, 1042), (1, 1835, 1044), (1, 1835, 1046), (1, 1835, 1044), (1, 1851, 1051),\n",
    "    (1, 1836, 1044), (2, 1919, 1080), (2, 1919, 1080), (2, 1919, 1080), (1, 1844, 1046), (1, 1851, 1038), (1, 1838, 1044), (1, 1840, 1048), (1, 1840, 1044), (2, 1919, 1080),\n",
    "    (2, 1919, 1080), (1, 1846, 1046), (1, 1864, 1034), (1, 1864, 1029), (1, 1874, 1033), (1, 1866, 1027), (2, 1919, 1080), (2, 1919, 1080), (2, 1919, 1080), (1, 1391, 222),\n",
    "    (1, 1395, 331), (1, 1078, 281), (1, 1155, 386), (1, 1144, 297), (1, 872, 836), (2, 1919, 1080), (0, 872, 504), (0, 959, 904), (0, 954, 900), (0, 832, 312),\n",
    "    (1, 825, 321), (1, 849, 725), (2, 1919, 1082), (2, 1919, 1082), (2, 1919, 1080), (0, 896, 492), (1, 762, 368), (1, 795, 378), (2, 1919, 1079), (0, 875, 507),\n",
    "    (2, 1919, 1080), (1, 1056, 466), (1, 1093, 499), (1, 872, 820), (1, 784, 828), (1, 754, 763), (1, 979, 850), (1, 818, 763), (1, 516, 699), (1, 588, 431), # 400\n",
    "    (1, 601, 349), (1, 808, 336), (0, 805, 247), (1, 1002, 735), (1, 1067, 440), (0, 1099, 429), (1, 1112, 538), (1, 1101, 433), (1, 726, 643), (1, 799, 648),\n",
    "    (1, 587, 604), (1, 991, 301), (1, 719, 351), (1, 1183, 418), (1, 980, 630), (1, 689, 522), (1, 762, 556), (1, 659, 491), (0, 1045, 219), (0, 877, 497),\n",
    "    (1, 1145, 435), (1, 1067, 615), (1, 1043, 457), (1, 886, 321), (1, 618, 535), (1, 745, 315), (1, 758, 577), (1, 637, 446), (0, 769, 921), (2, 1919, 1079),\n",
    "    (0, 821, 319), (0, 656, 315), (2, 1919, 1079), (1, 786, 484), (1, 1201, 468), (1, 1063, 283), (2, 1919, 1079), (2, 1919, 1080), (0, 870, 517), (1, 978, 742),\n",
    "    (1, 648, 559), (1, 1043, 403), (1, 965, 597), (1, 734, 807), (1, 609, 822), (1, 957, 412), (1, 801, 794), (1, 823, 815), (1, 1047, 556), (1, 609, 669),\n",
    "    (2, 1919, 1081), (0, 721, 345), (2, 1919, 1080), (1, 790, 245), (1, 914, 649), (1, 846, 772), (1, 708, 368), (1, 588, 438), (2, 1919, 1079), (1, 1112, 340),\n",
    "    (1, 1071, 429), (1, 901, 722), (2, 1919, 1085), (2, 1919, 1085), (0, 1233, 1061), (0, 1233, 1061), (1, 1272, 377), (0, 1233, 1061), (0, 1162, 1056), (0, 1272, 1054),\n",
    "    (1, 1270, 377), (1, 760, 476), (1, 823, 670), (1, 736, 532), (1, 1831, 1053), (1, 1848, 1046), (1, 1836, 1048), (2, 1919, 1080), (1, 1833, 1048), (1, 1848, 1046),\n",
    "    (1, 1830, 1040), (1, 1851, 1048), (1, 1827, 1046), (1, 1840, 1040), (1, 1848, 1046), (1, 1827, 1038), (1, 1851, 1038), (1, 1842, 1040), (1, 1833, 1049), (2, 1919, 1080),\n",
    "    (2, 1919, 1080), (2, 1919, 1080), (1, 1851, 1042), (1, 1855, 1042), (1, 1844, 1038), (1, 1849, 1034), (2, 1919, 1080), (2, 1919, 1080), (1, 1872, 1029), (1, 1874, 1025), # 500\n",
    "    (1, 1503, 226), (1, 1307, 323), (2, 1919, 1081), (1, 1198, 429), (2, 1919, 1079), (1, 1041, 288), (1, 1121, 489), (2, 1919, 1079), (2, 1919, 1080), (0, 872, 518),\n",
    "    (1, 456, 532), (0, 1058, 319), (1, 842, 744), (1, 538, 763), (2, 1919, 1079), (2, 1919, 1082), (2, 1919, 1081), (2, 1919, 1079), (0, 814, 422), (1, 635, 329),\n",
    "    (1, 724, 306), (0, 911, 193), (2, 1919, 1080), (2, 1919, 1083), (0, 1127, 558), (0, 886, 511), (1, 1035, 636), (2, 1919, 1079), (2, 1919, 1080), (1, 615, 779),\n",
    "    (1, 839, 733), (1, 841, 732), (0, 821, 917), (1, 570, 712), (0, 823, 920), (2, 1919, 1080), (2, 1919, 1092), (1, 1276, 252), (1, 1242, 299), (1, 1127, 247),\n",
    "    (1, 1106, 293), (1, 1060, 282), (1, 1144, 294), (1, 1097, 282), (1, 1116, 293), (1, 1112, 293), (2, 1919, 1079), (2, 1919, 1081), (0, 881, 515), (1, 1012, 289),\n",
    "    (2, 1919, 1079), (0, 961, 125), (1, 616, 556), (2, 1919, 1079), (2, 1919, 1081), (1, 901, 805), (2, 1919, 1079), (0, 1153, 297), (1, 696, 649), (2, 1919, 1080),\n",
    "    (2, 1919, 1079), (0, 1028, 356), (1, 1216, 161), (1, 1022, 254), (1, 1013, 215), (2, 1919, 1079), (0, 922, 161), (1, 929, 790), (1, 885, 809), (2, 1919, 1085),\n",
    "    (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (0, 1224, 1057), (0, 1185, 1061), (0, 1193, 1067), (0, 1197, 1063), (0, 1200, 1061), (0, 641, 481), (0, 685, 475),\n",
    "    (1, 690, 471), (1, 749, 492), (1, 797, 628), (1, 892, 597), (1, 870, 611), (1, 1835, 1046), (1, 1846, 1048), (2, 1919, 1080), (1, 1840, 1042), (1, 1833, 1040),\n",
    "    (1, 1833, 1044), (1, 1829, 1051), (2, 1919, 1080), (1, 1844, 1059), (1, 1859, 1044), (1, 1838, 1049), (1, 1844, 1049), (2, 1919, 1080), (2, 1919, 1080), (2, 1919, 1080), # 600\n",
    "    (1, 1840, 1044), (1, 1842, 1044), (1, 1851, 1038), (1, 1848, 1044), (1, 1870, 1036), (2, 1919, 1080), (1, 1455, 435), (1, 1667, 222), (1, 1399, 297), (1, 1268, 306),\n",
    "    (2, 1919, 1079), (0, 1080, 312), (2, 1919, 1081), (1, 1116, 200), (2, 1919, 1079), (1, 825, 189), (1, 1160, 466), (1, 840, 410), (1, 1117, 460), (1, 998, 394),\n",
    "    (2, 1919, 1079), (0, 944, 180), (2, 1919, 1081), (0, 888, 507), (1, 816, 267), (1, 812, 528), (1, 615, 416), (1, 624, 368), (1, 981, 232), (1, 955, 250),\n",
    "    (1, 955, 265), (1, 985, 256), (2, 1919, 1079), (2, 1919, 1079), (0, 773, 278), (1, 985, 248), (1, 1054, 204), (2, 1919, 1079), (0, 838, 258), (2, 1919, 1080),\n",
    "    (1, 959, 248), (1, 946, 247), (1, 980, 364), (2, 1919, 1079), (1, 866, 859), (1, 795, 854), (1, 549, 817), (1, 711, 729), (2, 1919, 1079), (0, 1015, 237),\n",
    "    (2, 1919, 1080), (1, 952, 722), (2, 1919, 1079), (0, 1101, 381), (1, 788, 811), (0, 762, 926), (0, 1026, 377), (2, 1919, 1082), (0, 881, 481), (1, 1119, 204),\n",
    "    (1, 1157, 139), (1, 1274, 261), (1, 1181, 166), (1, 1097, 283), (1, 1073, 273), (1, 1103, 267), (2, 1919, 1079), (2, 1919, 1081), (1, 803, 841), (1, 901, 463),\n",
    "    (2, 1919, 1079), (1, 963, 630), (1, 1037, 577), (1, 980, 843), (1, 972, 622), (1, 993, 610), (1, 1119, 725), (1, 771, 228), (1, 754, 154), (1, 968, 105),\n",
    "    (1, 913, 131), (1, 1021, 165), (1, 1168, 329), (1, 955, 289), (1, 540, 511), (2, 1919, 1079), (2, 1919, 1080), (2, 1919, 1081), (2, 1919, 1079), (0, 819, 299),\n",
    "    (1, 1138, 552), (2, 1919, 1079), (0, 765, 176), (1, 941, 792), (2, 1919, 1079), (0, 827, 215), (2, 1919, 1080), (2, 1919, 1081), (2, 1919, 1079), (0, 609, 478), # 700\n",
    "    (1, 620, 761), (1, 743, 614), (1, 1186, 435), (1, 957, 852), (1, 1199, 383), (1, 769, 722), (2, 1919, 1092), (0, 611, 591), (2, 1919, 1085), (2, 1919, 1085),\n",
    "    (2, 1919, 1085), (0, 1209, 1061), (0, 1237, 1064), (0, 1058, 528), (0, 1056, 515), (0, 1091, 548), (0, 985, 569), (1, 1360, 282), (1, 1375, 289), (0, 1000, 480),\n",
    "    (0, 1014, 522), (0, 1008, 518), (0, 1024, 492), (0, 1076, 455), (1, 1456, 373), (1, 693, 537), (1, 827, 622), (1, 1835, 1042), (2, 1919, 1080), (1, 1838, 1048),\n",
    "    (1, 1848, 1042), (1, 1829, 1046), (1, 1840, 1040), (2, 1919, 1080), (1, 1833, 1048), (1, 1838, 1042), (1, 1838, 1040), (1, 1835, 1044), (2, 1919, 1080), (2, 1919, 1080),\n",
    "    (1, 1831, 1046), (1, 1836, 1042), (1, 1838, 1044), (1, 1885, 1027), (1, 1876, 1032), (2, 1919, 1080), (2, 1919, 1080), (1, 1857, 1040), (1, 1728, 297), (1, 1695, 356),\n",
    "    (2, 1919, 1080), (2, 1919, 1080), (1, 1341, 258), (1, 1211, 278), (1, 1235, 256), (1, 1196, 170), (1, 1248, 604), (1, 1168, 528), (2, 1919, 1080), (2, 1919, 1081),\n",
    "    (2, 1919, 1079), (0, 1130, 210), (1, 654, 664), (2, 1919, 1079), (2, 1919, 1080), (1, 575, 753), (2, 1919, 1084), (1, 464, 712), (2, 1919, 1080), (2, 1919, 1080),\n",
    "    (1, 1158, 453), (2, 1919, 1079), (2, 1919, 1080), (2, 1919, 1079), (0, 1117, 362), (1, 952, 409), (2, 1919, 1079), (2, 1919, 1082), (2, 1919, 1080), (2, 1919, 1081),\n",
    "    (1, 1108, 260), (2, 1919, 1080), (1, 1076, 237), (1, 998, 282), (1, 888, 410), (1, 827, 356), (1, 1097, 207), (1, 955, 219), (1, 991, 178), (1, 913, 189),\n",
    "    (1, 961, 133), (1, 892, 183), (1, 963, 135), (1, 1190, 399), (1, 1175, 388), (2, 1919, 1079), (2, 1919, 1080), (2, 1919, 1081), (1, 939, 235), (1, 1101, 312), # 800\n",
    "    (1, 989, 709), (1, 987, 736), (1, 795, 265), (1, 803, 38), (1, 847, 186), (1, 853, 265), (1, 847, 260), (1, 847, 269), (2, 1919, 1079), (2, 1919, 1081),\n",
    "    (1, 1017, 656), (1, 814, 202), (1, 907, 865), (1, 1076, 604), (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (0, 1225, 1056),\n",
    "    (0, 710, 751), (1, 1280, 375), (1, 1278, 373), (1, 1274, 375), (0, 1548, 124), (1, 732, 515), (1, 803, 617), (1, 760, 494), (1, 875, 460), (1, 877, 550),\n",
    "    (1, 873, 457), (1, 877, 526), (2, 1919, 1080), (1, 1842, 1042), (1, 1835, 1046), (1, 1840, 1044), (1, 1844, 1040), (2, 1919, 1080), (2, 1919, 1080), (2, 1919, 1080),\n",
    "    (1, 1835, 1042), (1, 1842, 1044), (1, 1836, 1044), (2, 1919, 1080), (1, 1857, 1051), (1, 1847, 1039), (1, 1840, 1044), (1, 1836, 1048), (2, 1919, 1080), (1, 1840, 1048),\n",
    "    (1, 1715, 317), (1, 1462, 323), (1, 1415, 453), (1, 1557, 381), (1, 983, 621), (1, 823, 424), (1, 743, 505), (2, 1919, 1079), (0, 953, 383), (1, 1132, 343),\n",
    "    (2, 1919, 1079), (1, 812, 675), (1, 572, 654), (2, 1919, 1083), (2, 1919, 1079), (2, 1919, 1082), (2, 1919, 1080), (0, 1289, 368), (1, 633, 544), (1, 857, 554),\n",
    "    (1, 825, 582), (1, 1116, 416), (1, 724, 604), (2, 1919, 1079), (2, 1919, 1080), (2, 1919, 1079), (0, 1145, 448), (1, 680, 563), (2, 1919, 1079), (2, 1919, 1079),\n",
    "    (1, 1157, 323), (0, 1021, 330), (2, 1919, 1081), (0, 877, 519), (1, 1185, 695), (2, 1919, 1080), (1, 754, 226), (1, 1019, 755), (2, 1919, 1092), (0, 575, 299),\n",
    "    (1, 1050, 621), (1, 1073, 96), (1, 957, 222), (1, 1071, 71), (1, 961, 234), (1, 875, 213), (1, 1145, 198), (1, 754, 308), (1, 585, 340), (2, 1919, 1079), # 900\n",
    "    (2, 1919, 1081), (0, 873, 494), (1, 756, 589), (1, 670, 641), (1, 777, 541), (2, 1919, 1079), (2, 1919, 1081), (0, 881, 485), (1, 823, 234), (1, 708, 183),\n",
    "    (1, 693, 282), (1, 695, 273), (1, 698, 274), (1, 694, 276), (1, 680, 291), (1, 1032, 796), (1, 900, 822), (1, 814, 751), (2, 1919, 1080), (1, 534, 779),\n",
    "    (1, 637, 645), (2, 1919, 1085), (2, 1919, 1085), (0, 1227, 1061), (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (0, 1155, 1066), (0, 1265, 1067), (0, 1150, 1060),\n",
    "    (0, 1246, 1055), (1, 1371, 278), (1, 734, 520), (1, 786, 593), (1, 762, 522), (1, 890, 617), (1, 920, 622), (1, 1846, 1048), (2, 1919, 1080), (1, 1853, 1044),\n",
    "    (1, 1848, 1046), (1, 1849, 1042), (1, 1844, 1038), (2, 1919, 1080), (1, 1850, 1038), (1, 1848, 1040), (1, 1855, 1044), (1, 1851, 1045), (1, 1846, 1038), (2, 1919, 1080),\n",
    "    (1, 1846, 1044), (1, 1851, 1040), (1, 1848, 1046), (2, 1919, 1080), (1, 1851, 1042), (1, 1594, 375), (1, 1397, 515), (1, 1581, 416), (1, 1082, 427), (2, 1919, 1079),\n",
    "    (0, 1225, 269), (2, 1919, 1080), (0, 881, 511), (2, 1919, 1082), (2, 1919, 1079), (2, 1919, 1080), (1, 410, 671), (2, 1919, 1079), (2, 1919, 1081), (2, 1919, 1080),\n",
    "    (2, 1919, 1079), (0, 937, 278), (1, 927, 248), (1, 935, 228), (2, 1919, 1080), (1, 886, 94), (1, 892, 129), (1, 873, 148), (0, 851, 317), (1, 886, 325),\n",
    "    (1, 1183, 599), (0, 756, 621), (2, 1919, 1080), (1, 857, 731), (2, 1919, 1079), (1, 816, 828), (1, 927, 679), (1, 883, 764), (1, 965, 740), (0, 1004, 200),\n",
    "    (0, 879, 478), (1, 751, 697), (1, 1084, 366), (1, 1155, 295), (1, 914, 124), (1, 836, 189), (1, 745, 222), (1, 840, 247), (1, 846, 226), (1, 846, 225), # 1000\n",
    "    (1, 860, 202), (1, 1229, 619), (1, 881, 292), (1, 903, 355), (1, 860, 478), (2, 1919, 1079), (2, 1919, 1080), (0, 870, 509), (1, 1013, 370), (1, 959, 578),\n",
    "    (1, 687, 703), (1, 516, 688), (1, 857, 548), (1, 363, 129), (1, 417, 241), (1, 538, 355), (1, 471, 381), (2, 1919, 1079), (0, 520, 455), (2, 1919, 1081),\n",
    "    (0, 898, 500), (1, 696, 487), (1, 726, 438), (1, 700, 463), (1, 695, 489), (1, 1162, 422), (0, 890, 917), (1, 980, 781), (1, 624, 824), (0, 890, 902),\n",
    "    (1, 1859, 1025), (1, 1857, 1023), (1, 931, 621), (1, 959, 735), (1, 1050, 755), (1, 1000, 811), (1, 1035, 776), (1, 1479, 966), (1, 1214, 812), (1, 1157, 673),\n",
    "    (1, 1030, 310), (2, 1919, 1079), (0, 1063, 239), (2, 1919, 1082), (2, 1919, 1082), (2, 1919, 1080), (2, 1919, 1081), (0, 881, 505), (1, 751, 736), (2, 1919, 1080),\n",
    "    (2, 1919, 1079), (0, 1013, 219), (2, 1919, 1081), (2, 1919, 1079), (2, 1919, 1080), (1, 900, 94), (1, 978, 133), (2, 1919, 1079), (0, 810, 358), (2, 1919, 1079),\n",
    "    (2, 1919, 1080), (1, 868, 263), (1, 1009, 878), (2, 1919, 1081), (1, 927, 723), (1, 844, 772), (1, 846, 738), (1, 829, 792), (1, 825, 779), (1, 890, 776),\n",
    "    (1, 907, 824), (1, 484, 736), (1, 710, 688), (2, 1919, 1080), (2, 1919, 1081), (1, 935, 252), (1, 831, 245), (1, 849, 247), (1, 506, 235), (1, 1104, 214),\n",
    "    (1, 1166, 288), (2, 1919, 1079), (2, 1919, 1081), (1, 784, 403), (1, 894, 256), (1, 838, 220), (1, 993, 312), (2, 1919, 1085), (2, 1919, 1085), (0, 1237, 1057),\n",
    "    (0, 1239, 1054), (2, 1919, 1085), (1, 698, 280), (1, 680, 144), (1, 968, 776), (2, 1919, 1079), (2, 1919, 1081), (0, 885, 502), (1, 780, 690), (2, 1919, 1079), # 1100\n",
    "    (0, 827, 224), (2, 1919, 1080), (2, 1919, 1081), (0, 886, 519), (1, 829, 843), (1, 637, 774), (2, 1919, 1080), (1, 546, 641), (2, 1919, 1085), (0, 1239, 1055),\n",
    "    (0, 1034, 526), (0, 1078, 500), (0, 1056, 520), (0, 1058, 492), (0, 1026, 548), (0, 1039, 498), (0, 1024, 435), (0, 1015, 466), (1, 1015, 466), (0, 1194, 1049),\n",
    "    (1, 1835, 1048), (1, 1835, 1044), (1, 1840, 1049), (1, 1844, 1046), (2, 1919, 1080), (2, 1919, 1080), (1, 1838, 1044), (1, 1836, 1051), (1, 1836, 1031), (2, 1919, 1080),\n",
    "    (2, 1919, 1080), (1, 1863, 1048), (1, 1853, 1040), (1, 1855, 1040), (2, 1919, 1080), (2, 1919, 1080), (1, 1598, 464), (1, 1710, 381), (1, 1589, 388), (2, 1919, 1080),\n",
    "    (2, 1919, 1079), (1, 1030, 319), (2, 1919, 1082), (1, 1242, 148), (2, 1919, 1081), (2, 1919, 1083), (1, 993, 446), (0, 590, 714), (1, 1004, 358), (2, 1919, 1079),\n",
    "    (1, 900, 124), (2, 1919, 1079), (0, 877, 505), (0, 875, 340), (1, 1220, 681), (2, 1919, 1084), (1, 866, 830), (2, 1919, 1081), (0, 946, 271), (2, 1919, 1080),\n",
    "    (2, 1919, 1079), (0, 929, 180), (2, 1919, 1081), (2, 1919, 1081), (0, 883, 522), (1, 769, 844), (1, 726, 712), (1, 821, 809), (2, 1919, 1090), (2, 1919, 1090),\n",
    "    (2, 1919, 1079), (2, 1919, 1080), (1, 631, 534), (0, 953, 906), (1, 791, 658), (1, 860, 610), (1, 611, 757), (1, 691, 559), (2, 1919, 1092), (0, 890, 642),\n",
    "    (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (0, 1190, 1065), (0, 1198, 1062), (0, 1252, 1062), (0, 872, 476), (0, 1192, 1062),\n",
    "    (0, 1080, 481), (1, 1265, 412), (0, 1220, 1057), (1, 1715, 995), (2, 1919, 1080), (1, 1732, 982), (1, 1732, 984), (1, 1728, 982), (1, 1727, 986), (1, 1350, 96), # 1200\n",
    "    (1, 860, 451), (0, 881, 267), (2, 1919, 1079), (1, 821, 503), (1, 1136, 174), (2, 1919, 1080), (2, 1919, 1079), (0, 1240, 358), (2, 1919, 1081), (2, 1919, 1079),\n",
    "    (2, 1919, 1080), (1, 868, 545), (2, 1919, 1079), (1, 1173, 358), (2, 1919, 1081), (1, 814, 764), (2, 1919, 1079), (2, 1919, 1080), (2, 1919, 1079), (2, 1919, 1081),\n",
    "    (2, 1919, 1079), (2, 1919, 1082), (2, 1919, 1080), (1, 1069, 500), (1, 1074, 543), (0, 890, 500), (2, 1919, 1079), (0, 1166, 189), (1, 1078, 235), (1, 1192, 291),\n",
    "    (1, 1009, 302), (2, 1919, 1079), (2, 1919, 1081), (1, 963, 299), (1, 1021, 394), (1, 1106, 446), (1, 1354, 468), (1, 1253, 476), (1, 1272, 429), (1, 1274, 474),\n",
    "    (1, 1117, 220), (1, 1136, 196), (1, 1281, 591), (1, 1181, 271), (1, 937, 234), (1, 937, 237), (1, 936, 238), (1, 954, 258), (1, 927, 243), (1, 900, 265),\n",
    "    (1, 918, 269), (1, 922, 276), (1, 918, 263), (1, 939, 250), (1, 1919, 1080), (0, 877, 502), (1, 579, 679), (1, 1104, 353), (1, 1063, 424), (1, 622, 578),\n",
    "    (1, 1090, 342), (2, 1919, 1079), (2, 1919, 1080), (0, 881, 511), (1, 1144, 278), (1, 1188, 191), (1, 574, 623), (1, 628, 796), (2, 1919, 1081), (0, 931, 394),\n",
    "    (1, 698, 749), (1, 721, 697), (1, 670, 748), (1, 1021, 263), (2, 1919, 1080), (1, 847, 783), (1, 698, 861), (1, 656, 740), (1, 650, 740), (1, 786, 779),\n",
    "    (1, 1069, 818), (1, 1030, 712), (2, 1919, 1085), (0, 1229, 1061), (0, 1253, 1067), (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (0, 1212, 1051),\n",
    "    (0, 1248, 1049), (0, 1361, 319), (1, 1367, 334), (1, 1367, 327), (0, 1147, 973), (0, 592, 820), (0, 877, 500), (0, 1058, 526), (1, 1274, 457), (0, 1220, 1061), # 1300\n",
    "    (1, 1745, 975), (1, 1749, 975), (1, 1743, 973), (1, 1743, 971), (2, 1919, 1080), (1, 1745, 973), (1, 1743, 975), (1, 1309, 207), (2, 1919, 2079), (0, 1155, 254),\n",
    "    (2, 1919, 1080), (0, 877, 481), (2, 1919, 2079), (0, 1155, 256), (2, 1919, 1082), (2, 1919, 1080), (0, 888, 908), (2, 1919, 1079), (2, 1919, 1081), (2, 1919, 1080),\n",
    "    (1, 959, 252), (0, 1075, 379), (1, 1101, 450), (1, 1075, 420), (1, 598, 701), (1, 1114, 430), (2, 1919, 1080), (2, 1919, 1081), (0, 1116, 478), (1, 1274, 256),\n",
    "    (1, 1341, 323), (2, 1919, 1080), (1, 909, 275), (2, 1919, 1079), (0, 1231, 252), (2, 1919, 1081), (2, 1919, 1079), (0, 1091, 278), (1, 1099, 314), (1, 1130, 280),\n",
    "    (1, 1157, 213), (1, 1201, 185), (1, 901, 282), (1, 972, 709), (1, 806, 835), (2, 1919, 1080), (1, 1335, 472), (0, 931, 323), (1, 1587, 489), (1, 1129, 779),\n",
    "    (2, 1919, 1080), (2, 1919, 1079), (2, 1919, 1081), (1, 700, 435), (2, 1919, 1080), (2, 1919, 1079), (1, 1043, 312), (1, 1583, 569), (1, 1214, 496), (2, 1919, 1079),\n",
    "    (2, 1919, 1081), (0, 881, 492), (1, 1013, 492), (2, 1919, 1079), (0, 836, 194), (2, 1919, 1081), (0, 879, 489), (1, 857, 874), (2, 1919, 1080), (1, 846, 833),\n",
    "    (2, 1919, 1079), (0, 875, 507), (1, 922, 152), (1, 918, 230), (1, 1112, 299), (2, 1919, 1080), (1, 693, 295), (1, 717, 407), (2, 1919, 1080), (1, 840, 327),\n",
    "    (0, 875, 507), (1, 894, 235), (1, 801, 481), (1, 1002, 420), (1, 989, 338), (1, 877, 247), (1, 767, 174), (1, 819, 254), (1, 786, 243), (1, 782, 247),\n",
    "    (1, 790, 241), (1, 797, 263), (1, 791, 239), (1, 907, 740), (2, 1919, 1085), (0, 1231, 1053), (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), (2, 1919, 1085), # 1400\n",
    "    (2, 1919, 1085), (0, 1224, 1064), (1, 1356, 288), (1, 493, 334), (0, 1190, 1057), (1, 1160, 62), (1, 1255, 58), (1, 1345, 131), (1, 1397, 271), (2, 1919, 1079),\n",
    "    (2, 1919, 1080), (0, 1404, 448), (2, 1919, 1081), (0, 892, 472), (1, 1373, 358), (2, 1919, 1079), (0, 1123, 524), (1, 1125, 502), (2, 1919, 1079), (1, 1266, 252),\n",
    "    (1, 1088, 325), (2, 1919, 1080), (1, 1214, 120), (2, 1919, 1079), (0, 814, 273), (1, 1220, 152), (2, 1919, 1080), (1, 1197, 176), (2, 1919, 1083), (0, 1224, 349),\n",
    "    (2, 1919, 1090), (2, 1919, 1090), (2, 1919, 1082), (2, 1919, 1079), (1, 1071, 669), (2, 1919, 1081), (1, 696, 658), (0, 1153, 1051), (0, 1155, 1055), (0, 1152, 1048),\n",
    "    (0, 1280, 1060), (0, 1204, 1038), (0, 1220, 1050), (0, 1240, 1061), (0, 1223, 1057), (0, 1225, 1050), (0, 1191, 1055), (0, 1195, 1052), (0, 1199, 1055), (0, 1167, 1049),\n",
    "    (0, 1200, 1058), (0, 1188, 1060), (0, 1180, 1049), (0, 1177, 1047), (0, 860, 561), (0, 488, 332), (1, 493, 343), (0, 1183, 1064), (0, 1185, 1063), (0, 1160, 1050),\n",
    "    (0, 1280, 1060), (0, 1204, 1038), (0, 1220, 1050), (0, 1240, 1061), (0, 1223, 1057), (0, 1225, 1050), (0, 1191, 1055), (0, 1195, 1052), (0, 1199, 1055), (0, 1167, 1049),\n",
    "    (0, 1280, 1060), (0, 1204, 1038), (0, 1220, 1050), (0, 1240, 1061), (0, 1223, 1057), (0, 1225, 1050), (0, 1191, 1055), (0, 1195, 1052), (0, 1199, 1055), (0, 1167, 1049),\n",
    "    (0, 1280, 1060), (0, 1204, 1038), (0, 1220, 1050), (0, 1240, 1061), (0, 1223, 1057), (0, 1225, 1050), (0, 1191, 1055), (0, 1195, 1052), (0, 1199, 1055), (0, 1167, 1049),\n",
    "    (0, 1280, 1060), (0, 1204, 1038), (0, 1220, 1050), (0, 1240, 1061), (0, 1223, 1057), (0, 1225, 1050), (0, 1191, 1055), (0, 1195, 1052), (0, 1199, 1055), (0, 1167, 1049), # 1500\n",
    "    (0, 1188, 1058), (0, 1207, 1042), (0, 1203, 1049), (0, 1205, 1053), (0, 1230, 1054), (1, 1309, 42), (1, 1717, 997), (1, 1363, 137), (1, 1730, 985), (1, 1738, 982),\n",
    "    (1, 1361, 265), (1, 1268, 410), (1, 1244, 308), (1, 1175, 312), (2, 1919, 1079), (2, 1919, 1081), (0, 872, 485), (2, 1919, 1079), (0, 1093, 226), (2, 1919, 1082),\n",
    "    (2, 1919, 1079), (1, 1160, 358), (0, 1157, 399), (2, 1919, 1080), (1, 981, 362), (1, 1104, 315), (2, 1919, 1079), (0, 1244, 219), (2, 1919, 1081), (2, 1919, 1080),\n",
    "    (1, 1125, 351), (1, 1483, 79), (1, 1307, 275), (1, 1263, 226), (1, 1246, 234), (1, 1436, 135), (1, 844, 193), (2, 1919, 1092), (0, 471, 345), (1, 1429, 388),\n",
    "    (1, 1233, 299), (2, 1919, 1079), (2, 1919, 1081), (1, 1062, 401), (1, 1026, 372), (2, 1919, 1079), (2, 1919, 1080), (2, 1919, 1081), (0, 983, 319), (1, 864, 319),\n",
    "    (1, 1399, 183), (2, 1919, 1080), (1, 1177, 215), (1, 1382, 181), (1, 1337, 269), (1, 1473, 187), (2, 1919, 1080), (1, 1132, 323), (1, 1149, 338), (1, 1125, 312),\n",
    "    (1, 1123, 302), (1, 1125, 308), (0, 993, 202), (1, 1302, 390), (1, 1919, 1079), (0, 1341, 200), (1, 929, 416), (2, 1919, 1080), (2, 1919, 1081), (2, 1919, 1079),\n",
    "    (0, 1201, 355), (2, 1919, 1084), (2, 1919, 1090), (2, 1919, 1090), (2, 1919, 1090), (0, 1281, 1061), (0, 1277, 1056), (0, 1278, 1057), (0, 1199, 1042), (0, 1202, 1047),\n",
    "    (0, 1280, 1060), (0, 1204, 1038), (0, 1220, 1050), (0, 1240, 1061), (0, 1223, 1057), (0, 1225, 1050), (0, 1191, 1055), (0, 1195, 1052), (0, 1199, 1055), (0, 1167, 1049),\n",
    "    (0, 1280, 1060), (0, 1204, 1038), (0, 1220, 1050), (0, 1240, 1061), (0, 1223, 1057), (0, 1225, 1050), (0, 1191, 1055), (0, 1195, 1052), (0, 1199, 1055), (0, 1167, 1049), # 1600\n",
    "    (), (), (), (), (), (), (), (), (), () \n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), () # 1700\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), () # 1800\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), () # 1900\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), ()\n",
    "    (), (), (), (), (), (), (), (), (), () # 2000\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = [(0, 0, 3), (0, 0, 4), (0, 0, 6), (0, 0, 2), (0, 0, 0), (0, 0, 0), (0, 1, 0), (0, 0, 2), (0, 0, 6), (0, 0, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_data = torch.from_numpy(images_data)\n",
    "images_data = images_data.view(images_data.size(0), images_data.size(3), images_data.size(1), images_data.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done! Train the vectorizer and then use it to generate the input mapping dictionary\n"
     ]
    }
   ],
   "source": [
    "dataset.use_readymade_data(images_data, commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 7])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.encoded_command_type.size())\n",
    "print(dataset.encoded_actions1.size())\n",
    "print(dataset.encoded_actions2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN fitted in actions 1\n",
      "KNN fitted in actions 2\n",
      "All action maps have been properly fitted by their respective KNN algorithm\n"
     ]
    }
   ],
   "source": [
    "# Bullet Heaven\n",
    "\n",
    "command_types = ['move', 'click', 'rightclick']\n",
    "\n",
    "actions1 = [i for i in range(1, 1919)] # Avoiding using the extremes so we don't have to shut down PyAutoGUI safety lock.\n",
    "\n",
    "actions2 = [i for i in range(1, 1079)]\n",
    "\n",
    "dataset = Dataset(command_types, actions1, actions2, explore_train_steps=100, memory_size=100, resize=(200,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['key']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.command_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bullet Heaven screen regions\n",
    "\n",
    "\"Score\": (180, 1, 213, 249),\n",
    "#\"Score\": (36, 154, 74, 418),\n",
    "\"Next Level\": (951, 1153, 1028, 1484),\n",
    "\"Restart\": (950, 685, 1024, 928)\n",
    "#\"Rank\": (125, 1742, 177, 1851), \n",
    "#\"Hearts\": (794, 1671, 889, 1920), # Might not work\n",
    "#\"Bombs\": (917, 1671, 1025, 1920) # Might not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# League of Legends --- Intense use of mouse ---> labels might be too heavy\n",
    "\n",
    "command_types = ['click', 'rightclick', 'key']\n",
    "\n",
    "actions1 = [i for i in range(100, 1900)] # Avoiding using the extremes so we don't have to shut down PyAutoGUI safety lock.\n",
    "\n",
    "actions1 = actions1 + ['press']\n",
    "\n",
    "actions2 = [i for i in range(90, 1000)]\n",
    "\n",
    "keyboard_commands = [\n",
    "    'q', 'w', 'e', 'r',\n",
    "    'd', 'f', 'b',\n",
    "    '1', '2', '3', '4', '5', '6', '7', 'space'\n",
    "]\n",
    "\n",
    "\n",
    "actions2 = actions2 + keyboard_commands\n",
    "\n",
    "dataset = Dataset(command_types, actions1, actions2, explore_train_steps=10000, memory_size=2000, resize=(200,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN fitted in actions 1\n",
      "KNN fitted in actions 2\n",
      "All action maps have been properly fitted by their respective KNN algorithm\n"
     ]
    }
   ],
   "source": [
    "# Eternal Return - intense use of mouse commands --> labels might be too heavy\n",
    "\n",
    "mouse_commands = ['click', 'rightclick'] # mouse.move is already included in click and right click. It won't be necessary here.\n",
    "\n",
    "keyboard_commands = [\n",
    "    'q', 'w', 'e', 'r',\n",
    "    'd', 'f', 'x', \n",
    "    '1', '2', '3', '4', '5', '6', '7', '8', '9'\n",
    "]\n",
    "\n",
    "command_types = ['click', 'rightclick', 'key']\n",
    "\n",
    "actions1 = [i for i in range(90, 1900)]\n",
    "actions1 = actions1 + ['press']\n",
    "\n",
    "actions2 = [i for i in range(70, 950)]\n",
    "actions2 = actions2 + keyboard_commands\n",
    "\n",
    "\n",
    "dataset = Dataset(command_types, actions1, actions2, explore_train_steps=100, memory_size=100, resize=(200,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hakisa = Hakisa(command_type, actions1, actions2, mode='Explore').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hakisa = Hakisa(n_command_types=3, mode='Explore').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.command_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Jigoku(score):\n",
    "    # For the game Jigoku Kisetsukan: Sense of the Seasons\n",
    "\n",
    "    score = score.replace('S', '5').replace('s', '8').replace('e', '2').replace('O', '0').replace('B', '8').replace('o', '4').replace('b', '4')\n",
    "    score = score.replace('I', '1').replace('l', '1').replace('.', '')\n",
    "\n",
    "    try:\n",
    "        score = float(score)\n",
    "\n",
    "    except ValueError:\n",
    "            score = 1.0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_BH2(score):\n",
    "    # For the game Bullet Heaven 2\n",
    "\n",
    "    score = score.replace('S', '5').replace('s', '8').replace('e', '2').replace('O', '0').replace('B', '8').replace('o', '0').replace('.', '')\n",
    "    score = sub('[^0-9]', '', score)\n",
    "\n",
    "    try:\n",
    "        score = float(score)\n",
    "\n",
    "    except ValueError:\n",
    "        score = 1.0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eternal Return - Preprocess\n",
    "\n",
    "def preprocess_ER(score):\n",
    "\n",
    "    score = score.replace('w', '1').replace('f', '/').replace('y', '1').replace('e', '2').replace('o', '0')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# League of Legends - Preprocess\n",
    "\n",
    "def preprocess_LoL_KDA(kda):\n",
    "\n",
    "    kda = kda.replace(')', '5').replace('O', '0').replace('B', '8').replace('o', '0')\n",
    "\n",
    "    kda = kda.split('/')\n",
    "\n",
    "    try:\n",
    "        kills = kda[0]\n",
    "        deaths = kda[1]\n",
    "        assists = kda[2]\n",
    "    \n",
    "    except IndexError:\n",
    "        kills = 0\n",
    "        deaths = 1\n",
    "        assists = 0\n",
    "\n",
    "    try:\n",
    "        kills = int(kills)\n",
    "\n",
    "    except ValueError:\n",
    "        kills = 0\n",
    "\n",
    "    try:\n",
    "        deaths = int(deaths)\n",
    "\n",
    "    except ValueError:\n",
    "        deaths = 0\n",
    "\n",
    "    try:\n",
    "        assists = int(assists)\n",
    "\n",
    "    except ValueError:\n",
    "        assists = 0\n",
    "\n",
    "    return kills, deaths, assists\n",
    "\n",
    "def preprocess_LoL_farm(farm):\n",
    "\n",
    "    farm = farm.replace(')', '5').replace('O', '0').replace('B', '8').replace('o', '0')\n",
    "\n",
    "    try:\n",
    "        farm = int(farm)\n",
    "\n",
    "    except ValueError:\n",
    "        farm = 0\n",
    "\n",
    "    return farm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step complete! 3.1053524017333984\n",
      "Step complete! 1.1490864753723145\n",
      "Step complete! 1.2049307823181152\n",
      "Step complete! 1.1546459197998047\n",
      "Step complete! 1.242347002029419\n",
      "Step complete! 1.1627273559570312\n",
      "Step complete! 1.2535831928253174\n",
      "Step complete! 1.1614558696746826\n",
      "Step complete! 1.1721348762512207\n",
      "Step complete! 1.2026121616363525\n",
      "Step complete! 1.3177871704101562\n",
      "Step complete! 1.419440746307373\n",
      "Step complete! 1.1799516677856445\n",
      "Step complete! 1.2060444355010986\n",
      "Step complete! 1.1519408226013184\n",
      "Step complete! 1.2051591873168945\n",
      "Step complete! 1.1098694801330566\n",
      "Step complete! 1.1699788570404053\n",
      "Step complete! 1.3901948928833008\n",
      "Step complete! 1.1433022022247314\n",
      "Step complete! 1.175487995147705\n",
      "Step complete! 1.2000741958618164\n",
      "Step complete! 1.161041259765625\n",
      "Step complete! 1.1995055675506592\n",
      "Step complete! 1.2251219749450684\n",
      "Step complete! 1.234760046005249\n",
      "Step complete! 1.3327898979187012\n",
      "Step complete! 1.170922040939331\n",
      "Step complete! 1.2980163097381592\n",
      "Step complete! 1.2090215682983398\n",
      "Step complete! 1.168766736984253\n",
      "Step complete! 1.2600905895233154\n",
      "Step complete! 1.1520576477050781\n",
      "Step complete! 1.2160677909851074\n",
      "Step complete! 1.2158052921295166\n",
      "Step complete! 1.2259135246276855\n",
      "Step complete! 1.1743323802947998\n",
      "Step complete! 1.2559399604797363\n",
      "Step complete! 1.176269292831421\n",
      "Step complete! 1.213426113128662\n",
      "Step complete! 1.2203664779663086\n",
      "Step complete! 1.2401487827301025\n",
      "Step complete! 1.1692204475402832\n",
      "Step complete! 1.1507952213287354\n",
      "Step complete! 0.9799139499664307\n",
      "Step complete! 1.1319999694824219\n",
      "Step complete! 1.2381396293640137\n",
      "Step complete! 1.1907837390899658\n",
      "Step complete! 1.1990652084350586\n",
      "Step complete! 1.2296442985534668\n",
      "Step complete! 1.1766047477722168\n",
      "Step complete! 1.1902439594268799\n",
      "Step complete! 1.2098917961120605\n",
      "Step complete! 1.2301456928253174\n",
      "Step complete! 1.1401360034942627\n",
      "Step complete! 1.1579153537750244\n",
      "Step complete! 1.2015841007232666\n",
      "Step complete! 1.1792855262756348\n",
      "Step complete! 1.1920549869537354\n",
      "Step complete! 1.185701608657837\n",
      "Step complete! 1.207324504852295\n",
      "Step complete! 1.124845266342163\n",
      "Step complete! 1.1490042209625244\n",
      "Step complete! 1.2438585758209229\n",
      "Step complete! 1.1410260200500488\n",
      "Step complete! 1.223130464553833\n",
      "Step complete! 1.1727252006530762\n",
      "Step complete! 1.1200668811798096\n",
      "Step complete! 1.2203562259674072\n",
      "Step complete! 1.178741216659546\n",
      "Step complete! 1.1212432384490967\n",
      "Step complete! 1.1700413227081299\n",
      "Step complete! 1.280181646347046\n",
      "Step complete! 1.230051040649414\n",
      "Step complete! 1.220400094985962\n",
      "Step complete! 1.1520588397979736\n",
      "Step complete! 1.0270609855651855\n",
      "Step complete! 1.118135929107666\n",
      "Step complete! 1.1247649192810059\n",
      "Step complete! 1.2280116081237793\n",
      "Step complete! 1.1398487091064453\n",
      "Step complete! 1.1403791904449463\n",
      "Step complete! 1.2250065803527832\n",
      "Step complete! 1.1646819114685059\n",
      "Step complete! 1.3036491870880127\n",
      "Step complete! 1.1916248798370361\n",
      "Step complete! 1.2380621433258057\n",
      "Step complete! 1.3117516040802002\n",
      "Step complete! 1.1851651668548584\n",
      "Step complete! 1.137974739074707\n",
      "Step complete! 1.2043755054473877\n",
      "Step complete! 1.1787889003753662\n",
      "Step complete! 1.2978510856628418\n",
      "Step complete! 1.1407041549682617\n",
      "Step complete! 1.1101372241973877\n",
      "Step complete! 1.1998512744903564\n",
      "Step complete! 1.160050630569458\n",
      "Step complete! 1.2500782012939453\n",
      "Step complete! 1.1695468425750732\n",
      "Step complete! 1.2519519329071045\n",
      "Loop complete!\n",
      "Time spent: 129.32867813110352 seconds\n",
      "Number of steps: 99\n"
     ]
    }
   ],
   "source": [
    "# Exploration loop\n",
    "\n",
    "from win32gui import GetWindowText, GetForegroundWindow # To make sure she's playing your game\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "game_window = GetWindowText(GetForegroundWindow())\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok.\n",
    "\n",
    "reward = 0\n",
    "\n",
    "for step in range(dataset.steps):\n",
    "\n",
    "    if GetWindowText(GetForegroundWindow()) != game_window:\n",
    "        print(\"I don't want to play anymore!\")\n",
    "        break\n",
    "\n",
    "    start_step = time()\n",
    "\n",
    "    frame = dataset._grab_frame() # Getting environment state\n",
    "\n",
    "    cmds = hakisa()\n",
    "\n",
    "    command = dataset.get_command(cmds[0].cpu().numpy(), cmds[1].cpu().numpy(), cmds[2].cpu().numpy(), exploration=True)\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    # If Hakisa acts too fast, the consequence for her action might not appear right now. Not that I wouldn't want her to be fast and efficient...\n",
    "\n",
    "    score = dataset.get_consequences(1008, 1429, 1723-1429, 1046-1008, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_Jigoku(score)\n",
    "\n",
    "    mult_score = dataset.get_consequences(933, 1536, 1723-1536, 978-933, tesseract_config='--psm 8')\n",
    "\n",
    "    mult_score = preprocess_Jigoku(mult_score)\n",
    "    mult_score = mult_score/100\n",
    "\n",
    "    life = dataset.get_consequences(849, 400, 498-400, 904-849, tesseract_config='--psm 8')\n",
    "\n",
    "    life = preprocess_Jigoku(life)\n",
    "    life = life/100\n",
    "\n",
    "    power = dataset.get_consequences(923, 405, 503-405, 978-923, tesseract_config='--psm 8')\n",
    "\n",
    "    power = preprocess_Jigoku(power)\n",
    "    power = power/100\n",
    "\n",
    "    aura = dataset.get_consequences(1001, 400, 1045-1001, 503-400, tesseract_config='--psm 8')\n",
    "\n",
    "    aura = preprocess_Jigoku(aura)\n",
    "    aura = aura/100\n",
    "\n",
    "    if life == 0:\n",
    "\n",
    "        try:\n",
    "            reward += -(100/(score * mult_score))\n",
    "        \n",
    "        except ZeroDivisionError:\n",
    "            reward += -10\n",
    "\n",
    "    else:\n",
    "\n",
    "        reward += ((score * mult_score) + (power * aura))*1e-6 # Jigoku Kisetsukan deals with score numbers around hundreds of thousands.\n",
    "\n",
    "    dataset.create_memory(frame, keys=command, values=(cmds[0].cpu(), cmds[1].cpu(), cmds[2].cpu()), reward=reward)\n",
    "\n",
    "    end_step = time()\n",
    "\n",
    "    print(f\"Step complete! {end_step-start_step}\")\n",
    "\n",
    "    if step == (dataset.steps - 1):\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en.wav', winsound.SND_FILENAME) # Because yes\n",
    "\n",
    "end = time()\n",
    "print(f\"Loop complete!\\nTime spent: {end-start} seconds\\nNumber of steps: {step}\")\n",
    "\n",
    "del frame, command, score, mult_score, life, power, aura, reward, step, start, end\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_rest_1_en.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step complete! 9.375584125518799\n",
      "Step complete! 9.755366086959839\n",
      "Step complete! 9.327844619750977\n",
      "Step complete! 9.362699270248413\n",
      "Step complete! 9.022846460342407\n",
      "Step complete! 8.908733367919922\n",
      "Step complete! 3.6130123138427734\n",
      "I don't want to play anymore!\n",
      "Loop complete!\n",
      "Time spent: 68.7579116821289 seconds\n",
      "Number of steps: 7\n"
     ]
    }
   ],
   "source": [
    "# BULLET HEAVEN\n",
    "\n",
    "from win32gui import GetWindowText, GetForegroundWindow # To make sure she's playing your game\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "start = time()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "game_window = GetWindowText(GetForegroundWindow())\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "reward = 0\n",
    "\n",
    "for step in range(dataset.steps):\n",
    "\n",
    "    if GetWindowText(GetForegroundWindow()) != game_window:\n",
    "        print(\"I don't want to play anymore!\")\n",
    "        break\n",
    "\n",
    "    start_step = time()\n",
    "\n",
    "    frame = dataset._grab_frame() # Getting environment state\n",
    "\n",
    "    cmds = hakisa()\n",
    "\n",
    "    command = dataset.get_command(cmds[0].cpu().numpy(), cmds[1].cpu().numpy(), cmds[2].cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    # If Hakisa acts too fast, the consequence for her action might not appear right now. Not that I wouldn't want her to be fast and efficient...\n",
    "\n",
    "    score = dataset.get_consequences(180, 1, 249-1, 213-180, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_BH2(score)\n",
    "\n",
    "    try:\n",
    "\n",
    "        reward += math.log10(score)\n",
    "    \n",
    "    except:\n",
    "\n",
    "        reward += 0.0\n",
    "\n",
    "    dataset.create_memory(frame, keys=command, values=(cmds[0].cpu(), cmds[1].cpu(), cmds[2].cpu()), reward=reward)\n",
    "\n",
    "    end_step = time()\n",
    "\n",
    "    print(f\"Step complete! {end_step-start_step}\")\n",
    "\n",
    "    if step == (dataset.steps - 1):\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en.wav', winsound.SND_FILENAME) # Because yes\n",
    "\n",
    "end = time()\n",
    "print(f\"Loop complete!\\nTime spent: {end-start} seconds\\nNumber of steps: {step}\")\n",
    "\n",
    "del frame, command, score, reward, step, start, end\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_rest_1_en.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETERNAL RETURN\n",
    "\n",
    "from win32gui import GetWindowText, GetForegroundWindow # To make sure she's playing your game\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "game_window = GetWindowText(GetForegroundWindow())\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "reward = 0.\n",
    "\n",
    "\n",
    "for step in range(dataset.steps):\n",
    "\n",
    "    if GetWindowText(GetForegroundWindow()) != game_window:\n",
    "        print(\"I don't want to play anymore!\")\n",
    "        break\n",
    "\n",
    "    start_step = time()\n",
    "\n",
    "    frame = dataset._grab_frame() # Getting environment state\n",
    "\n",
    "    cmds = hakisa()\n",
    "\n",
    "    command = dataset.get_command(cmds[0].cpu().numpy(), cmds[1].cpu().numpy(), cmds[2].cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    # If Hakisa acts too fast, the consequence for her action might not appear right now. Not that I wouldn't want her to be fast and efficient...\n",
    "\n",
    "    hp = dataset.get_consequences(1036, 845, 908-845, 1050-1036, togray=True, threshold=True, thresh_gauss=7, thresh_C=-13, tesseract_config='--psm 6')\n",
    "\n",
    "    hp = preprocess_ER(hp)\n",
    "    hp = hp.split('/')\n",
    "\n",
    "    current_hp, max_hp = hp[0], hp[1]\n",
    "\n",
    "    del hp\n",
    "\n",
    "    try:\n",
    "        current_hp, max_hp = float(current_hp), float(max_hp)\n",
    "\n",
    "    except ValueError:\n",
    "        current_hp, max_hp = 0.0, 1000.0\n",
    "\n",
    "    # Dispensando SP por incapacidade do OCR e thresholding\n",
    "\n",
    "    # No Need for thresholding/grayscale here\n",
    "\n",
    "    team_kills = dataset.get_consequences(3, 1611, 1661-1611, 35-3, tesseract_config='--psm 6')\n",
    "\n",
    "    team_kills = preprocess_ER(team_kills).replace(\"TK\",'')\n",
    "\n",
    "    try:\n",
    "        team_kills = float(team_kills)\n",
    "\n",
    "    except ValueError:\n",
    "        team_kills = 0\n",
    "\n",
    "    # Usando apenas team kills, pois no modo solo --> solo kill = team kill. No Cobalt, kills pessoais so menos relevantes q kills do time.\n",
    "\n",
    "    assists = dataset.get_consequences(3, 1711, 1761-1711, 35-3, tesseract_config='--psm 6')\n",
    "\n",
    "    assists = preprocess_ER(assists).replace(\"A\", '')\n",
    "\n",
    "    try:\n",
    "        assists = float(assists)\n",
    "\n",
    "    except ValueError:\n",
    "        assists = 0\n",
    "\n",
    "    farm = dataset.get_consequences(3, 1761, 1821-1761, 35-3, tesseract_config='--psm 6')\n",
    "\n",
    "    farm = preprocess_ER(farm).replace(\"H\", '')\n",
    "\n",
    "    try:\n",
    "        farm = float(farm)\n",
    "\n",
    "    except ValueError:\n",
    "        farm = 0\n",
    "    \n",
    "    farm = farm/4.\n",
    "\n",
    "    if current_hp > 0:\n",
    "\n",
    "        reward += (farm+team_kills+assists)*(current_hp/max_hp)\n",
    "\n",
    "        #reward = (farm+kills+assists)*(hp+sp)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        countdown = dataset.get_consequences(56, 961, 990-961, 79-56, tesseract_config='--psm 6')\n",
    "        countdown = preprocess_ER(countdown)\n",
    "        try:\n",
    "            countdown = float(countdown)\n",
    "        \n",
    "        except ValueError:\n",
    "            countdown = 0\n",
    "\n",
    "        reward += (-1*(90-countdown)) - (1/(team_kills+assists+farm+1))\n",
    "\n",
    "        del countdown\n",
    "\n",
    "    del current_hp, max_hp, team_kills, assists, farm\n",
    "\n",
    "    dataset.create_memory(frame, keys=command, values=(cmds[0].cpu(), cmds[1].cpu(), cmds[2].cpu()), reward=reward)\n",
    "\n",
    "    end_step = time()\n",
    "\n",
    "    print(f\"Step complete! {end_step-start_step}\")\n",
    "\n",
    "    if step == (dataset.steps - 1):\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en.wav', winsound.SND_FILENAME) # Because yes\n",
    "\n",
    "end = time()\n",
    "print(f\"Loop complete!\\nTime spent: {end-start} seconds\\nNumber of steps: {step}\")\n",
    "\n",
    "del frame, command, reward, step, start, end\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_rest_1_en.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step complete! 0.7439885139465332\n",
      "Step complete! 0.5330026149749756\n",
      "Step complete! 0.5349900722503662\n",
      "Step complete! 0.5559985637664795\n",
      "Step complete! 0.5380043983459473\n",
      "Step complete! 0.7139954566955566\n",
      "Step complete! 0.5610103607177734\n",
      "Step complete! 0.7859928607940674\n",
      "Step complete! 0.8009991645812988\n",
      "Step complete! 0.7060012817382812\n",
      "Step complete! 0.5770018100738525\n",
      "Step complete! 0.807997465133667\n",
      "Step complete! 0.8249979019165039\n",
      "Step complete! 0.5700061321258545\n",
      "Step complete! 0.5260024070739746\n",
      "Step complete! 0.7465052604675293\n",
      "Step complete! 0.7459936141967773\n",
      "Step complete! 0.5249898433685303\n",
      "Step complete! 0.7438862323760986\n",
      "Step complete! 0.5189604759216309\n",
      "Step complete! 0.5955173969268799\n",
      "Step complete! 0.5760056972503662\n",
      "Step complete! 0.5940136909484863\n",
      "Step complete! 0.8669884204864502\n",
      "Step complete! 0.790001392364502\n",
      "Step complete! 0.7800028324127197\n",
      "Step complete! 0.5589950084686279\n",
      "Step complete! 0.5185248851776123\n",
      "Step complete! 0.7290010452270508\n",
      "Step complete! 0.5180127620697021\n",
      "Step complete! 0.7679862976074219\n",
      "Step complete! 0.5530006885528564\n",
      "Step complete! 0.5019996166229248\n",
      "Step complete! 0.7550029754638672\n",
      "Step complete! 0.7645359039306641\n",
      "Step complete! 0.5689868927001953\n",
      "Step complete! 0.7720015048980713\n",
      "Step complete! 0.788999080657959\n",
      "Step complete! 0.5989987850189209\n",
      "Step complete! 0.7850024700164795\n",
      "Step complete! 0.7500073909759521\n",
      "Step complete! 0.7445132732391357\n",
      "Step complete! 0.5180082321166992\n",
      "Step complete! 0.5169985294342041\n",
      "Step complete! 0.5499999523162842\n",
      "Step complete! 0.7749969959259033\n",
      "Step complete! 0.7849993705749512\n",
      "Step complete! 0.496997594833374\n",
      "Step complete! 0.7460012435913086\n",
      "Step complete! 0.7695291042327881\n",
      "Step complete! 0.5289983749389648\n",
      "Step complete! 0.8219943046569824\n",
      "Step complete! 0.6019997596740723\n",
      "Step complete! 0.7330009937286377\n",
      "Step complete! 0.7259988784790039\n",
      "Step complete! 0.5695219039916992\n",
      "Step complete! 0.7209982872009277\n",
      "Step complete! 0.7487165927886963\n",
      "Step complete! 0.7130086421966553\n",
      "Step complete! 0.7669970989227295\n",
      "Step complete! 0.7579977512359619\n",
      "Step complete! 0.5790035724639893\n",
      "Step complete! 0.7789981365203857\n",
      "Step complete! 0.7449893951416016\n",
      "Step complete! 0.5740029811859131\n",
      "Step complete! 0.7909986972808838\n",
      "Step complete! 0.5489981174468994\n",
      "Step complete! 0.5198342800140381\n",
      "Step complete! 0.7560052871704102\n",
      "Step complete! 0.7320001125335693\n",
      "Step complete! 0.5485255718231201\n",
      "Step complete! 0.5439889430999756\n",
      "Step complete! 0.7410144805908203\n",
      "Step complete! 0.547982931137085\n",
      "Step complete! 0.7429986000061035\n",
      "Step complete! 0.6249990463256836\n",
      "Step complete! 0.5660004615783691\n",
      "Step complete! 0.751518964767456\n",
      "Step complete! 0.7609994411468506\n",
      "Step complete! 0.7529995441436768\n",
      "Step complete! 0.5150010585784912\n",
      "Step complete! 0.7310028076171875\n",
      "Step complete! 0.74300217628479\n",
      "Step complete! 0.5409929752349854\n",
      "Step complete! 0.7310185432434082\n",
      "Step complete! 0.5235254764556885\n",
      "Step complete! 0.5369997024536133\n",
      "Step complete! 0.758000373840332\n",
      "Step complete! 0.7619979381561279\n",
      "Step complete! 0.7759971618652344\n",
      "Step complete! 0.8825178146362305\n",
      "Step complete! 0.5610041618347168\n",
      "Step complete! 0.5719945430755615\n",
      "Step complete! 0.5380008220672607\n",
      "Step complete! 0.710766077041626\n",
      "Step complete! 0.7290048599243164\n",
      "Step complete! 0.541999340057373\n",
      "Step complete! 0.4960017204284668\n",
      "Step complete! 0.7389955520629883\n",
      "Step complete! 0.7880005836486816\n",
      "Step complete! 0.547003984451294\n",
      "Step complete! 0.7589983940124512\n",
      "Step complete! 0.7400021553039551\n",
      "Step complete! 0.7579967975616455\n",
      "Step complete! 0.7580020427703857\n",
      "Step complete! 0.5359988212585449\n",
      "Step complete! 0.5759990215301514\n",
      "Step complete! 0.7480013370513916\n",
      "Step complete! 0.5060000419616699\n",
      "Step complete! 0.5155203342437744\n",
      "Step complete! 0.5050005912780762\n",
      "Step complete! 0.7110016345977783\n",
      "Step complete! 0.4969968795776367\n",
      "Step complete! 0.7269980907440186\n",
      "Step complete! 0.5499961376190186\n",
      "Step complete! 0.530010461807251\n",
      "Step complete! 0.5539999008178711\n",
      "Step complete! 0.5509974956512451\n",
      "Step complete! 0.5455141067504883\n",
      "Step complete! 0.5399982929229736\n",
      "Step complete! 0.7202951908111572\n",
      "Step complete! 0.527001142501831\n",
      "Step complete! 0.5060031414031982\n",
      "Step complete! 0.7239973545074463\n",
      "Step complete! 0.705996036529541\n",
      "Step complete! 0.7070009708404541\n",
      "Step complete! 0.5149974822998047\n",
      "Step complete! 0.740001916885376\n",
      "Step complete! 0.49700260162353516\n",
      "Step complete! 0.7459855079650879\n",
      "Step complete! 0.7535247802734375\n",
      "Step complete! 0.5389981269836426\n",
      "Step complete! 0.5310137271881104\n",
      "Step complete! 0.6909976005554199\n",
      "Step complete! 0.7279999256134033\n",
      "Step complete! 0.47300004959106445\n",
      "Step complete! 0.7120006084442139\n",
      "Step complete! 0.5029973983764648\n",
      "Step complete! 0.5225052833557129\n",
      "Step complete! 0.5129964351654053\n",
      "Step complete! 0.7029991149902344\n",
      "Step complete! 0.6599981784820557\n",
      "Step complete! 0.7150027751922607\n",
      "Step complete! 0.4790034294128418\n",
      "Step complete! 0.6965231895446777\n",
      "Step complete! 0.7469973564147949\n",
      "Step complete! 0.725372314453125\n",
      "Step complete! 0.7620019912719727\n",
      "Step complete! 0.506004810333252\n",
      "Step complete! 0.7589282989501953\n",
      "Step complete! 0.4759976863861084\n",
      "Step complete! 0.49500465393066406\n",
      "Step complete! 0.6989996433258057\n",
      "Step complete! 0.6899337768554688\n",
      "Step complete! 0.6919972896575928\n",
      "Step complete! 0.7400002479553223\n",
      "Step complete! 0.5215158462524414\n",
      "Step complete! 0.733001708984375\n",
      "Step complete! 0.5389988422393799\n",
      "Step complete! 0.560004472732544\n",
      "Step complete! 0.491992712020874\n",
      "Step complete! 0.75\n",
      "Step complete! 0.5050048828125\n",
      "Step complete! 0.6099984645843506\n",
      "Step complete! 0.6820449829101562\n",
      "Step complete! 0.5060017108917236\n",
      "Step complete! 0.501997709274292\n",
      "Step complete! 0.6960065364837646\n",
      "Step complete! 0.7287178039550781\n",
      "Step complete! 0.6799986362457275\n",
      "Step complete! 0.4930105209350586\n",
      "Step complete! 0.7159919738769531\n",
      "Step complete! 0.7269992828369141\n",
      "Step complete! 0.7170026302337646\n",
      "Step complete! 0.7400033473968506\n",
      "Step complete! 0.5469965934753418\n",
      "Step complete! 0.5300014019012451\n",
      "Step complete! 0.48600292205810547\n",
      "Step complete! 0.6629962921142578\n",
      "Step complete! 0.5220205783843994\n",
      "Step complete! 0.6970057487487793\n",
      "Step complete! 0.6811845302581787\n",
      "Step complete! 0.4940190315246582\n",
      "Step complete! 0.5089991092681885\n",
      "Step complete! 0.4909994602203369\n",
      "Step complete! 0.6820023059844971\n",
      "Step complete! 0.5259995460510254\n",
      "Step complete! 0.740999698638916\n",
      "Step complete! 0.7620000839233398\n",
      "Step complete! 0.5170025825500488\n",
      "Step complete! 0.7360012531280518\n",
      "Step complete! 0.5409986972808838\n",
      "Step complete! 0.6882929801940918\n",
      "Step complete! 0.49199938774108887\n",
      "Step complete! 0.4749948978424072\n",
      "Step complete! 0.7299942970275879\n",
      "Step complete! 0.7049641609191895\n",
      "Step complete! 0.4765286445617676\n",
      "Step complete! 0.48099851608276367\n",
      "Step complete! 0.4799997806549072\n",
      "Step complete! 0.7021956443786621\n",
      "Step complete! 0.7060019969940186\n",
      "Step complete! 0.6699960231781006\n",
      "Step complete! 0.7470006942749023\n",
      "Step complete! 0.49499964714050293\n",
      "Step complete! 0.544992208480835\n",
      "Step complete! 0.4929990768432617\n",
      "Step complete! 0.4869954586029053\n",
      "Step complete! 0.5010027885437012\n",
      "Step complete! 0.7305209636688232\n",
      "Step complete! 0.5149967670440674\n",
      "Step complete! 0.46105337142944336\n",
      "Step complete! 0.5060153007507324\n",
      "Step complete! 0.6539995670318604\n",
      "Step complete! 0.4738142490386963\n",
      "Step complete! 0.699986457824707\n",
      "Step complete! 0.504997730255127\n",
      "Step complete! 0.5070033073425293\n",
      "Step complete! 0.514998197555542\n",
      "Step complete! 0.7110021114349365\n",
      "Step complete! 0.5170016288757324\n",
      "Step complete! 0.5349986553192139\n",
      "Step complete! 0.695864200592041\n",
      "Step complete! 0.5230021476745605\n",
      "Step complete! 0.47800564765930176\n",
      "Step complete! 0.6859993934631348\n",
      "Step complete! 0.6979973316192627\n",
      "Step complete! 0.4979977607727051\n",
      "Step complete! 0.487001895904541\n",
      "Step complete! 0.7020018100738525\n",
      "Step complete! 0.5045187473297119\n",
      "Step complete! 0.7180085182189941\n",
      "Step complete! 0.736992359161377\n",
      "Step complete! 0.71199631690979\n",
      "Step complete! 0.7251162528991699\n",
      "Step complete! 0.7000019550323486\n",
      "Step complete! 0.4740023612976074\n",
      "Step complete! 0.49199843406677246\n",
      "Step complete! 0.4605140686035156\n",
      "Step complete! 0.7216217517852783\n",
      "Step complete! 0.5039987564086914\n",
      "Step complete! 0.7060015201568604\n",
      "Step complete! 0.4740171432495117\n",
      "Step complete! 0.6900022029876709\n",
      "Step complete! 0.4819941520690918\n",
      "Step complete! 0.5369806289672852\n",
      "Step complete! 0.7450008392333984\n",
      "Step complete! 0.7209973335266113\n",
      "Step complete! 0.50099778175354\n",
      "Step complete! 0.49500226974487305\n",
      "Step complete! 0.48801445960998535\n",
      "Step complete! 0.6929833889007568\n",
      "Step complete! 0.5000028610229492\n",
      "Step complete! 0.4669969081878662\n",
      "Step complete! 0.5\n",
      "Step complete! 0.4609982967376709\n",
      "Step complete! 0.4665336608886719\n",
      "Step complete! 0.454282283782959\n",
      "Step complete! 0.4720027446746826\n",
      "Step complete! 0.6899974346160889\n",
      "Step complete! 0.4976053237915039\n",
      "Step complete! 0.4980134963989258\n",
      "Step complete! 0.5359950065612793\n",
      "Step complete! 0.4759979248046875\n",
      "Step complete! 0.523003339767456\n",
      "Step complete! 0.486997127532959\n",
      "Step complete! 0.714998722076416\n",
      "Step complete! 0.6899971961975098\n",
      "Step complete! 0.47301673889160156\n",
      "Step complete! 0.46599912643432617\n",
      "Step complete! 0.45700669288635254\n",
      "Step complete! 0.6819977760314941\n",
      "Step complete! 0.6537020206451416\n",
      "Step complete! 0.4750022888183594\n",
      "Step complete! 0.46400022506713867\n",
      "Step complete! 0.49199581146240234\n",
      "Step complete! 0.6990048885345459\n",
      "Step complete! 0.6720030307769775\n",
      "Step complete! 0.73199462890625\n",
      "Step complete! 0.529998779296875\n",
      "Step complete! 0.4900014400482178\n",
      "Step complete! 0.5139985084533691\n",
      "Step complete! 0.5429990291595459\n",
      "Step complete! 0.6959984302520752\n",
      "Step complete! 0.692002534866333\n",
      "Step complete! 0.6979990005493164\n",
      "Step complete! 0.6890003681182861\n",
      "Step complete! 0.6619958877563477\n",
      "Step complete! 0.6570150852203369\n",
      "Step complete! 0.48301267623901367\n",
      "Step complete! 0.47298383712768555\n",
      "Step complete! 0.7095310688018799\n",
      "Step complete! 0.5010058879852295\n",
      "Step complete! 0.7550003528594971\n",
      "Step complete! 0.7000012397766113\n",
      "Step complete! 0.5049998760223389\n",
      "Step complete! 0.6939983367919922\n",
      "Step complete! 0.48399806022644043\n",
      "Step complete! 0.6688272953033447\n",
      "Step complete! 0.46300363540649414\n",
      "Step complete! 0.6719996929168701\n",
      "Step complete! 0.6619963645935059\n",
      "Step complete! 0.4920048713684082\n",
      "Step complete! 0.46693921089172363\n",
      "Step complete! 0.6830019950866699\n",
      "Step complete! 0.5199964046478271\n",
      "Step complete! 0.5729942321777344\n",
      "Step complete! 0.6369979381561279\n",
      "Step complete! 0.7910041809082031\n",
      "Step complete! 0.5229949951171875\n",
      "Step complete! 0.7690021991729736\n",
      "Step complete! 0.4695169925689697\n",
      "Step complete! 0.48200464248657227\n",
      "Step complete! 0.48099589347839355\n",
      "Step complete! 0.7000129222869873\n",
      "Step complete! 0.7069985866546631\n",
      "Step complete! 0.6979970932006836\n",
      "Step complete! 0.5820014476776123\n",
      "Step complete! 0.5259995460510254\n",
      "Step complete! 0.48752403259277344\n",
      "Step complete! 0.7240030765533447\n",
      "Step complete! 0.5349957942962646\n",
      "Step complete! 0.7389895915985107\n",
      "Step complete! 0.522005558013916\n",
      "Step complete! 0.5250034332275391\n",
      "Step complete! 0.724992036819458\n",
      "Step complete! 0.4959993362426758\n",
      "Step complete! 0.7075116634368896\n",
      "Step complete! 0.47500133514404297\n",
      "Step complete! 0.7212018966674805\n",
      "Step complete! 0.7129964828491211\n",
      "Step complete! 0.6900060176849365\n",
      "Step complete! 0.6840014457702637\n",
      "Step complete! 0.4979891777038574\n",
      "Step complete! 0.516000509262085\n",
      "Step complete! 0.5479996204376221\n",
      "Step complete! 0.7380006313323975\n",
      "Step complete! 0.5170001983642578\n",
      "Step complete! 0.5130009651184082\n",
      "Step complete! 0.5550003051757812\n",
      "Step complete! 0.5020010471343994\n",
      "Step complete! 0.70499587059021\n",
      "Step complete! 0.5650036334991455\n",
      "Step complete! 0.4960000514984131\n",
      "Step complete! 0.543999433517456\n",
      "Step complete! 0.7220003604888916\n",
      "Step complete! 0.5439996719360352\n",
      "Step complete! 0.47099828720092773\n",
      "Step complete! 0.7239973545074463\n",
      "Step complete! 0.7289979457855225\n",
      "Step complete! 0.7339918613433838\n",
      "Step complete! 0.5295295715332031\n",
      "Step complete! 0.7269937992095947\n",
      "Step complete! 0.5430006980895996\n",
      "Step complete! 0.6910011768341064\n",
      "Step complete! 0.4980003833770752\n",
      "Step complete! 0.6799983978271484\n",
      "Step complete! 0.4849982261657715\n",
      "Step complete! 0.48387646675109863\n",
      "Step complete! 0.4869990348815918\n",
      "Step complete! 0.498424768447876\n",
      "Step complete! 0.7219970226287842\n",
      "Step complete! 0.4810051918029785\n",
      "Step complete! 0.7479944229125977\n",
      "Step complete! 0.5109999179840088\n",
      "Step complete! 0.7049996852874756\n",
      "Step complete! 0.5425143241882324\n",
      "Step complete! 0.7009992599487305\n",
      "Step complete! 0.5070009231567383\n",
      "Step complete! 0.48399877548217773\n",
      "Step complete! 0.5000011920928955\n",
      "Step complete! 1.0179986953735352\n",
      "Step complete! 0.5020031929016113\n",
      "Step complete! 0.698998212814331\n",
      "Step complete! 0.4695088863372803\n",
      "Step complete! 0.6950030326843262\n",
      "Step complete! 0.6972711086273193\n",
      "Step complete! 0.7190003395080566\n",
      "Step complete! 0.5719995498657227\n",
      "Step complete! 0.750999927520752\n",
      "Step complete! 0.542999267578125\n",
      "Step complete! 0.7160336971282959\n",
      "Step complete! 0.5180070400238037\n",
      "Step complete! 0.49899983406066895\n",
      "Step complete! 0.4809997081756592\n",
      "Step complete! 0.6900022029876709\n",
      "Step complete! 0.7035479545593262\n",
      "Step complete! 0.6919023990631104\n",
      "Step complete! 0.6967709064483643\n",
      "Step complete! 0.6780023574829102\n",
      "Step complete! 0.7096173763275146\n",
      "Step complete! 0.5429952144622803\n",
      "Step complete! 0.562000036239624\n",
      "Step complete! 0.5239982604980469\n",
      "Step complete! 0.5330042839050293\n",
      "Step complete! 0.7359933853149414\n",
      "Step complete! 0.7270004749298096\n",
      "Step complete! 0.5050003528594971\n",
      "Step complete! 0.6930444240570068\n",
      "Step complete! 0.6770007610321045\n",
      "Step complete! 0.5066158771514893\n",
      "Step complete! 0.47200798988342285\n",
      "Step complete! 0.46799421310424805\n",
      "Step complete! 0.5100023746490479\n",
      "Step complete! 0.4799976348876953\n",
      "Step complete! 0.5140008926391602\n",
      "Step complete! 0.5509982109069824\n",
      "Step complete! 0.7779989242553711\n",
      "Step complete! 0.720003604888916\n",
      "Step complete! 0.5029971599578857\n",
      "Step complete! 0.5169990062713623\n",
      "Step complete! 0.7430140972137451\n",
      "Step complete! 0.49698305130004883\n",
      "Step complete! 0.5440034866333008\n",
      "Step complete! 0.4949991703033447\n",
      "Step complete! 0.7140088081359863\n",
      "Step complete! 0.6809952259063721\n",
      "Step complete! 0.48399996757507324\n",
      "Step complete! 0.6796565055847168\n",
      "Step complete! 0.6819989681243896\n",
      "Step complete! 0.7615523338317871\n",
      "Step complete! 0.5260031223297119\n",
      "Step complete! 0.5599987506866455\n",
      "Step complete! 0.5270001888275146\n",
      "Step complete! 0.7239952087402344\n",
      "Step complete! 0.5090014934539795\n",
      "Step complete! 0.48999929428100586\n",
      "Step complete! 0.6640081405639648\n",
      "Step complete! 0.5070006847381592\n",
      "Step complete! 0.6950023174285889\n",
      "Step complete! 0.705998420715332\n",
      "Step complete! 0.6939964294433594\n",
      "Step complete! 0.6820015907287598\n",
      "Step complete! 0.48600220680236816\n",
      "Step complete! 0.729996919631958\n",
      "Step complete! 0.9420034885406494\n",
      "Step complete! 0.5309994220733643\n",
      "Step complete! 0.5379979610443115\n",
      "Step complete! 0.5449912548065186\n",
      "Step complete! 0.7380027770996094\n",
      "Step complete! 0.47456979751586914\n",
      "Step complete! 0.4949986934661865\n",
      "Step complete! 0.46700191497802734\n",
      "Step complete! 0.6939992904663086\n",
      "Step complete! 0.7700040340423584\n",
      "Step complete! 0.7309958934783936\n",
      "Step complete! 0.5509991645812988\n",
      "Step complete! 0.46500515937805176\n",
      "Step complete! 0.4969942569732666\n",
      "Step complete! 0.49700188636779785\n",
      "Step complete! 0.7155225276947021\n",
      "Step complete! 0.5040011405944824\n",
      "Step complete! 0.5319993495941162\n",
      "Step complete! 0.5240013599395752\n",
      "Step complete! 0.542517900466919\n",
      "Step complete! 0.5159978866577148\n",
      "Step complete! 0.7370097637176514\n",
      "Step complete! 0.5150022506713867\n",
      "Step complete! 0.6919958591461182\n",
      "Step complete! 0.4770174026489258\n",
      "Step complete! 0.7247540950775146\n",
      "Step complete! 0.44499778747558594\n",
      "Step complete! 0.7310035228729248\n",
      "Step complete! 0.7319991588592529\n",
      "Step complete! 0.7139978408813477\n",
      "Step complete! 0.7420022487640381\n",
      "Step complete! 0.4989948272705078\n",
      "Step complete! 0.5090017318725586\n",
      "Step complete! 0.7769985198974609\n",
      "Step complete! 0.671004056930542\n",
      "Step complete! 0.47699689865112305\n",
      "Step complete! 0.7060017585754395\n",
      "Step complete! 0.4609980583190918\n",
      "Step complete! 0.6753275394439697\n",
      "Step complete! 0.47156262397766113\n",
      "Step complete! 0.6749973297119141\n",
      "Step complete! 0.6850011348724365\n",
      "Step complete! 0.4869997501373291\n",
      "Step complete! 0.8077857494354248\n",
      "Step complete! 0.6819994449615479\n",
      "Step complete! 0.5209989547729492\n",
      "Step complete! 0.5110030174255371\n",
      "Step complete! 0.7179956436157227\n",
      "Step complete! 0.49600911140441895\n",
      "Step complete! 0.4870014190673828\n",
      "Step complete! 0.6840012073516846\n",
      "Step complete! 0.668999195098877\n",
      "Step complete! 0.4660015106201172\n",
      "Step complete! 0.6969976425170898\n",
      "Step complete! 0.6739978790283203\n",
      "Step complete! 0.49199676513671875\n",
      "Step complete! 0.4909954071044922\n",
      "Step complete! 0.7209892272949219\n",
      "Step complete! 0.527000904083252\n",
      "Step complete! 0.5280001163482666\n",
      "Step complete! 0.5049996376037598\n",
      "Step complete! 0.7429995536804199\n",
      "Step complete! 0.5439980030059814\n",
      "Step complete! 0.557985782623291\n",
      "Step complete! 0.4719996452331543\n",
      "Step complete! 0.4910011291503906\n",
      "Step complete! 0.47299909591674805\n",
      "Step complete! 0.4689958095550537\n",
      "Step complete! 0.46500158309936523\n",
      "Step complete! 0.49352264404296875\n",
      "Step complete! 0.4590001106262207\n",
      "Step complete! 0.47896528244018555\n",
      "Step complete! 0.6909980773925781\n",
      "Step complete! 0.6950037479400635\n",
      "Step complete! 0.525996208190918\n",
      "Step complete! 0.513991117477417\n",
      "Step complete! 0.5330007076263428\n",
      "Step complete! 0.7081902027130127\n",
      "Step complete! 0.5310041904449463\n",
      "Step complete! 0.7800009250640869\n",
      "Step complete! 0.49753355979919434\n",
      "Step complete! 0.4849979877471924\n",
      "Step complete! 0.6720044612884521\n",
      "Step complete! 0.7099967002868652\n",
      "Step complete! 0.7170119285583496\n",
      "Step complete! 0.4980003833770752\n",
      "Step complete! 0.5040011405944824\n",
      "Step complete! 0.46799731254577637\n",
      "Step complete! 0.6905257701873779\n",
      "Step complete! 0.7419915199279785\n",
      "Step complete! 0.7030003070831299\n",
      "Step complete! 0.7100009918212891\n",
      "Step complete! 0.4980123043060303\n",
      "Step complete! 0.5340015888214111\n",
      "Step complete! 0.4729957580566406\n",
      "Step complete! 0.4850001335144043\n",
      "Step complete! 0.7775213718414307\n",
      "Step complete! 0.49900364875793457\n",
      "Step complete! 0.6950013637542725\n",
      "Step complete! 0.4779994487762451\n",
      "Step complete! 0.464998722076416\n",
      "Step complete! 0.47299814224243164\n",
      "Step complete! 0.4940001964569092\n",
      "Step complete! 0.7113194465637207\n",
      "Step complete! 0.5199985504150391\n",
      "Step complete! 0.7399933338165283\n",
      "Step complete! 0.7329974174499512\n",
      "Step complete! 0.7370085716247559\n",
      "Step complete! 0.725989580154419\n",
      "Step complete! 0.4629988670349121\n",
      "Step complete! 0.7230031490325928\n",
      "Step complete! 0.547999382019043\n",
      "Step complete! 0.5230011940002441\n",
      "Step complete! 0.5480010509490967\n",
      "Step complete! 0.7094402313232422\n",
      "Step complete! 0.6750020980834961\n",
      "Step complete! 0.4699993133544922\n",
      "Step complete! 0.7130024433135986\n",
      "Step complete! 0.7169995307922363\n",
      "Step complete! 0.5119967460632324\n",
      "Step complete! 0.5069987773895264\n",
      "Step complete! 0.5000007152557373\n",
      "Step complete! 0.7340006828308105\n",
      "Step complete! 0.6597104072570801\n",
      "Step complete! 0.4739995002746582\n",
      "Step complete! 0.6810023784637451\n",
      "Step complete! 0.4759964942932129\n",
      "Step complete! 0.701998233795166\n",
      "Step complete! 0.4810018539428711\n",
      "Step complete! 0.6989970207214355\n",
      "Step complete! 0.49855780601501465\n",
      "Step complete! 0.7149982452392578\n",
      "Step complete! 0.5199999809265137\n",
      "Step complete! 0.7470009326934814\n",
      "Step complete! 0.689000129699707\n",
      "Step complete! 0.7009992599487305\n",
      "Step complete! 0.5140013694763184\n",
      "Step complete! 0.4629964828491211\n",
      "Step complete! 0.4870021343231201\n",
      "Step complete! 0.5139999389648438\n",
      "Step complete! 0.702009916305542\n",
      "Step complete! 0.6860001087188721\n",
      "Step complete! 0.49899888038635254\n",
      "Step complete! 0.46860694885253906\n",
      "Step complete! 0.6889996528625488\n",
      "Step complete! 0.6789999008178711\n",
      "Step complete! 0.5210011005401611\n",
      "Step complete! 0.7169983386993408\n",
      "Step complete! 0.7029988765716553\n",
      "Step complete! 0.7249987125396729\n",
      "Step complete! 0.7200040817260742\n",
      "Step complete! 0.46199893951416016\n",
      "Step complete! 0.4799973964691162\n",
      "Step complete! 0.48600292205810547\n",
      "Step complete! 0.6839988231658936\n",
      "Step complete! 0.4819953441619873\n",
      "Step complete! 0.5040016174316406\n",
      "Step complete! 0.6829979419708252\n",
      "Step complete! 0.4660019874572754\n",
      "Step complete! 0.49399685859680176\n",
      "Step complete! 0.4805159568786621\n",
      "Step complete! 0.5090012550354004\n",
      "Step complete! 0.5130133628845215\n",
      "Step complete! 0.5039887428283691\n",
      "Step complete! 0.5159964561462402\n",
      "Step complete! 0.7540011405944824\n",
      "Step complete! 0.5540018081665039\n",
      "Step complete! 0.6860039234161377\n",
      "Step complete! 0.7135171890258789\n",
      "Step complete! 0.7299976348876953\n",
      "Step complete! 0.7030003070831299\n",
      "Step complete! 0.4970107078552246\n",
      "Step complete! 0.6740014553070068\n",
      "Step complete! 0.49500370025634766\n",
      "Step complete! 0.5259959697723389\n",
      "Step complete! 0.7140004634857178\n",
      "Step complete! 0.4955260753631592\n",
      "Step complete! 0.540999174118042\n",
      "Step complete! 0.7100152969360352\n",
      "Step complete! 0.5090055465698242\n",
      "Step complete! 0.6800000667572021\n",
      "Step complete! 0.7389991283416748\n",
      "Step complete! 0.6860008239746094\n",
      "Step complete! 0.6790249347686768\n",
      "Step complete! 0.6835203170776367\n",
      "Step complete! 0.6775424480438232\n",
      "Step complete! 0.48000121116638184\n",
      "Step complete! 0.6999967098236084\n",
      "Step complete! 0.46970176696777344\n",
      "Step complete! 0.719001293182373\n",
      "Step complete! 0.7060098648071289\n",
      "Step complete! 0.5559995174407959\n",
      "Step complete! 0.5270037651062012\n",
      "Step complete! 0.7179989814758301\n",
      "Step complete! 0.7439942359924316\n",
      "Step complete! 0.477999210357666\n",
      "Step complete! 0.495999813079834\n",
      "Step complete! 0.5219488143920898\n",
      "Step complete! 0.6929957866668701\n",
      "Step complete! 0.498004674911499\n",
      "Step complete! 0.4689962863922119\n",
      "Step complete! 0.6820015907287598\n",
      "Step complete! 0.46999311447143555\n",
      "Step complete! 0.6667146682739258\n",
      "Step complete! 0.521000862121582\n",
      "Step complete! 0.5030007362365723\n",
      "Step complete! 0.5270006656646729\n",
      "Step complete! 0.5159971714019775\n",
      "Step complete! 0.7280023097991943\n",
      "Step complete! 0.7029974460601807\n",
      "Step complete! 0.7844605445861816\n",
      "Step complete! 0.47099995613098145\n",
      "Step complete! 0.5030016899108887\n",
      "Step complete! 0.4779961109161377\n",
      "Step complete! 0.7066335678100586\n",
      "Step complete! 0.6899690628051758\n",
      "Step complete! 0.7070009708404541\n",
      "Step complete! 0.6900105476379395\n",
      "Step complete! 0.5260009765625\n",
      "Step complete! 0.492999792098999\n",
      "Step complete! 0.516000509262085\n",
      "Step complete! 0.7260050773620605\n",
      "Step complete! 0.75\n",
      "Step complete! 0.5410034656524658\n",
      "Step complete! 0.4829986095428467\n",
      "Step complete! 0.6895749568939209\n",
      "Step complete! 0.6910011768341064\n",
      "Step complete! 0.5059986114501953\n",
      "Step complete! 0.5319986343383789\n",
      "Step complete! 0.4880030155181885\n",
      "Step complete! 0.715996503829956\n",
      "Step complete! 0.48200321197509766\n",
      "Step complete! 0.7059988975524902\n",
      "Step complete! 0.5139944553375244\n",
      "Step complete! 0.5590000152587891\n",
      "Step complete! 0.4960005283355713\n",
      "Step complete! 0.7559988498687744\n",
      "Step complete! 0.8069977760314941\n",
      "Step complete! 0.575000524520874\n",
      "Step complete! 0.5059986114501953\n",
      "Step complete! 0.7049965858459473\n",
      "Step complete! 0.7110183238983154\n",
      "Step complete! 0.5190019607543945\n",
      "Step complete! 0.48999857902526855\n",
      "Step complete! 0.4980039596557617\n",
      "Step complete! 0.5079927444458008\n",
      "Step complete! 0.681999683380127\n",
      "Step complete! 0.4739999771118164\n",
      "Step complete! 0.5545220375061035\n",
      "Step complete! 0.5660016536712646\n",
      "Step complete! 0.5150051116943359\n",
      "Step complete! 0.7410054206848145\n",
      "Step complete! 0.7560021877288818\n",
      "Step complete! 0.5480010509490967\n",
      "Step complete! 0.7285144329071045\n",
      "Step complete! 0.7140023708343506\n",
      "Step complete! 0.4940011501312256\n",
      "Step complete! 0.6980001926422119\n",
      "Step complete! 0.49399614334106445\n",
      "Step complete! 0.6969954967498779\n",
      "Step complete! 0.7050032615661621\n",
      "Step complete! 0.720146656036377\n",
      "Step complete! 0.6070013046264648\n",
      "Step complete! 0.7319967746734619\n",
      "Step complete! 0.7695116996765137\n",
      "Step complete! 0.7489993572235107\n",
      "Step complete! 0.7370028495788574\n",
      "Step complete! 0.5129983425140381\n",
      "Step complete! 0.6930027008056641\n",
      "Step complete! 0.509998083114624\n",
      "Step complete! 0.5030117034912109\n",
      "Step complete! 0.7035162448883057\n",
      "Step complete! 0.4889979362487793\n",
      "Step complete! 0.686997652053833\n",
      "Step complete! 0.5029993057250977\n",
      "Step complete! 0.7239973545074463\n",
      "Step complete! 0.5230038166046143\n",
      "Step complete! 0.5339994430541992\n",
      "Step complete! 0.5370128154754639\n",
      "Step complete! 0.7705211639404297\n",
      "Step complete! 0.7309987545013428\n",
      "Step complete! 0.5350160598754883\n",
      "Step complete! 0.7259969711303711\n",
      "Step complete! 0.4699995517730713\n",
      "Step complete! 0.49900388717651367\n",
      "Step complete! 0.5030021667480469\n",
      "Step complete! 0.4909992218017578\n",
      "Step complete! 0.6695218086242676\n",
      "Step complete! 0.527003288269043\n",
      "Step complete! 0.4999990463256836\n",
      "Step complete! 0.49443984031677246\n",
      "Step complete! 0.5279967784881592\n",
      "Step complete! 0.5460009574890137\n",
      "Step complete! 0.5190026760101318\n",
      "Step complete! 0.7595601081848145\n",
      "Step complete! 0.5269978046417236\n",
      "Step complete! 0.7410037517547607\n",
      "Step complete! 0.7439994812011719\n",
      "Step complete! 0.7460005283355713\n",
      "Step complete! 0.6740002632141113\n",
      "Step complete! 0.5009996891021729\n",
      "Step complete! 0.7485995292663574\n",
      "Step complete! 0.6979913711547852\n",
      "Step complete! 0.5090041160583496\n",
      "Step complete! 0.7139942646026611\n",
      "Step complete! 0.7354416847229004\n",
      "Step complete! 0.5430047512054443\n",
      "Step complete! 0.5459907054901123\n",
      "Step complete! 0.7689948081970215\n",
      "Step complete! 0.786003828048706\n",
      "Step complete! 0.7169995307922363\n",
      "Step complete! 0.7070004940032959\n",
      "Step complete! 0.4849405288696289\n",
      "Step complete! 0.6820030212402344\n",
      "Step complete! 0.7080004215240479\n",
      "Step complete! 0.4790186882019043\n",
      "Step complete! 0.5060024261474609\n",
      "Step complete! 0.6954936981201172\n",
      "Step complete! 0.6820042133331299\n",
      "Step complete! 0.7349996566772461\n",
      "Step complete! 0.7129955291748047\n",
      "Step complete! 0.5370028018951416\n",
      "Step complete! 0.5299975872039795\n",
      "Step complete! 0.5280013084411621\n",
      "Step complete! 0.7329971790313721\n",
      "Step complete! 0.699984073638916\n",
      "Step complete! 0.6869978904724121\n",
      "Step complete! 0.7050879001617432\n",
      "Step complete! 0.4710052013397217\n",
      "Step complete! 0.4749937057495117\n",
      "Step complete! 0.7019920349121094\n",
      "Step complete! 0.5110001564025879\n",
      "Step complete! 0.7050185203552246\n",
      "Step complete! 0.5285191535949707\n",
      "Step complete! 0.5079941749572754\n",
      "Step complete! 0.5220062732696533\n",
      "Step complete! 0.5759928226470947\n",
      "Step complete! 0.5439987182617188\n",
      "Step complete! 0.5280086994171143\n",
      "Step complete! 0.7310028076171875\n",
      "Step complete! 0.5280084609985352\n",
      "Step complete! 0.5139825344085693\n",
      "Step complete! 0.6845190525054932\n",
      "Step complete! 0.6857395172119141\n",
      "Step complete! 0.6809966564178467\n",
      "Step complete! 0.6800007820129395\n",
      "Step complete! 0.4890007972717285\n",
      "Step complete! 0.5260674953460693\n",
      "Step complete! 0.7240021228790283\n",
      "Step complete! 0.534003496170044\n",
      "Step complete! 0.7219963073730469\n",
      "Step complete! 0.5459990501403809\n",
      "Step complete! 0.7190048694610596\n",
      "Step complete! 0.7190093994140625\n",
      "Step complete! 0.5000019073486328\n",
      "Step complete! 0.7090003490447998\n",
      "Step complete! 0.683997392654419\n",
      "Step complete! 0.4864485263824463\n",
      "Step complete! 0.7089884281158447\n",
      "Step complete! 0.6859993934631348\n",
      "Step complete! 0.49500131607055664\n",
      "Step complete! 0.7299983501434326\n",
      "Step complete! 0.5790023803710938\n",
      "Step complete! 0.7809929847717285\n",
      "Step complete! 0.7289993762969971\n",
      "Step complete! 0.6959996223449707\n",
      "Step complete! 0.4999837875366211\n",
      "Step complete! 0.7145378589630127\n",
      "Step complete! 0.4699971675872803\n",
      "Step complete! 0.6630122661590576\n",
      "Step complete! 0.5229887962341309\n",
      "Step complete! 0.5099992752075195\n",
      "Step complete! 0.4610140323638916\n",
      "Step complete! 0.6659979820251465\n",
      "Step complete! 0.6730012893676758\n",
      "Step complete! 0.7019832134246826\n",
      "Step complete! 0.7275218963623047\n",
      "Step complete! 0.5309948921203613\n",
      "Step complete! 0.7100009918212891\n",
      "Step complete! 0.5049991607666016\n",
      "Step complete! 0.5270006656646729\n",
      "Step complete! 0.7040019035339355\n",
      "Step complete! 0.7110011577606201\n",
      "Step complete! 0.4805150032043457\n",
      "Step complete! 0.4850020408630371\n",
      "Step complete! 0.6820023059844971\n",
      "Step complete! 0.7059977054595947\n",
      "Step complete! 0.483414888381958\n",
      "Step complete! 0.47500181198120117\n",
      "Step complete! 0.4889860153198242\n",
      "Step complete! 0.6870002746582031\n",
      "Step complete! 0.7050163745880127\n",
      "Step complete! 0.550011396408081\n",
      "Step complete! 0.5600016117095947\n",
      "Step complete! 0.7390003204345703\n",
      "Step complete! 0.7070002555847168\n",
      "Step complete! 0.7479984760284424\n",
      "Step complete! 0.4865126609802246\n",
      "Step complete! 0.6875002384185791\n",
      "Step complete! 0.5139210224151611\n",
      "Step complete! 0.4779994487762451\n",
      "Step complete! 0.7030062675476074\n",
      "Step complete! 0.6838529109954834\n",
      "Step complete! 0.5300002098083496\n",
      "Step complete! 0.6692047119140625\n",
      "Step complete! 0.5309970378875732\n",
      "Step complete! 0.7090024948120117\n",
      "Step complete! 0.5250029563903809\n",
      "Step complete! 0.7089955806732178\n",
      "Step complete! 0.5390002727508545\n",
      "Step complete! 0.7409954071044922\n",
      "Step complete! 0.6803083419799805\n",
      "Step complete! 0.5099353790283203\n",
      "Step complete! 0.5029995441436768\n",
      "Step complete! 0.6829988956451416\n",
      "Step complete! 0.7220077514648438\n",
      "Step complete! 0.5795137882232666\n",
      "Step complete! 0.8699967861175537\n",
      "Step complete! 0.47300267219543457\n",
      "Step complete! 0.5379960536956787\n",
      "Step complete! 0.7270019054412842\n",
      "Step complete! 0.7140016555786133\n",
      "Step complete! 0.7445409297943115\n",
      "Step complete! 0.5050003528594971\n",
      "Step complete! 0.5339987277984619\n",
      "Step complete! 0.7160000801086426\n",
      "Step complete! 0.4989922046661377\n",
      "Step complete! 0.4920005798339844\n",
      "Step complete! 0.681999683380127\n",
      "Step complete! 0.7350008487701416\n",
      "Step complete! 0.6659998893737793\n",
      "Step complete! 0.5105183124542236\n",
      "Step complete! 0.4959983825683594\n",
      "Step complete! 0.7260012626647949\n",
      "Step complete! 0.5400011539459229\n",
      "Step complete! 0.7370030879974365\n",
      "Step complete! 0.544994592666626\n",
      "Step complete! 0.741999626159668\n",
      "Step complete! 0.5075147151947021\n",
      "Step complete! 0.49799633026123047\n",
      "Step complete! 0.6989238262176514\n",
      "Step complete! 0.6770040988922119\n",
      "Step complete! 0.4940018653869629\n",
      "Step complete! 0.5020020008087158\n",
      "Step complete! 0.5020005702972412\n",
      "Step complete! 0.6957266330718994\n",
      "Step complete! 0.7260012626647949\n",
      "Step complete! 0.49000120162963867\n",
      "Step complete! 0.4880092144012451\n",
      "Step complete! 0.7649908065795898\n",
      "Step complete! 0.7450015544891357\n",
      "Step complete! 0.5029985904693604\n",
      "Step complete! 0.7730093002319336\n",
      "Step complete! 0.7319893836975098\n",
      "Step complete! 0.7140011787414551\n",
      "Step complete! 0.6995177268981934\n",
      "Step complete! 0.6839981079101562\n",
      "Step complete! 0.674001932144165\n",
      "Step complete! 0.7249999046325684\n",
      "Step complete! 0.6860013008117676\n",
      "Step complete! 0.4859962463378906\n",
      "Step complete! 0.5029945373535156\n",
      "Step complete! 0.5439999103546143\n",
      "Step complete! 0.5295164585113525\n",
      "Step complete! 0.7370016574859619\n",
      "Step complete! 0.5360071659088135\n",
      "Step complete! 0.5599939823150635\n",
      "Step complete! 0.7339985370635986\n",
      "Step complete! 0.6389999389648438\n",
      "Step complete! 0.71700119972229\n",
      "Step complete! 0.48999881744384766\n",
      "Step complete! 0.5285170078277588\n",
      "Step complete! 0.49400973320007324\n",
      "Step complete! 0.48799967765808105\n",
      "Step complete! 0.5039989948272705\n",
      "Step complete! 0.5019989013671875\n",
      "Step complete! 0.48000383377075195\n",
      "Step complete! 0.7169985771179199\n",
      "Step complete! 0.7300050258636475\n",
      "Step complete! 0.555006742477417\n",
      "Step complete! 0.7395055294036865\n",
      "Step complete! 0.7390024662017822\n",
      "Step complete! 0.6929981708526611\n",
      "Step complete! 0.6000001430511475\n",
      "Step complete! 0.5060019493103027\n",
      "Step complete! 0.5290048122406006\n",
      "Step complete! 0.5149972438812256\n",
      "Step complete! 0.7065207958221436\n",
      "Step complete! 0.6887643337249756\n",
      "Step complete! 0.4989969730377197\n",
      "Step complete! 0.7009987831115723\n",
      "Step complete! 0.5660006999969482\n",
      "Step complete! 0.5350031852722168\n",
      "Step complete! 0.7440025806427002\n",
      "Step complete! 0.7195732593536377\n",
      "Step complete! 0.7759864330291748\n",
      "Step complete! 0.7309985160827637\n",
      "Step complete! 0.5030050277709961\n",
      "Step complete! 0.7169957160949707\n",
      "Step complete! 0.6839978694915771\n",
      "Step complete! 0.706000566482544\n",
      "Step complete! 0.6970009803771973\n",
      "Step complete! 0.7016932964324951\n",
      "Step complete! 0.6930055618286133\n",
      "Step complete! 0.5379934310913086\n",
      "Step complete! 0.534998893737793\n",
      "Step complete! 0.5140013694763184\n",
      "Step complete! 0.5459895133972168\n",
      "Step complete! 0.7480018138885498\n",
      "Step complete! 0.5549993515014648\n",
      "Step complete! 0.48600196838378906\n",
      "Step complete! 0.7246382236480713\n",
      "Step complete! 0.5069983005523682\n",
      "Step complete! 0.6809999942779541\n",
      "Step complete! 0.49699831008911133\n",
      "Step complete! 0.6929976940155029\n",
      "Step complete! 0.48700737953186035\n",
      "Step complete! 0.495985746383667\n",
      "Step complete! 0.5150001049041748\n",
      "Step complete! 0.5890085697174072\n",
      "Step complete! 0.7569897174835205\n",
      "Step complete! 0.7449944019317627\n",
      "Step complete! 0.759002685546875\n",
      "Step complete! 0.5599994659423828\n",
      "Step complete! 0.5379998683929443\n",
      "Step complete! 0.7090070247650146\n",
      "Step complete! 0.7309925556182861\n",
      "Step complete! 0.7550160884857178\n",
      "Step complete! 0.5180015563964844\n",
      "Step complete! 0.5140008926391602\n",
      "Step complete! 0.7208678722381592\n",
      "Step complete! 0.7105128765106201\n",
      "Step complete! 0.595012903213501\n",
      "Step complete! 0.7959988117218018\n",
      "Step complete! 0.7099990844726562\n",
      "Step complete! 0.5460000038146973\n",
      "Step complete! 0.7470040321350098\n",
      "Step complete! 0.7299962043762207\n",
      "Step complete! 0.5395312309265137\n",
      "Step complete! 0.5269997119903564\n",
      "Step complete! 0.49999022483825684\n",
      "Step complete! 0.520000696182251\n",
      "Step complete! 0.711998701095581\n",
      "Step complete! 0.7620022296905518\n",
      "Step complete! 0.5175173282623291\n",
      "Step complete! 0.5010020732879639\n",
      "Step complete! 0.728205680847168\n",
      "Step complete! 0.5449981689453125\n",
      "Step complete! 0.5640013217926025\n",
      "Step complete! 0.5629997253417969\n",
      "Step complete! 0.7230007648468018\n",
      "Step complete! 0.7783584594726562\n",
      "Step complete! 0.5080068111419678\n",
      "Step complete! 0.5199954509735107\n",
      "Step complete! 0.512000322341919\n",
      "Step complete! 0.5119941234588623\n",
      "Step complete! 0.5520012378692627\n",
      "Step complete! 0.7169985771179199\n",
      "Step complete! 0.6979999542236328\n",
      "Step complete! 0.542001485824585\n",
      "Step complete! 0.5041370391845703\n",
      "Step complete! 0.5279977321624756\n",
      "Step complete! 0.7350027561187744\n",
      "Step complete! 0.7449936866760254\n",
      "Step complete! 0.7225115299224854\n",
      "Step complete! 0.5560173988342285\n",
      "Step complete! 0.7440028190612793\n",
      "Step complete! 0.7430157661437988\n",
      "Step complete! 0.7340013980865479\n",
      "Step complete! 0.707000732421875\n",
      "Step complete! 0.523007869720459\n",
      "Step complete! 0.5075240135192871\n",
      "Step complete! 0.688997745513916\n",
      "Step complete! 0.7350127696990967\n",
      "Step complete! 0.5319890975952148\n",
      "Step complete! 0.76300048828125\n",
      "Step complete! 0.7389981746673584\n",
      "Step complete! 0.5679874420166016\n",
      "Step complete! 0.5850017070770264\n",
      "Step complete! 0.8025236129760742\n",
      "Step complete! 0.6980035305023193\n",
      "Step complete! 0.704003095626831\n",
      "Step complete! 0.5280008316040039\n",
      "Step complete! 0.5049934387207031\n",
      "Step complete! 0.6840009689331055\n",
      "Step complete! 0.7162110805511475\n",
      "Step complete! 0.691990852355957\n",
      "Step complete! 0.514002799987793\n",
      "Step complete! 0.5359981060028076\n",
      "Step complete! 0.7535150051116943\n",
      "Step complete! 0.7400021553039551\n",
      "Step complete! 0.7890002727508545\n",
      "Step complete! 0.7339982986450195\n",
      "Step complete! 0.5120055675506592\n",
      "Step complete! 0.7109925746917725\n",
      "Step complete! 0.4940018653869629\n",
      "Step complete! 0.7121708393096924\n",
      "Step complete! 0.5049967765808105\n",
      "Step complete! 0.48400235176086426\n",
      "Step complete! 0.49499940872192383\n",
      "Step complete! 0.4777872562408447\n",
      "Step complete! 0.7263298034667969\n",
      "Step complete! 0.5480015277862549\n",
      "Step complete! 0.7529995441436768\n",
      "Step complete! 0.7499990463256836\n",
      "Step complete! 0.529998779296875\n",
      "Step complete! 0.5500071048736572\n",
      "Step complete! 0.7379915714263916\n",
      "Step complete! 0.5160014629364014\n",
      "Step complete! 0.7009985446929932\n",
      "Step complete! 0.48200368881225586\n",
      "Step complete! 0.6749982833862305\n",
      "Step complete! 0.4870011806488037\n",
      "Step complete! 0.6875190734863281\n",
      "Step complete! 0.7330029010772705\n",
      "Step complete! 0.47800564765930176\n",
      "Step complete! 0.5590023994445801\n",
      "Step complete! 0.7370016574859619\n",
      "Step complete! 0.7609982490539551\n",
      "Step complete! 0.7600007057189941\n",
      "Step complete! 0.5260000228881836\n",
      "Step complete! 0.739999532699585\n",
      "Step complete! 0.5085239410400391\n",
      "Step complete! 0.7165248394012451\n",
      "Step complete! 0.5339980125427246\n",
      "Step complete! 0.558995246887207\n",
      "Step complete! 0.5208797454833984\n",
      "Step complete! 0.717625617980957\n",
      "Step complete! 0.6790025234222412\n",
      "Step complete! 0.7250006198883057\n",
      "Step complete! 0.7429995536804199\n",
      "Step complete! 0.5039989948272705\n",
      "Step complete! 0.5419981479644775\n",
      "Step complete! 0.7119994163513184\n",
      "Step complete! 0.7389998435974121\n",
      "Step complete! 0.7350029945373535\n",
      "Step complete! 0.46900391578674316\n",
      "Step complete! 0.4719970226287842\n",
      "Step complete! 0.7170062065124512\n",
      "Step complete! 0.5309998989105225\n",
      "Step complete! 0.7650008201599121\n",
      "Step complete! 0.6959149837493896\n",
      "Step complete! 0.6799979209899902\n",
      "Step complete! 0.7220008373260498\n",
      "Step complete! 0.7199983596801758\n",
      "Step complete! 0.537531852722168\n",
      "Step complete! 0.7489981651306152\n",
      "Step complete! 0.521998405456543\n",
      "Step complete! 0.7369973659515381\n",
      "Step complete! 0.708000898361206\n",
      "Step complete! 0.6760036945343018\n",
      "Step complete! 0.4559957981109619\n",
      "Step complete! 0.4955165386199951\n",
      "Step complete! 0.6930005550384521\n",
      "Step complete! 0.49300074577331543\n",
      "Step complete! 0.5039975643157959\n",
      "Step complete! 0.7310175895690918\n",
      "Step complete! 0.7250111103057861\n",
      "Step complete! 0.512998104095459\n",
      "Step complete! 0.5220036506652832\n",
      "Step complete! 0.7125144004821777\n",
      "Step complete! 0.7400639057159424\n",
      "Step complete! 0.5369999408721924\n",
      "Step complete! 0.6719985008239746\n",
      "Step complete! 0.4720001220703125\n",
      "Step complete! 0.4890007972717285\n",
      "Step complete! 0.47099757194519043\n",
      "Step complete! 0.6713106632232666\n",
      "Step complete! 0.6760039329528809\n",
      "Step complete! 0.6909961700439453\n",
      "Step complete! 0.6920008659362793\n",
      "Step complete! 0.7139995098114014\n",
      "Step complete! 0.7910008430480957\n",
      "Step complete! 0.7205231189727783\n",
      "Step complete! 0.7009978294372559\n",
      "Step complete! 0.49100828170776367\n",
      "Step complete! 0.5000011920928955\n",
      "Step complete! 0.6964035034179688\n",
      "Step complete! 0.4699985980987549\n",
      "Step complete! 0.6956393718719482\n",
      "Step complete! 0.6989974975585938\n",
      "Step complete! 0.677001953125\n",
      "Step complete! 0.5319991111755371\n",
      "Step complete! 0.6910021305084229\n",
      "Step complete! 0.4999983310699463\n",
      "Step complete! 0.771003007888794\n",
      "Step complete! 0.5359885692596436\n",
      "Step complete! 0.7179980278015137\n",
      "Step complete! 0.5319998264312744\n",
      "Step complete! 0.5769994258880615\n",
      "Step complete! 0.5650010108947754\n",
      "Step complete! 0.4940001964569092\n",
      "Step complete! 0.5070006847381592\n",
      "Step complete! 0.6880028247833252\n",
      "Step complete! 0.7089970111846924\n",
      "Step complete! 0.6620004177093506\n",
      "Step complete! 0.48400378227233887\n",
      "Step complete! 0.4645230770111084\n",
      "Step complete! 0.6819980144500732\n",
      "Step complete! 0.4740028381347656\n",
      "Step complete! 0.7069966793060303\n",
      "Step complete! 0.5089952945709229\n",
      "Step complete! 0.5060045719146729\n",
      "Step complete! 0.7129900455474854\n",
      "Step complete! 0.7239999771118164\n",
      "Step complete! 0.6915156841278076\n",
      "Step complete! 0.5380053520202637\n",
      "Step complete! 0.459993839263916\n",
      "Step complete! 0.49199986457824707\n",
      "Step complete! 0.4719996452331543\n",
      "Step complete! 0.4920003414154053\n",
      "Step complete! 0.6800134181976318\n",
      "Step complete! 0.6619977951049805\n",
      "Step complete! 0.5025200843811035\n",
      "Step complete! 0.49100160598754883\n",
      "Step complete! 0.501000165939331\n",
      "Step complete! 0.5299994945526123\n",
      "Step complete! 0.521998405456543\n",
      "Step complete! 0.7080004215240479\n",
      "Step complete! 0.5330002307891846\n",
      "Step complete! 0.7349908351898193\n",
      "Step complete! 0.5095193386077881\n",
      "Step complete! 0.6906061172485352\n",
      "Step complete! 0.4720022678375244\n",
      "Step complete! 0.7250049114227295\n",
      "Step complete! 0.4729917049407959\n",
      "Step complete! 0.4949979782104492\n",
      "Step complete! 0.47899818420410156\n",
      "Step complete! 0.692000150680542\n",
      "Step complete! 0.6756017208099365\n",
      "Step complete! 0.5269966125488281\n",
      "Step complete! 0.511991024017334\n",
      "Step complete! 0.6980023384094238\n",
      "Step complete! 0.7480008602142334\n",
      "Step complete! 0.7290027141571045\n",
      "Step complete! 0.7229979038238525\n",
      "Step complete! 0.4979970455169678\n",
      "Step complete! 0.49899959564208984\n",
      "Step complete! 0.47700047492980957\n",
      "Step complete! 0.5020020008087158\n",
      "Step complete! 0.682997465133667\n",
      "Step complete! 0.5379996299743652\n",
      "Step complete! 0.7009985446929932\n",
      "Step complete! 0.7079970836639404\n",
      "Step complete! 0.5120017528533936\n",
      "Step complete! 0.5250074863433838\n",
      "Step complete! 0.7555098533630371\n",
      "Step complete! 0.7510020732879639\n",
      "Step complete! 0.5329978466033936\n",
      "Step complete! 0.7470014095306396\n",
      "Step complete! 0.6900043487548828\n",
      "Step complete! 0.6840000152587891\n",
      "Step complete! 0.492002010345459\n",
      "Step complete! 0.5309977531433105\n",
      "Step complete! 0.504002571105957\n",
      "Step complete! 0.4765195846557617\n",
      "Step complete! 0.476001501083374\n",
      "Step complete! 0.4659996032714844\n",
      "Step complete! 0.7539987564086914\n",
      "Step complete! 0.693000316619873\n",
      "Step complete! 0.5290143489837646\n",
      "Step complete! 0.5089828968048096\n",
      "Step complete! 0.707003116607666\n",
      "Step complete! 0.5130069255828857\n",
      "Step complete! 0.5195140838623047\n",
      "Step complete! 0.7223186492919922\n",
      "Step complete! 0.48799657821655273\n",
      "Step complete! 0.5160050392150879\n",
      "Step complete! 0.6899962425231934\n",
      "Step complete! 0.5210025310516357\n",
      "Step complete! 0.7080028057098389\n",
      "Step complete! 0.6939964294433594\n",
      "Step complete! 0.7020769119262695\n",
      "Step complete! 0.7079999446868896\n",
      "Step complete! 0.545001745223999\n",
      "Step complete! 0.7329931259155273\n",
      "Step complete! 0.7485270500183105\n",
      "Step complete! 0.5029962062835693\n",
      "Step complete! 0.5019984245300293\n",
      "Step complete! 0.516998291015625\n",
      "Step complete! 0.7340023517608643\n",
      "Step complete! 0.7159690856933594\n",
      "Step complete! 0.4829986095428467\n",
      "Step complete! 0.7215275764465332\n",
      "Step complete! 0.6729981899261475\n",
      "Step complete! 0.5130019187927246\n",
      "Step complete! 0.4830033779144287\n",
      "Step complete! 0.6919951438903809\n",
      "Step complete! 0.47224879264831543\n",
      "Step complete! 0.5049993991851807\n",
      "Step complete! 0.7229952812194824\n",
      "Step complete! 0.7430000305175781\n",
      "Step complete! 0.7839994430541992\n",
      "Step complete! 0.5560002326965332\n",
      "Step complete! 0.7280006408691406\n",
      "Step complete! 0.46952080726623535\n",
      "Step complete! 0.4720015525817871\n",
      "Step complete! 0.4909946918487549\n",
      "Step complete! 0.48700380325317383\n",
      "Step complete! 0.49799418449401855\n",
      "Step complete! 0.48900461196899414\n",
      "Step complete! 0.5090398788452148\n",
      "Step complete! 0.6819984912872314\n",
      "Step complete! 0.4869966506958008\n",
      "Step complete! 0.7590005397796631\n",
      "Step complete! 0.727515697479248\n",
      "Step complete! 0.742999792098999\n",
      "Step complete! 0.5430002212524414\n",
      "Step complete! 0.5740008354187012\n",
      "Step complete! 0.5139980316162109\n",
      "Step complete! 0.46700406074523926\n",
      "Step complete! 0.670997142791748\n",
      "Step complete! 0.7370467185974121\n",
      "Step complete! 0.6765508651733398\n",
      "Step complete! 0.6689963340759277\n",
      "Step complete! 0.46100354194641113\n",
      "Step complete! 0.6829986572265625\n",
      "Step complete! 0.5039997100830078\n",
      "Step complete! 0.5189979076385498\n",
      "Step complete! 0.7289986610412598\n",
      "Step complete! 0.7660024166107178\n",
      "Step complete! 0.530526876449585\n",
      "Step complete! 0.7390003204345703\n",
      "Step complete! 0.7220010757446289\n",
      "Step complete! 0.7460019588470459\n",
      "Step complete! 0.6680006980895996\n",
      "Step complete! 0.4799995422363281\n",
      "Step complete! 0.4830021858215332\n",
      "Step complete! 0.7165157794952393\n",
      "Step complete! 0.49663686752319336\n",
      "Step complete! 0.7129998207092285\n",
      "Step complete! 0.47699785232543945\n",
      "Step complete! 0.726006031036377\n",
      "Step complete! 0.5279974937438965\n",
      "Step complete! 0.5439913272857666\n",
      "Step complete! 0.5050013065338135\n",
      "Step complete! 0.5309979915618896\n",
      "Step complete! 0.5210056304931641\n",
      "Step complete! 0.5279977321624756\n",
      "Step complete! 0.5200121402740479\n",
      "Step complete! 0.45600223541259766\n",
      "Step complete! 0.49899840354919434\n",
      "Step complete! 0.4870014190673828\n",
      "Step complete! 0.7350022792816162\n",
      "Step complete! 0.6819982528686523\n",
      "Step complete! 0.6719982624053955\n",
      "Step complete! 0.5069990158081055\n",
      "Step complete! 0.4569988250732422\n",
      "Step complete! 0.7455241680145264\n",
      "Step complete! 0.49500012397766113\n",
      "Step complete! 0.5399985313415527\n",
      "Step complete! 0.5265219211578369\n",
      "Step complete! 0.5410022735595703\n",
      "Step complete! 0.7999978065490723\n",
      "Step complete! 0.7590014934539795\n",
      "Step complete! 0.6880021095275879\n",
      "Step complete! 0.48799777030944824\n",
      "Step complete! 0.6950011253356934\n",
      "Step complete! 0.49799680709838867\n",
      "Step complete! 0.48399972915649414\n",
      "Step complete! 0.7180001735687256\n",
      "Step complete! 0.6865062713623047\n",
      "Step complete! 0.4749908447265625\n",
      "Step complete! 0.7239975929260254\n",
      "Step complete! 0.7190032005310059\n",
      "Step complete! 0.7419981956481934\n",
      "Step complete! 0.7280075550079346\n",
      "Step complete! 0.734992265701294\n",
      "Step complete! 0.6950023174285889\n",
      "Step complete! 0.6915018558502197\n",
      "Step complete! 0.4940025806427002\n",
      "Step complete! 0.7249977588653564\n",
      "Step complete! 0.5149998664855957\n",
      "Step complete! 0.6919994354248047\n",
      "Step complete! 0.673154354095459\n",
      "Step complete! 0.73299241065979\n",
      "Step complete! 0.7280025482177734\n",
      "Step complete! 0.5660016536712646\n",
      "Step complete! 0.5339982509613037\n",
      "Step complete! 0.7155194282531738\n",
      "Step complete! 0.5389981269836426\n",
      "Step complete! 0.5230038166046143\n",
      "Step complete! 0.5079998970031738\n",
      "Step complete! 0.7150013446807861\n",
      "Step complete! 0.6939983367919922\n",
      "Step complete! 0.5220000743865967\n",
      "Step complete! 0.4929966926574707\n",
      "Step complete! 0.676520586013794\n",
      "Step complete! 0.4960012435913086\n",
      "Step complete! 0.7360022068023682\n",
      "Step complete! 0.5559892654418945\n",
      "Step complete! 0.5509974956512451\n",
      "Step complete! 0.7370004653930664\n",
      "Step complete! 0.724999189376831\n",
      "Step complete! 0.7195172309875488\n",
      "Step complete! 0.5140142440795898\n",
      "Step complete! 0.5167021751403809\n",
      "Step complete! 0.4739816188812256\n",
      "Step complete! 0.4920024871826172\n",
      "Step complete! 0.4909970760345459\n",
      "Step complete! 0.49799466133117676\n",
      "Step complete! 0.48799800872802734\n",
      "Step complete! 0.47199535369873047\n",
      "Step complete! 0.5910193920135498\n",
      "Step complete! 0.6909990310668945\n",
      "Step complete! 0.6900043487548828\n",
      "Step complete! 0.5030004978179932\n",
      "Step complete! 0.7250034809112549\n",
      "Step complete! 0.7369980812072754\n",
      "Step complete! 0.5460000038146973\n",
      "Step complete! 0.5629982948303223\n",
      "Step complete! 0.503000020980835\n",
      "Step complete! 0.5039999485015869\n",
      "Step complete! 0.7270505428314209\n",
      "Step complete! 0.690500020980835\n",
      "Step complete! 0.510000467300415\n",
      "Step complete! 0.733006477355957\n",
      "Step complete! 0.46704936027526855\n",
      "Step complete! 0.4740009307861328\n",
      "Step complete! 1.7070026397705078\n",
      "Step complete! 0.5309984683990479\n",
      "Step complete! 0.520521879196167\n",
      "Step complete! 0.5439980030059814\n",
      "Step complete! 0.4999985694885254\n",
      "Step complete! 0.7519993782043457\n",
      "Step complete! 0.7723422050476074\n",
      "Step complete! 0.6687653064727783\n",
      "Step complete! 0.485973596572876\n",
      "Step complete! 0.48600172996520996\n",
      "Step complete! 0.5019979476928711\n",
      "Step complete! 0.4850003719329834\n",
      "Step complete! 0.5050010681152344\n",
      "Step complete! 0.4739995002746582\n",
      "Step complete! 0.476686954498291\n",
      "Step complete! 0.7149994373321533\n",
      "Step complete! 0.5220005512237549\n",
      "Step complete! 0.5290110111236572\n",
      "Step complete! 0.5069887638092041\n",
      "Step complete! 0.7270083427429199\n",
      "Step complete! 0.7579925060272217\n",
      "Step complete! 0.6799972057342529\n",
      "Step complete! 0.5120017528533936\n",
      "Step complete! 0.502000093460083\n",
      "Step complete! 0.7179975509643555\n",
      "Step complete! 0.7067770957946777\n",
      "Step complete! 0.7200021743774414\n",
      "Step complete! 0.814997673034668\n",
      "Step complete! 0.6950027942657471\n",
      "Step complete! 0.5159990787506104\n",
      "Step complete! 0.5319929122924805\n",
      "Step complete! 0.5569992065429688\n",
      "Step complete! 0.5739986896514893\n",
      "Step complete! 0.7569968700408936\n",
      "Step complete! 0.7359843254089355\n",
      "Step complete! 0.7890002727508545\n",
      "Step complete! 0.7540137767791748\n",
      "Step complete! 0.5240023136138916\n",
      "Step complete! 0.5130181312561035\n",
      "Step complete! 0.7809998989105225\n",
      "Step complete! 0.5490052700042725\n",
      "Step complete! 0.7121114730834961\n",
      "Step complete! 0.8790018558502197\n",
      "Step complete! 0.5749993324279785\n",
      "Step complete! 0.68300461769104\n",
      "Step complete! 0.7749967575073242\n",
      "Step complete! 0.7739994525909424\n",
      "Step complete! 0.7989974021911621\n",
      "Step complete! 0.5530040264129639\n",
      "Step complete! 0.5265200138092041\n",
      "Step complete! 0.7499978542327881\n",
      "Step complete! 0.536996603012085\n",
      "Step complete! 0.7169992923736572\n",
      "Step complete! 0.5360004901885986\n",
      "Step complete! 0.8449974060058594\n",
      "Step complete! 0.5390033721923828\n",
      "Step complete! 0.5875244140625\n",
      "Step complete! 0.5859994888305664\n",
      "Step complete! 0.5949873924255371\n",
      "Step complete! 0.5809969902038574\n",
      "Step complete! 0.7610008716583252\n",
      "Step complete! 0.561007022857666\n",
      "Step complete! 0.5675156116485596\n",
      "Step complete! 0.753997802734375\n",
      "Step complete! 0.7320032119750977\n",
      "Step complete! 0.5159969329833984\n",
      "Step complete! 0.556999921798706\n",
      "Step complete! 0.523000955581665\n",
      "Step complete! 0.529000997543335\n",
      "Step complete! 0.5469999313354492\n",
      "Step complete! 0.5835123062133789\n",
      "Step complete! 0.582005500793457\n",
      "Step complete! 0.7760002613067627\n",
      "Step complete! 0.5990023612976074\n",
      "Step complete! 0.7910001277923584\n",
      "Step complete! 0.5859971046447754\n",
      "Step complete! 0.6030035018920898\n",
      "Step complete! 0.7589895725250244\n",
      "Step complete! 0.6250064373016357\n",
      "Step complete! 0.7942466735839844\n",
      "Step complete! 0.7803030014038086\n",
      "Step complete! 0.5699987411499023\n",
      "Step complete! 0.5850014686584473\n",
      "Step complete! 0.7720022201538086\n",
      "Step complete! 0.8269977569580078\n",
      "Step complete! 0.8689985275268555\n",
      "Step complete! 0.7805242538452148\n",
      "Step complete! 0.6119942665100098\n",
      "Step complete! 0.5979952812194824\n",
      "Step complete! 0.7350022792816162\n",
      "Step complete! 0.8580114841461182\n",
      "Step complete! 0.5439977645874023\n",
      "Step complete! 0.5900013446807861\n",
      "Step complete! 0.5505263805389404\n",
      "Step complete! 0.7810065746307373\n",
      "Step complete! 0.7789976596832275\n",
      "Step complete! 0.7969939708709717\n",
      "Step complete! 0.7969989776611328\n",
      "Step complete! 0.8009994029998779\n",
      "Step complete! 0.6405200958251953\n",
      "Step complete! 0.5619990825653076\n",
      "Step complete! 0.7890028953552246\n",
      "Step complete! 0.5469968318939209\n",
      "Step complete! 0.7300002574920654\n",
      "Step complete! 0.7750005722045898\n",
      "Step complete! 0.752000093460083\n",
      "Step complete! 0.5340008735656738\n",
      "Step complete! 0.5485079288482666\n",
      "Step complete! 0.5589964389801025\n",
      "Step complete! 0.7639994621276855\n",
      "Step complete! 0.5949978828430176\n",
      "Step complete! 0.8650009632110596\n",
      "Step complete! 0.5850028991699219\n",
      "Step complete! 0.7745206356048584\n",
      "Step complete! 0.601996898651123\n",
      "Step complete! 0.5739929676055908\n",
      "Step complete! 0.7659914493560791\n",
      "Step complete! 0.5550034046173096\n",
      "Step complete! 0.5799994468688965\n",
      "Step complete! 0.7510056495666504\n",
      "Step complete! 0.7230880260467529\n",
      "Step complete! 0.5330002307891846\n",
      "Step complete! 0.756507396697998\n",
      "Step complete! 0.7760035991668701\n",
      "Step complete! 0.574995756149292\n",
      "Step complete! 0.7899973392486572\n",
      "Step complete! 0.7940013408660889\n",
      "Step complete! 0.7520020008087158\n",
      "Step complete! 0.7225332260131836\n",
      "Step complete! 0.5719993114471436\n",
      "Step complete! 0.549004077911377\n",
      "Step complete! 0.5680043697357178\n",
      "Step complete! 0.5649926662445068\n",
      "Step complete! 0.7440018653869629\n",
      "Step complete! 0.7549982070922852\n",
      "Step complete! 0.7699985504150391\n",
      "Step complete! 0.581000566482544\n",
      "Step complete! 0.598522424697876\n",
      "Step complete! 0.5820040702819824\n",
      "Step complete! 0.5950009822845459\n",
      "Step complete! 0.5779986381530762\n",
      "Step complete! 0.7799959182739258\n",
      "Step complete! 0.5319995880126953\n",
      "Step complete! 0.561265230178833\n",
      "Step complete! 0.5450003147125244\n",
      "Step complete! 0.7939906120300293\n",
      "Step complete! 0.7030055522918701\n",
      "Step complete! 0.7310035228729248\n",
      "Step complete! 0.5255153179168701\n",
      "Step complete! 0.7549998760223389\n",
      "Step complete! 0.5949993133544922\n",
      "Step complete! 0.7900059223175049\n",
      "Step complete! 0.5800118446350098\n",
      "Step complete! 0.7679946422576904\n",
      "Step complete! 0.5965192317962646\n",
      "Step complete! 0.5999984741210938\n",
      "Step complete! 0.5760030746459961\n",
      "Step complete! 0.5329990386962891\n",
      "Step complete! 0.7409977912902832\n",
      "Step complete! 0.7299988269805908\n",
      "Step complete! 0.5179998874664307\n",
      "Step complete! 0.5820014476776123\n",
      "Step complete! 0.7260072231292725\n",
      "Step complete! 0.5315258502960205\n",
      "Step complete! 0.7729980945587158\n",
      "Step complete! 0.5720000267028809\n",
      "Step complete! 0.5950024127960205\n",
      "Step complete! 0.609992504119873\n",
      "Step complete! 0.5959987640380859\n",
      "Step complete! 0.6009986400604248\n",
      "Step complete! 0.5485265254974365\n",
      "Step complete! 0.5499975681304932\n",
      "Step complete! 0.7595219612121582\n",
      "Step complete! 0.5189993381500244\n",
      "Step complete! 0.5700170993804932\n",
      "Step complete! 0.7239973545074463\n",
      "Step complete! 0.5219993591308594\n",
      "Step complete! 0.5260021686553955\n",
      "Step complete! 0.5560004711151123\n",
      "Step complete! 0.5649993419647217\n",
      "Step complete! 0.5610027313232422\n",
      "Step complete! 0.603996992111206\n",
      "Step complete! 0.554006814956665\n",
      "Step complete! 0.8069882392883301\n",
      "Step complete! 0.7500004768371582\n",
      "Step complete! 0.6980190277099609\n",
      "Step complete! 0.5445215702056885\n",
      "Step complete! 0.7539980411529541\n",
      "Step complete! 0.5199987888336182\n",
      "Step complete! 0.5180015563964844\n",
      "Step complete! 0.7090017795562744\n",
      "Step complete! 0.521998405456543\n",
      "Step complete! 0.5039982795715332\n",
      "Step complete! 0.523003339767456\n",
      "Step complete! 0.764998197555542\n",
      "Step complete! 0.6175153255462646\n",
      "Step complete! 0.7609996795654297\n",
      "Step complete! 0.759000301361084\n",
      "Step complete! 0.7290081977844238\n",
      "Step complete! 0.5580036640167236\n",
      "Step complete! 0.7429988384246826\n",
      "Step complete! 0.515510082244873\n",
      "Step complete! 0.5375759601593018\n",
      "Step complete! 0.6990001201629639\n",
      "Step complete! 0.7349984645843506\n",
      "Step complete! 0.5320029258728027\n",
      "Step complete! 0.5249958038330078\n",
      "Step complete! 0.543001651763916\n",
      "Step complete! 0.5369994640350342\n",
      "Step complete! 0.7380037307739258\n",
      "Step complete! 0.5500004291534424\n",
      "Step complete! 0.5889906883239746\n",
      "Step complete! 0.5259983539581299\n",
      "Step complete! 0.7439987659454346\n",
      "Step complete! 0.5440020561218262\n",
      "Step complete! 0.7540013790130615\n",
      "Step complete! 0.5159873962402344\n",
      "Step complete! 0.7445180416107178\n",
      "Step complete! 0.49399876594543457\n",
      "Step complete! 0.5030009746551514\n",
      "Step complete! 0.5230162143707275\n",
      "Step complete! 0.7429823875427246\n",
      "Step complete! 0.7599987983703613\n",
      "Step complete! 0.7510063648223877\n",
      "Step complete! 0.6039953231811523\n",
      "Step complete! 0.5675172805786133\n",
      "Step complete! 0.5350019931793213\n",
      "Step complete! 0.7690012454986572\n",
      "Step complete! 0.5320000648498535\n",
      "Step complete! 0.5269992351531982\n",
      "Step complete! 0.7459967136383057\n",
      "Step complete! 0.7090249061584473\n",
      "Step complete! 0.7155129909515381\n",
      "Step complete! 0.517998218536377\n",
      "Step complete! 0.7396106719970703\n",
      "Step complete! 0.544999361038208\n",
      "Step complete! 0.7670092582702637\n",
      "Step complete! 0.5579941272735596\n",
      "Step complete! 0.7330005168914795\n",
      "Step complete! 0.7279999256134033\n",
      "Step complete! 0.7690017223358154\n",
      "Step complete! 0.5079991817474365\n",
      "Step complete! 0.4969959259033203\n",
      "Step complete! 0.5730020999908447\n",
      "Step complete! 0.7189981937408447\n",
      "Step complete! 0.6970078945159912\n",
      "Step complete! 0.7159960269927979\n",
      "Step complete! 0.5049991607666016\n",
      "Step complete! 0.5259997844696045\n",
      "Step complete! 0.5565168857574463\n",
      "Step complete! 0.5970010757446289\n",
      "Step complete! 0.5879981517791748\n",
      "Step complete! 0.7300057411193848\n",
      "Step complete! 0.7299995422363281\n",
      "Step complete! 0.5559985637664795\n",
      "Step complete! 0.5290029048919678\n",
      "Step complete! 0.552997350692749\n",
      "Step complete! 0.5005168914794922\n",
      "Step complete! 0.5359976291656494\n",
      "Step complete! 0.6984055042266846\n",
      "Step complete! 0.7119996547698975\n",
      "Step complete! 0.7459943294525146\n",
      "Step complete! 0.7210016250610352\n",
      "Step complete! 0.5079858303070068\n",
      "Step complete! 0.7349987030029297\n",
      "Step complete! 0.7790021896362305\n",
      "Step complete! 0.5460071563720703\n",
      "Step complete! 0.5565357208251953\n",
      "Step complete! 0.7109990119934082\n",
      "Step complete! 0.5349962711334229\n",
      "Step complete! 0.4920012950897217\n",
      "Step complete! 0.5379989147186279\n",
      "Step complete! 0.7109980583190918\n",
      "Step complete! 0.5249896049499512\n",
      "Step complete! 0.5140068531036377\n",
      "Step complete! 0.6949942111968994\n",
      "Step complete! 0.7240200042724609\n",
      "Step complete! 0.7300057411193848\n",
      "Step complete! 0.7615087032318115\n",
      "Step complete! 0.7409989833831787\n",
      "Step complete! 0.7780027389526367\n",
      "Step complete! 0.554997444152832\n",
      "Step complete! 0.588998556137085\n",
      "Step complete! 0.5650019645690918\n",
      "Step complete! 0.7200360298156738\n",
      "Step complete! 0.4780001640319824\n",
      "Step complete! 0.6925175189971924\n",
      "Step complete! 0.7107279300689697\n",
      "Step complete! 0.5010027885437012\n",
      "Step complete! 0.7500011920928955\n",
      "Step complete! 0.5189986228942871\n",
      "Step complete! 0.7410023212432861\n",
      "Step complete! 0.5339953899383545\n",
      "Step complete! 0.5430021286010742\n",
      "Step complete! 0.5260004997253418\n",
      "Step complete! 0.7399997711181641\n",
      "Step complete! 0.5260014533996582\n",
      "Step complete! 0.7320001125335693\n",
      "Step complete! 0.6789989471435547\n",
      "Step complete! 0.693000078201294\n",
      "Step complete! 0.691997766494751\n",
      "Step complete! 0.5003657341003418\n",
      "Step complete! 0.7120001316070557\n",
      "Step complete! 0.7209980487823486\n",
      "Step complete! 0.7379992008209229\n",
      "Step complete! 0.5240013599395752\n",
      "Step complete! 0.5570034980773926\n",
      "Step complete! 0.7719950675964355\n",
      "Step complete! 0.5469973087310791\n",
      "Step complete! 0.5540099143981934\n",
      "Step complete! 0.7365052700042725\n",
      "Step complete! 0.5139918327331543\n",
      "Step complete! 0.5149974822998047\n",
      "Step complete! 0.7190015316009521\n",
      "Step complete! 0.7360031604766846\n",
      "Step complete! 0.5089964866638184\n",
      "Step complete! 0.5050029754638672\n",
      "Step complete! 0.5170001983642578\n",
      "Step complete! 0.6215403079986572\n",
      "Step complete! 0.7253592014312744\n",
      "Step complete! 0.4949982166290283\n",
      "Step complete! 0.7730019092559814\n",
      "Step complete! 0.5590019226074219\n",
      "Step complete! 0.6040010452270508\n",
      "Step complete! 0.5590031147003174\n",
      "Step complete! 0.7109935283660889\n",
      "Step complete! 0.7389981746673584\n",
      "Step complete! 0.5379996299743652\n",
      "Step complete! 0.7179994583129883\n",
      "Step complete! 0.5500102043151855\n",
      "Step complete! 0.5155234336853027\n",
      "Step complete! 0.7360002994537354\n",
      "Step complete! 0.537999153137207\n",
      "Step complete! 0.7380020618438721\n",
      "Step complete! 0.7399985790252686\n",
      "Step complete! 0.5279989242553711\n",
      "Step complete! 0.7380044460296631\n",
      "Step complete! 0.5459976196289062\n",
      "Step complete! 0.5325307846069336\n",
      "Step complete! 0.7391772270202637\n",
      "Step complete! 0.54500412940979\n",
      "Step complete! 0.5039939880371094\n",
      "Step complete! 0.73073410987854\n",
      "Step complete! 0.5000021457672119\n",
      "Step complete! 0.680999755859375\n",
      "Step complete! 0.7080032825469971\n",
      "Step complete! 0.49899983406066895\n",
      "Step complete! 0.7350010871887207\n",
      "Step complete! 0.7569990158081055\n",
      "Step complete! 0.540001392364502\n",
      "Step complete! 0.5549952983856201\n",
      "Step complete! 0.5280053615570068\n",
      "Step complete! 0.7519967555999756\n",
      "Step complete! 0.512998104095459\n",
      "Step complete! 0.7144820690155029\n",
      "Step complete! 0.7037074565887451\n",
      "Step complete! 0.5210001468658447\n",
      "Step complete! 0.7199957370758057\n",
      "Step complete! 0.5130093097686768\n",
      "Step complete! 0.5130000114440918\n",
      "Step complete! 0.5200088024139404\n",
      "Step complete! 0.5010113716125488\n",
      "Step complete! 0.5169968605041504\n",
      "Step complete! 0.5880014896392822\n",
      "Step complete! 0.5459985733032227\n",
      "Step complete! 0.7740030288696289\n",
      "Step complete! 0.7710039615631104\n",
      "Step complete! 0.7419970035552979\n",
      "Step complete! 0.5299992561340332\n",
      "Step complete! 0.6870017051696777\n",
      "Step complete! 0.4920005798339844\n",
      "Step complete! 0.7129971981048584\n",
      "Step complete! 0.4970118999481201\n",
      "Step complete! 0.5380008220672607\n",
      "Step complete! 0.518996000289917\n",
      "Step complete! 0.6999993324279785\n",
      "Step complete! 0.5139999389648438\n",
      "Step complete! 0.5395233631134033\n",
      "Step complete! 0.5419981479644775\n",
      "Step complete! 0.7660007476806641\n",
      "Step complete! 0.5129978656768799\n",
      "Step complete! 0.7670011520385742\n",
      "Step complete! 0.5359997749328613\n",
      "Step complete! 0.7259984016418457\n",
      "Step complete! 0.7464356422424316\n",
      "Step complete! 0.8110034465789795\n",
      "Step complete! 0.5099947452545166\n",
      "Step complete! 0.5330009460449219\n",
      "Step complete! 0.5030007362365723\n",
      "Step complete! 0.4959993362426758\n",
      "Step complete! 0.5500016212463379\n",
      "Step complete! 0.5120017528533936\n",
      "Step complete! 0.7720036506652832\n",
      "Step complete! 0.5709996223449707\n",
      "Step complete! 0.5640006065368652\n",
      "Step complete! 0.5449965000152588\n",
      "Step complete! 0.5390033721923828\n",
      "Step complete! 0.7290005683898926\n",
      "Step complete! 0.7090020179748535\n",
      "Step complete! 0.48699951171875\n",
      "Step complete! 0.4669985771179199\n",
      "Step complete! 0.6899979114532471\n",
      "Step complete! 0.5030016899108887\n",
      "Step complete! 0.6740121841430664\n",
      "Step complete! 0.4969937801361084\n",
      "Step complete! 0.4780008792877197\n",
      "Step complete! 0.7069973945617676\n",
      "Step complete! 0.7400000095367432\n",
      "Step complete! 0.7409980297088623\n",
      "Step complete! 0.5359995365142822\n",
      "Step complete! 0.7239978313446045\n",
      "Step complete! 0.7270023822784424\n",
      "Step complete! 0.5379948616027832\n",
      "Step complete! 0.7250041961669922\n",
      "Step complete! 0.5329968929290771\n",
      "Step complete! 0.5130014419555664\n",
      "Step complete! 0.5049984455108643\n",
      "Step complete! 0.681997537612915\n",
      "Step complete! 0.5050013065338135\n",
      "Step complete! 0.5019979476928711\n",
      "Step complete! 0.7009992599487305\n",
      "Step complete! 0.7479987144470215\n",
      "Step complete! 0.7345240116119385\n",
      "Step complete! 0.5890049934387207\n",
      "Step complete! 0.7259976863861084\n",
      "Step complete! 0.5639991760253906\n",
      "Step complete! 0.7480041980743408\n",
      "Step complete! 0.5070009231567383\n",
      "Step complete! 0.5200028419494629\n",
      "Step complete! 0.4925110340118408\n",
      "Step complete! 0.668999433517456\n",
      "Step complete! 0.6959872245788574\n",
      "Step complete! 0.7134110927581787\n",
      "Step complete! 0.7089989185333252\n",
      "Step complete! 0.708996057510376\n",
      "Step complete! 0.5139985084533691\n",
      "Step complete! 0.8300163745880127\n",
      "Step complete! 0.7159984111785889\n",
      "Step complete! 0.5430030822753906\n",
      "Step complete! 0.608997106552124\n",
      "Step complete! 0.7160029411315918\n",
      "Step complete! 0.4740018844604492\n",
      "Step complete! 0.521007776260376\n",
      "Step complete! 0.6995031833648682\n",
      "Step complete! 0.5019989013671875\n",
      "Step complete! 0.4729957580566406\n",
      "Step complete! 0.5019993782043457\n",
      "Step complete! 0.707000732421875\n",
      "Step complete! 0.7213857173919678\n",
      "Step complete! 0.7420022487640381\n",
      "Step complete! 0.755997896194458\n",
      "Step complete! 0.5590007305145264\n",
      "Step complete! 0.7339990139007568\n",
      "Step complete! 0.723006010055542\n",
      "Step complete! 0.5619981288909912\n",
      "Step complete! 0.508507490158081\n",
      "Step complete! 0.6950023174285889\n",
      "Step complete! 0.5319991111755371\n",
      "Step complete! 0.535301923751831\n",
      "Step complete! 0.67099928855896\n",
      "Step complete! 0.5040011405944824\n",
      "Step complete! 0.6890017986297607\n",
      "Step complete! 0.5069973468780518\n",
      "Step complete! 0.48200106620788574\n",
      "Step complete! 0.6069989204406738\n",
      "Step complete! 0.550999641418457\n",
      "Step complete! 0.5509994029998779\n",
      "Step complete! 0.781998872756958\n",
      "Step complete! 0.5450026988983154\n",
      "Step complete! 0.7545278072357178\n",
      "Step complete! 0.7299976348876953\n",
      "Step complete! 0.719003438949585\n",
      "Step complete! 0.7400150299072266\n",
      "Step complete! 0.5029995441436768\n",
      "Step complete! 0.5090055465698242\n",
      "Step complete! 0.7279973030090332\n",
      "Step complete! 0.7095181941986084\n",
      "Step complete! 0.7220146656036377\n",
      "Step complete! 0.5449886322021484\n",
      "Step complete! 0.5350048542022705\n",
      "Step complete! 0.7349998950958252\n",
      "Step complete! 0.5290102958679199\n",
      "Step complete! 0.5169973373413086\n",
      "Step complete! 0.5310025215148926\n",
      "Step complete! 0.5175189971923828\n",
      "Step complete! 0.5330042839050293\n",
      "Step complete! 0.4999964237213135\n",
      "Step complete! 0.6930027008056641\n",
      "Step complete! 0.5150001049041748\n",
      "Step complete! 0.7180013656616211\n",
      "Step complete! 0.7005610466003418\n",
      "Step complete! 0.715543270111084\n",
      "Step complete! 0.6040017604827881\n",
      "Step complete! 0.5549981594085693\n",
      "Step complete! 0.5310029983520508\n",
      "Step complete! 0.540001392364502\n",
      "Step complete! 0.5290000438690186\n",
      "Step complete! 0.7269988059997559\n",
      "Step complete! 0.5539989471435547\n",
      "Step complete! 0.7180099487304688\n",
      "Step complete! 0.7179989814758301\n",
      "Step complete! 0.5210003852844238\n",
      "Step complete! 0.7309978008270264\n",
      "Step complete! 0.532001256942749\n",
      "Step complete! 0.48200011253356934\n",
      "Step complete! 0.7049684524536133\n",
      "Step complete! 0.516002893447876\n",
      "Step complete! 0.5410103797912598\n",
      "Step complete! 0.5359897613525391\n",
      "Step complete! 0.7235214710235596\n",
      "Step complete! 0.5289995670318604\n",
      "Step complete! 0.8020024299621582\n",
      "Step complete! 0.555997371673584\n",
      "Step complete! 0.7319998741149902\n",
      "Step complete! 0.5000019073486328\n",
      "Step complete! 0.5479953289031982\n",
      "Step complete! 0.5099892616271973\n",
      "Step complete! 0.49651622772216797\n",
      "Step complete! 0.7229983806610107\n",
      "Step complete! 0.4920027256011963\n",
      "Step complete! 0.7430026531219482\n",
      "Step complete! 0.7109982967376709\n",
      "Step complete! 0.5580019950866699\n",
      "Step complete! 0.5740020275115967\n",
      "Step complete! 0.7515170574188232\n",
      "Step complete! 0.7469992637634277\n",
      "Step complete! 0.7740011215209961\n",
      "Step complete! 0.5509984493255615\n",
      "Step complete! 0.5540030002593994\n",
      "Step complete! 0.5019865036010742\n",
      "Step complete! 0.7140109539031982\n",
      "Step complete! 0.6965193748474121\n",
      "Step complete! 0.5249919891357422\n",
      "Step complete! 0.7240021228790283\n",
      "Step complete! 0.49399757385253906\n",
      "Step complete! 0.48800182342529297\n",
      "Step complete! 0.7391207218170166\n",
      "Step complete! 0.7380001544952393\n",
      "Step complete! 0.7660071849822998\n",
      "Step complete! 0.5399990081787109\n",
      "Step complete! 0.7245197296142578\n",
      "Step complete! 0.52699875831604\n",
      "Step complete! 0.7100002765655518\n",
      "Step complete! 0.4909982681274414\n",
      "Step complete! 0.49399614334106445\n",
      "Step complete! 0.48900365829467773\n",
      "Step complete! 0.49700140953063965\n",
      "Step complete! 0.5089986324310303\n",
      "Step complete! 0.6780092716217041\n",
      "Step complete! 0.6765117645263672\n",
      "Step complete! 0.7390811443328857\n",
      "Step complete! 0.5350031852722168\n",
      "Step complete! 0.5180275440216064\n",
      "Step complete! 0.5419981479644775\n",
      "Step complete! 0.6940016746520996\n",
      "Step complete! 0.5289990901947021\n",
      "Step complete! 0.7625298500061035\n",
      "Step complete! 0.7009990215301514\n",
      "Step complete! 0.5079998970031738\n",
      "Step complete! 0.7019977569580078\n",
      "Step complete! 0.7380070686340332\n",
      "Step complete! 0.6879937648773193\n",
      "Step complete! 0.5119984149932861\n",
      "Step complete! 0.5040040016174316\n",
      "Step complete! 0.4979989528656006\n",
      "Step complete! 0.7225568294525146\n",
      "Step complete! 0.7440028190612793\n",
      "Step complete! 0.5009980201721191\n",
      "Step complete! 0.730999231338501\n",
      "Step complete! 0.7249999046325684\n",
      "Step complete! 0.5180008411407471\n",
      "Step complete! 0.7390027046203613\n",
      "Step complete! 0.6979966163635254\n",
      "Step complete! 0.6935200691223145\n",
      "Step complete! 0.6980006694793701\n",
      "Step complete! 0.7110049724578857\n",
      "Step complete! 0.6980817317962646\n",
      "Step complete! 0.49097299575805664\n",
      "Step complete! 0.49300527572631836\n",
      "Step complete! 0.5280046463012695\n",
      "Step complete! 0.7419958114624023\n",
      "Step complete! 0.7195203304290771\n",
      "Step complete! 0.5769963264465332\n",
      "Step complete! 0.7990155220031738\n",
      "Step complete! 0.5240116119384766\n",
      "Step complete! 0.6940000057220459\n",
      "Step complete! 0.7030026912689209\n",
      "Step complete! 0.5020022392272949\n",
      "Step complete! 0.6979975700378418\n",
      "Step complete! 0.7465143203735352\n",
      "Step complete! 0.7110040187835693\n",
      "Step complete! 0.4980130195617676\n",
      "Step complete! 0.48699069023132324\n",
      "Step complete! 0.5120031833648682\n",
      "Step complete! 0.7269992828369141\n",
      "Step complete! 0.7139987945556641\n",
      "Step complete! 0.5159986019134521\n",
      "Step complete! 0.7055208683013916\n",
      "Step complete! 0.5369994640350342\n",
      "Step complete! 0.7129981517791748\n",
      "Step complete! 0.49000048637390137\n",
      "Step complete! 0.49900054931640625\n",
      "Step complete! 0.68499755859375\n",
      "Step complete! 0.6736693382263184\n",
      "Step complete! 0.48299503326416016\n",
      "Step complete! 0.47500014305114746\n",
      "Step complete! 0.47299742698669434\n",
      "Step complete! 0.6670043468475342\n",
      "Step complete! 0.7560012340545654\n",
      "Step complete! 0.5279994010925293\n",
      "Step complete! 0.7560014724731445\n",
      "Step complete! 0.7469992637634277\n",
      "Step complete! 0.7040026187896729\n",
      "Step complete! 0.48398590087890625\n",
      "Step complete! 0.7165300846099854\n",
      "Step complete! 0.4839959144592285\n",
      "Step complete! 0.46500658988952637\n",
      "Step complete! 0.7118794918060303\n",
      "Step complete! 0.4910011291503906\n",
      "Step complete! 0.6915154457092285\n",
      "Step complete! 0.5159995555877686\n",
      "Step complete! 0.7279984951019287\n",
      "Step complete! 0.5260014533996582\n",
      "Step complete! 0.5510106086730957\n",
      "Step complete! 0.525998592376709\n",
      "Step complete! 0.5450012683868408\n",
      "Step complete! 0.5139989852905273\n",
      "Step complete! 0.5069983005523682\n",
      "Step complete! 0.7329995632171631\n",
      "Step complete! 0.7035202980041504\n",
      "Step complete! 0.6920037269592285\n",
      "Step complete! 0.4759948253631592\n",
      "Step complete! 0.7109944820404053\n",
      "Step complete! 0.5119876861572266\n",
      "Step complete! 0.6969990730285645\n",
      "Step complete! 0.7080018520355225\n",
      "Step complete! 0.7945244312286377\n",
      "Step complete! 0.7639997005462646\n",
      "Step complete! 0.5600087642669678\n",
      "Step complete! 0.5459954738616943\n",
      "Step complete! 0.7449986934661865\n",
      "Step complete! 0.5370030403137207\n",
      "Step complete! 0.7195212841033936\n",
      "Step complete! 0.7139983177185059\n",
      "Step complete! 0.527003288269043\n",
      "Step complete! 0.506995677947998\n",
      "Step complete! 0.4969961643218994\n",
      "Step complete! 0.6825249195098877\n",
      "Step complete! 0.47899723052978516\n",
      "Step complete! 0.4770016670227051\n",
      "Step complete! 0.7190017700195312\n",
      "Step complete! 0.5510013103485107\n",
      "Step complete! 0.7409992218017578\n",
      "Step complete! 0.6259992122650146\n",
      "Step complete! 0.7820019721984863\n",
      "Step complete! 0.5239927768707275\n",
      "Step complete! 0.702002763748169\n",
      "Step complete! 0.7020034790039062\n",
      "Step complete! 0.48800063133239746\n",
      "Step complete! 0.47899818420410156\n",
      "Step complete! 0.7405133247375488\n",
      "Step complete! 0.5190002918243408\n",
      "Step complete! 0.5100011825561523\n",
      "Step complete! 0.7130043506622314\n",
      "Step complete! 0.47399377822875977\n",
      "Step complete! 0.7359933853149414\n",
      "Step complete! 0.7499992847442627\n",
      "Step complete! 0.7175235748291016\n",
      "Step complete! 0.5979952812194824\n",
      "Step complete! 0.5390005111694336\n",
      "Step complete! 0.49399590492248535\n",
      "Step complete! 0.6899161338806152\n",
      "Step complete! 0.4740030765533447\n",
      "Step complete! 0.49699854850769043\n",
      "Step complete! 0.6729950904846191\n",
      "Step complete! 0.6769981384277344\n",
      "Step complete! 0.7180089950561523\n",
      "Step complete! 0.5099453926086426\n",
      "Step complete! 0.5239973068237305\n",
      "Step complete! 0.732003927230835\n",
      "Step complete! 0.6970031261444092\n",
      "Step complete! 0.5369970798492432\n",
      "Step complete! 0.7300012111663818\n",
      "Step complete! 0.7179975509643555\n",
      "Step complete! 0.708853006362915\n",
      "Step complete! 0.4849967956542969\n",
      "Step complete! 0.4965248107910156\n",
      "Step complete! 0.6949961185455322\n",
      "Step complete! 0.546999454498291\n",
      "Step complete! 0.5170023441314697\n",
      "Step complete! 0.5049972534179688\n",
      "Step complete! 0.5340018272399902\n",
      "Step complete! 0.5509989261627197\n",
      "Step complete! 0.7910001277923584\n",
      "Step complete! 0.7215156555175781\n",
      "Step complete! 0.5379965305328369\n",
      "Step complete! 0.7409994602203369\n",
      "Step complete! 0.7910034656524658\n",
      "Step complete! 0.5309953689575195\n",
      "Step complete! 0.5389997959136963\n",
      "Step complete! 0.4699985980987549\n",
      "Step complete! 0.7115206718444824\n",
      "Step complete! 0.6886472702026367\n",
      "Step complete! 0.7059938907623291\n",
      "Step complete! 0.7036194801330566\n",
      "Step complete! 0.6689987182617188\n",
      "Step complete! 0.5319998264312744\n",
      "Step complete! 0.7209994792938232\n",
      "Step complete! 0.7339985370635986\n",
      "Step complete! 0.5390017032623291\n",
      "Step complete! 0.696998119354248\n",
      "Step complete! 0.7399983406066895\n",
      "Step complete! 0.6719996929168701\n",
      "Step complete! 0.6880035400390625\n",
      "Step complete! 0.49300098419189453\n",
      "Step complete! 0.687434196472168\n",
      "Step complete! 0.7099955081939697\n",
      "Step complete! 0.6889996528625488\n",
      "Step complete! 0.5540001392364502\n",
      "Step complete! 0.49300122261047363\n",
      "Step complete! 0.553001880645752\n",
      "Step complete! 0.7460017204284668\n",
      "Step complete! 0.7299985885620117\n",
      "Step complete! 0.5020036697387695\n",
      "Step complete! 0.7219998836517334\n",
      "Step complete! 0.5160014629364014\n",
      "Step complete! 0.6879959106445312\n",
      "Step complete! 0.5109994411468506\n",
      "Step complete! 0.6815004348754883\n",
      "Step complete! 0.6829965114593506\n",
      "Step complete! 0.7139995098114014\n",
      "Step complete! 0.48600196838378906\n",
      "Step complete! 0.4850180149078369\n",
      "Step complete! 0.5169861316680908\n",
      "Step complete! 0.7010045051574707\n",
      "Step complete! 0.5609908103942871\n",
      "Step complete! 0.49700188636779785\n",
      "Step complete! 0.5450000762939453\n",
      "Step complete! 0.5249981880187988\n",
      "Step complete! 0.5350027084350586\n",
      "Step complete! 0.5079994201660156\n",
      "Step complete! 0.4850020408630371\n",
      "Step complete! 0.6993505954742432\n",
      "Step complete! 0.7070012092590332\n",
      "Step complete! 0.4959986209869385\n",
      "Step complete! 0.48600292205810547\n",
      "Step complete! 0.688007116317749\n",
      "Step complete! 0.7200000286102295\n",
      "Step complete! 0.7050004005432129\n",
      "Step complete! 0.5700006484985352\n",
      "Step complete! 0.5270018577575684\n",
      "Step complete! 0.7279958724975586\n",
      "Step complete! 0.5590007305145264\n",
      "Step complete! 0.5329983234405518\n",
      "Step complete! 0.7059986591339111\n",
      "Step complete! 0.5470011234283447\n",
      "Step complete! 0.6900110244750977\n",
      "Step complete! 0.5020017623901367\n",
      "Step complete! 0.6899991035461426\n",
      "Step complete! 0.5129940509796143\n",
      "Step complete! 0.6840581893920898\n",
      "Step complete! 0.507002592086792\n",
      "Step complete! 0.7065107822418213\n",
      "Step complete! 0.4740025997161865\n",
      "Step complete! 0.5310006141662598\n",
      "Step complete! 0.5299990177154541\n",
      "Step complete! 0.7359988689422607\n",
      "Step complete! 0.5029964447021484\n",
      "Step complete! 0.7439990043640137\n",
      "Step complete! 0.5440006256103516\n",
      "Step complete! 0.6940009593963623\n",
      "Step complete! 0.5025134086608887\n",
      "Step complete! 0.5020008087158203\n",
      "Step complete! 0.6690027713775635\n",
      "Step complete! 0.48999738693237305\n",
      "Step complete! 0.6949999332427979\n",
      "Step complete! 0.684999942779541\n",
      "Step complete! 0.48199987411499023\n",
      "Step complete! 0.49899816513061523\n",
      "Step complete! 0.4915163516998291\n",
      "Step complete! 0.534015417098999\n",
      "Step complete! 0.5269815921783447\n",
      "Step complete! 0.5359959602355957\n",
      "Step complete! 0.5090005397796631\n",
      "Step complete! 0.5500082969665527\n",
      "Step complete! 0.526991605758667\n",
      "Step complete! 0.7540011405944824\n",
      "Step complete! 0.4855201244354248\n",
      "Step complete! 0.6995396614074707\n",
      "Step complete! 0.5119996070861816\n",
      "Step complete! 0.4887669086456299\n",
      "Step complete! 0.4980020523071289\n",
      "Step complete! 0.4889988899230957\n",
      "Step complete! 0.6669988632202148\n",
      "Step complete! 0.6959996223449707\n",
      "Step complete! 0.7140035629272461\n",
      "Step complete! 0.7209956645965576\n",
      "Step complete! 0.7259995937347412\n",
      "Step complete! 0.7229998111724854\n",
      "Step complete! 0.5149972438812256\n",
      "Step complete! 0.7500026226043701\n",
      "Step complete! 0.7070000171661377\n",
      "Step complete! 0.7199993133544922\n",
      "Step complete! 0.7270016670227051\n",
      "Step complete! 0.5019969940185547\n",
      "Step complete! 0.4790017604827881\n",
      "Step complete! 0.4910004138946533\n",
      "Step complete! 0.706000804901123\n",
      "Step complete! 0.7450137138366699\n",
      "Step complete! 0.7069849967956543\n",
      "Step complete! 0.7360014915466309\n",
      "Step complete! 0.531998872756958\n",
      "Step complete! 0.7163283824920654\n",
      "Step complete! 0.5230016708374023\n",
      "Step complete! 0.6890015602111816\n",
      "Step complete! 0.7070021629333496\n",
      "Step complete! 0.4910004138946533\n",
      "Step complete! 0.5320000648498535\n",
      "Step complete! 0.5219995975494385\n",
      "Step complete! 0.4605214595794678\n",
      "Step complete! 0.743999719619751\n",
      "Step complete! 0.5049998760223389\n",
      "Step complete! 0.6770029067993164\n",
      "Step complete! 0.7369961738586426\n",
      "Step complete! 0.5430011749267578\n",
      "Step complete! 0.5330002307891846\n",
      "Step complete! 0.5169970989227295\n",
      "Step complete! 0.745513916015625\n",
      "Step complete! 0.7110040187835693\n",
      "Step complete! 0.48500895500183105\n",
      "Step complete! 0.46999216079711914\n",
      "Step complete! 0.48000168800354004\n",
      "Step complete! 0.48799848556518555\n",
      "Step complete! 0.7199902534484863\n",
      "Step complete! 0.698540210723877\n",
      "Step complete! 0.7039928436279297\n",
      "Step complete! 0.5090005397796631\n",
      "Step complete! 0.556002140045166\n",
      "Step complete! 0.5220000743865967\n",
      "Step complete! 0.7219974994659424\n",
      "Step complete! 0.5140011310577393\n",
      "Step complete! 0.5149979591369629\n",
      "Step complete! 0.544996976852417\n",
      "Step complete! 0.7085225582122803\n",
      "Step complete! 0.49699902534484863\n",
      "Step complete! 0.5109837055206299\n",
      "Step complete! 0.49199819564819336\n",
      "Step complete! 0.6840007305145264\n",
      "Step complete! 0.6709997653961182\n",
      "Step complete! 0.6990046501159668\n",
      "Step complete! 0.5300157070159912\n",
      "Step complete! 0.4971339702606201\n",
      "Step complete! 0.7370035648345947\n",
      "Step complete! 0.7255187034606934\n",
      "Step complete! 0.5390021800994873\n",
      "Step complete! 0.5449991226196289\n",
      "Step complete! 0.5270094871520996\n",
      "Step complete! 0.5290040969848633\n",
      "Step complete! 0.5089976787567139\n",
      "Step complete! 0.5040042400360107\n",
      "Step complete! 0.49100661277770996\n",
      "Step complete! 0.6769895553588867\n",
      "Step complete! 0.727532148361206\n",
      "Step complete! 0.49499940872192383\n",
      "Step complete! 0.49700069427490234\n",
      "Step complete! 0.5649921894073486\n",
      "Step complete! 0.7750017642974854\n",
      "Step complete! 1.009000539779663\n",
      "Step complete! 0.8955166339874268\n",
      "Step complete! 0.7629997730255127\n",
      "Step complete! 0.7789998054504395\n",
      "Step complete! 0.7049989700317383\n",
      "Step complete! 0.6879990100860596\n",
      "Step complete! 0.7120099067687988\n",
      "Step complete! 0.451000452041626\n",
      "Step complete! 0.6875202655792236\n",
      "Step complete! 0.48799729347229004\n",
      "Step complete! 0.6869990825653076\n",
      "Step complete! 0.7040069103240967\n",
      "Step complete! 0.7019984722137451\n",
      "Step complete! 0.5110025405883789\n",
      "Step complete! 0.49852442741394043\n",
      "Step complete! 0.5210011005401611\n",
      "Step complete! 0.5220012664794922\n",
      "Step complete! 0.5060045719146729\n",
      "Step complete! 0.5279996395111084\n",
      "Step complete! 0.7159972190856934\n",
      "Step complete! 0.5639991760253906\n",
      "Step complete! 0.6759991645812988\n",
      "Step complete! 0.4699985980987549\n",
      "Step complete! 0.7120013236999512\n",
      "Step complete! 0.4660036563873291\n",
      "Step complete! 0.6775181293487549\n",
      "Step complete! 0.45800328254699707\n",
      "Step complete! 0.4920003414154053\n",
      "Step complete! 0.6669967174530029\n",
      "Step complete! 0.6060035228729248\n",
      "Step complete! 0.4959990978240967\n",
      "Step complete! 0.7230002880096436\n",
      "Step complete! 0.5185155868530273\n",
      "Step complete! 0.7410016059875488\n",
      "Step complete! 0.5129995346069336\n",
      "Step complete! 0.486985445022583\n",
      "Step complete! 0.49100208282470703\n",
      "Step complete! 0.5070009231567383\n",
      "Step complete! 0.5069987773895264\n",
      "Step complete! 0.4843151569366455\n",
      "Step complete! 0.6689972877502441\n",
      "Step complete! 0.7129974365234375\n",
      "Step complete! 0.4969954490661621\n",
      "Step complete! 0.4790003299713135\n",
      "Step complete! 0.4940018653869629\n",
      "Step complete! 0.5349996089935303\n",
      "Step complete! 0.7655203342437744\n",
      "Step complete! 0.5490024089813232\n",
      "Step complete! 0.5360181331634521\n",
      "Step complete! 0.7270042896270752\n",
      "Step complete! 0.6650905609130859\n",
      "Step complete! 0.48726868629455566\n",
      "Step complete! 0.505002498626709\n",
      "Step complete! 0.46899867057800293\n",
      "Step complete! 0.6940023899078369\n",
      "Step complete! 0.7223958969116211\n",
      "Step complete! 0.5670006275177002\n",
      "Step complete! 0.6960008144378662\n",
      "Step complete! 0.4769914150238037\n",
      "Step complete! 0.7189984321594238\n",
      "Step complete! 0.5099961757659912\n",
      "Step complete! 0.7359998226165771\n",
      "Step complete! 0.49900293350219727\n",
      "Step complete! 0.5059983730316162\n",
      "Step complete! 0.528001070022583\n",
      "Step complete! 0.5159966945648193\n",
      "Step complete! 0.5040032863616943\n",
      "Step complete! 0.4909951686859131\n",
      "Step complete! 0.6970019340515137\n",
      "Step complete! 0.4740025997161865\n",
      "Step complete! 0.48799610137939453\n",
      "Step complete! 0.49199748039245605\n",
      "Step complete! 0.4760119915008545\n",
      "Step complete! 0.7279996871948242\n",
      "Step complete! 0.7030000686645508\n",
      "Step complete! 0.546001672744751\n",
      "Step complete! 0.6470003128051758\n",
      "Step complete! 0.5199997425079346\n",
      "Step complete! 0.7130002975463867\n",
      "Step complete! 0.5080046653747559\n",
      "Step complete! 0.7160015106201172\n",
      "Step complete! 0.7065131664276123\n",
      "Step complete! 0.4660005569458008\n",
      "Step complete! 0.48799920082092285\n",
      "Step complete! 0.711000919342041\n",
      "Step complete! 0.5790019035339355\n",
      "Step complete! 0.5359985828399658\n",
      "Step complete! 0.5960023403167725\n",
      "Step complete! 0.7689938545227051\n",
      "Step complete! 0.6130087375640869\n",
      "Step complete! 0.8315215110778809\n",
      "Step complete! 0.6069982051849365\n",
      "Step complete! 0.8300037384033203\n",
      "Step complete! 0.7989981174468994\n",
      "Step complete! 0.7299947738647461\n",
      "Step complete! 0.7789990901947021\n",
      "Step complete! 0.49954748153686523\n",
      "Step complete! 0.6919970512390137\n",
      "Step complete! 1.2900042533874512\n",
      "Step complete! 0.9930038452148438\n",
      "Step complete! 0.7089967727661133\n",
      "Step complete! 0.4759986400604248\n",
      "Step complete! 0.7205140590667725\n",
      "Step complete! 0.7707562446594238\n",
      "Step complete! 0.7479960918426514\n",
      "Step complete! 0.7239990234375\n",
      "Step complete! 0.5439982414245605\n",
      "Step complete! 0.7250075340270996\n",
      "Step complete! 0.6916160583496094\n",
      "Step complete! 0.7070024013519287\n",
      "Step complete! 0.48899269104003906\n",
      "Step complete! 0.6799993515014648\n",
      "Step complete! 0.49799656867980957\n",
      "Step complete! 0.4699983596801758\n",
      "Step complete! 0.501000165939331\n",
      "Step complete! 0.4849996566772461\n",
      "Step complete! 0.7410013675689697\n",
      "Step complete! 0.49899792671203613\n",
      "Step complete! 0.5800025463104248\n",
      "Step complete! 0.5339999198913574\n",
      "Step complete! 0.7459976673126221\n",
      "Step complete! 0.5030019283294678\n",
      "Step complete! 0.4909992218017578\n",
      "Step complete! 0.6917579174041748\n",
      "Step complete! 0.7250034809112549\n",
      "Step complete! 0.5179958343505859\n",
      "Step complete! 0.5020005702972412\n",
      "Step complete! 0.6839971542358398\n",
      "Step complete! 0.49865221977233887\n",
      "Step complete! 0.4850029945373535\n",
      "Step complete! 0.725999116897583\n",
      "Step complete! 0.5219988822937012\n",
      "Step complete! 0.5470020771026611\n",
      "Step complete! 0.7189998626708984\n",
      "Step complete! 0.5289976596832275\n",
      "Step complete! 0.747004508972168\n",
      "Step complete! 0.6850004196166992\n",
      "Step complete! 0.6850001811981201\n",
      "Step complete! 0.4950125217437744\n",
      "Step complete! 0.5159924030303955\n",
      "Step complete! 0.7000505924224854\n",
      "Step complete! 0.6949996948242188\n",
      "Step complete! 0.47300124168395996\n",
      "Step complete! 0.48200011253356934\n",
      "Step complete! 0.49901556968688965\n",
      "Step complete! 0.5250000953674316\n",
      "Step complete! 0.559002161026001\n",
      "Step complete! 0.515995979309082\n",
      "Step complete! 0.5150024890899658\n",
      "Step complete! 0.7459981441497803\n",
      "Step complete! 0.7269992828369141\n",
      "Step complete! 0.5350015163421631\n",
      "Step complete! 0.7055263519287109\n",
      "Step complete! 0.482999324798584\n",
      "Step complete! 0.5610005855560303\n",
      "Step complete! 0.7040045261383057\n",
      "Step complete! 0.5279994010925293\n",
      "Step complete! 0.5195281505584717\n",
      "Step complete! 0.7120010852813721\n",
      "Step complete! 0.6950156688690186\n",
      "Step complete! 0.7379856109619141\n",
      "Step complete! 0.5260009765625\n",
      "Step complete! 0.7449972629547119\n",
      "Step complete! 0.7229993343353271\n",
      "Step complete! 0.7420039176940918\n",
      "Step complete! 0.5379977226257324\n",
      "Step complete! 0.5070035457611084\n",
      "Step complete! 0.6904387474060059\n",
      "Step complete! 0.6884856224060059\n",
      "Step complete! 0.6886153221130371\n",
      "Step complete! 0.6820034980773926\n",
      "Step complete! 0.6980018615722656\n",
      "Step complete! 0.4709968566894531\n",
      "Step complete! 0.6699986457824707\n",
      "Step complete! 0.703998327255249\n",
      "Step complete! 0.5260024070739746\n",
      "Step complete! 0.7309994697570801\n",
      "Step complete! 0.5210006237030029\n",
      "Step complete! 0.7239999771118164\n",
      "Step complete! 0.756005048751831\n",
      "Step complete! 0.7449932098388672\n",
      "Step complete! 0.6979947090148926\n",
      "Step complete! 0.49100661277770996\n",
      "Step complete! 0.47299718856811523\n",
      "Step complete! 0.6946640014648438\n",
      "Step complete! 0.4850032329559326\n",
      "Step complete! 0.7359914779663086\n",
      "Step complete! 0.6920046806335449\n",
      "Step complete! 0.6879956722259521\n",
      "Step complete! 0.7269983291625977\n",
      "Step complete! 0.7139983177185059\n",
      "Step complete! 0.6929984092712402\n",
      "Step complete! 0.516000509262085\n",
      "Step complete! 0.530998706817627\n",
      "Step complete! 0.5070004463195801\n",
      "Step complete! 0.6669983863830566\n",
      "Step complete! 0.46999621391296387\n",
      "Step complete! 0.6850011348724365\n",
      "Step complete! 0.46900081634521484\n",
      "Step complete! 0.706367015838623\n",
      "Step complete! 0.4649994373321533\n",
      "Step complete! 0.4799988269805908\n",
      "Step complete! 0.49000978469848633\n",
      "Step complete! 0.570995569229126\n",
      "Step complete! 0.5820026397705078\n",
      "Step complete! 0.5539984703063965\n",
      "Step complete! 0.49700260162353516\n",
      "Step complete! 0.7505135536193848\n",
      "Step complete! 0.5140023231506348\n",
      "Step complete! 0.7269964218139648\n",
      "Step complete! 0.49899935722351074\n",
      "Step complete! 0.4850034713745117\n",
      "Step complete! 0.664013147354126\n",
      "Step complete! 0.4849977493286133\n",
      "Step complete! 0.479006290435791\n",
      "Step complete! 0.4895188808441162\n",
      "Step complete! 0.6833846569061279\n",
      "Step complete! 0.48799920082092285\n",
      "Step complete! 0.47798800468444824\n",
      "Step complete! 0.7235963344573975\n",
      "Step complete! 0.5359995365142822\n",
      "Step complete! 0.5750091075897217\n",
      "Step complete! 0.5240001678466797\n",
      "Step complete! 0.509007453918457\n",
      "Step complete! 0.5290007591247559\n",
      "Step complete! 0.5270006656646729\n",
      "Step complete! 0.5200014114379883\n",
      "Step complete! 0.6669712066650391\n",
      "Step complete! 0.4810011386871338\n",
      "Step complete! 0.5039975643157959\n",
      "Step complete! 0.4930083751678467\n",
      "Step complete! 0.4759964942932129\n",
      "Step complete! 0.6840016841888428\n",
      "Step complete! 0.4900023937225342\n",
      "Step complete! 0.7099981307983398\n",
      "Step complete! 0.501002311706543\n",
      "Step complete! 0.7519989013671875\n",
      "Step complete! 0.5399904251098633\n",
      "Step complete! 0.5469987392425537\n",
      "Step complete! 0.5635290145874023\n",
      "Step complete! 0.5359888076782227\n",
      "Step complete! 0.7270145416259766\n",
      "Step complete! 0.7031021118164062\n",
      "Step complete! 0.6919810771942139\n",
      "Step complete! 0.6652112007141113\n",
      "Step complete! 0.47499895095825195\n",
      "Step complete! 0.7029922008514404\n",
      "Step complete! 0.7030129432678223\n",
      "Step complete! 0.6674211025238037\n",
      "Step complete! 0.7320492267608643\n",
      "Step complete! 0.7869899272918701\n",
      "Step complete! 0.7209954261779785\n",
      "Step complete! 0.754997730255127\n",
      "Step complete! 0.7219979763031006\n",
      "Step complete! 0.6440002918243408\n",
      "Step complete! 0.48551416397094727\n",
      "Step complete! 0.49200010299682617\n",
      "Step complete! 0.7086567878723145\n",
      "Step complete! 0.6819977760314941\n",
      "Step complete! 0.7060039043426514\n",
      "Step complete! 0.4750049114227295\n",
      "Step complete! 0.5089945793151855\n",
      "Step complete! 0.66300368309021\n",
      "Step complete! 0.7400023937225342\n",
      "Step complete! 0.7579936981201172\n",
      "Step complete! 0.7000043392181396\n",
      "Step complete! 0.7399983406066895\n",
      "Step complete! 0.5060014724731445\n",
      "Step complete! 0.5059919357299805\n",
      "Step complete! 0.5100030899047852\n",
      "Step complete! 0.7229974269866943\n",
      "Step complete! 0.5279979705810547\n",
      "Step complete! 0.7000048160552979\n",
      "Step complete! 0.6899983882904053\n",
      "Step complete! 0.4849991798400879\n",
      "Step complete! 0.7142226696014404\n",
      "Step complete! 0.712996244430542\n",
      "Step complete! 0.7610006332397461\n",
      "Step complete! 0.7470004558563232\n",
      "Step complete! 0.7989978790283203\n",
      "Step complete! 0.6149988174438477\n",
      "Step complete! 0.598996639251709\n",
      "Step complete! 0.5630018711090088\n",
      "Step complete! 0.5479955673217773\n",
      "Step complete! 0.6735684871673584\n",
      "Step complete! 0.7160000801086426\n",
      "Step complete! 0.7460029125213623\n",
      "Step complete! 0.7390012741088867\n",
      "Step complete! 0.49399542808532715\n",
      "Step complete! 0.49299120903015137\n",
      "Step complete! 0.7820005416870117\n",
      "Step complete! 0.7480087280273438\n",
      "Step complete! 0.5110006332397461\n",
      "Step complete! 0.7419993877410889\n",
      "Step complete! 0.5450036525726318\n",
      "Step complete! 0.5519974231719971\n",
      "Step complete! 0.7069988250732422\n",
      "Step complete! 0.49900007247924805\n",
      "Step complete! 0.4920041561126709\n",
      "Step complete! 0.5145175457000732\n",
      "Step complete! 0.49700164794921875\n",
      "Step complete! 0.5179972648620605\n",
      "Step complete! 0.6739976406097412\n",
      "Step complete! 0.5360007286071777\n",
      "Step complete! 0.4700024127960205\n",
      "Step complete! 0.7410025596618652\n",
      "Step complete! 0.5360007286071777\n",
      "Step complete! 0.5160083770751953\n",
      "Step complete! 0.5349953174591064\n",
      "Step complete! 0.7505114078521729\n",
      "Step complete! 0.538001537322998\n",
      "Step complete! 0.5179986953735352\n",
      "Step complete! 0.7370004653930664\n",
      "Step complete! 0.709000825881958\n",
      "Step complete! 0.5189988613128662\n",
      "Step complete! 0.4920156002044678\n",
      "Step complete! 0.5195200443267822\n",
      "Step complete! 0.7400014400482178\n",
      "Step complete! 0.5069997310638428\n",
      "Step complete! 0.5499827861785889\n",
      "Step complete! 0.8069987297058105\n",
      "Step complete! 0.7410006523132324\n",
      "Step complete! 0.76900315284729\n",
      "Step complete! 0.7805085182189941\n",
      "Step complete! 0.7439975738525391\n",
      "Step complete! 0.7169985771179199\n",
      "Step complete! 0.5220010280609131\n",
      "Step complete! 0.528003454208374\n",
      "Step complete! 0.5199966430664062\n",
      "Step complete! 0.5265204906463623\n",
      "Step complete! 0.7359974384307861\n",
      "Step complete! 0.7240102291107178\n",
      "Step complete! 0.5179975032806396\n",
      "Step complete! 0.5760047435760498\n",
      "Step complete! 0.7429976463317871\n",
      "Step complete! 0.5360002517700195\n",
      "Step complete! 0.760000467300415\n",
      "Step complete! 0.5329973697662354\n",
      "Step complete! 0.7375268936157227\n",
      "Step complete! 0.5109982490539551\n",
      "Step complete! 0.5140047073364258\n",
      "Step complete! 0.4989957809448242\n",
      "Step complete! 0.48800039291381836\n",
      "Step complete! 0.7190005779266357\n",
      "Step complete! 0.744999885559082\n",
      "Step complete! 0.7189943790435791\n",
      "Step complete! 0.7155101299285889\n",
      "Step complete! 0.7519991397857666\n",
      "Step complete! 0.5450117588043213\n",
      "Step complete! 0.5599963665008545\n",
      "Step complete! 0.5250072479248047\n",
      "Step complete! 0.7290008068084717\n",
      "Step complete! 0.5700080394744873\n",
      "Step complete! 0.48400068283081055\n",
      "Step complete! 0.5775198936462402\n",
      "Step complete! 0.5\n",
      "Step complete! 0.6940085887908936\n",
      "Step complete! 0.5049960613250732\n",
      "Step complete! 0.5080058574676514\n",
      "Step complete! 0.6940035820007324\n",
      "Step complete! 0.6889986991882324\n",
      "Step complete! 0.7740001678466797\n",
      "Step complete! 0.5545175075531006\n",
      "Step complete! 0.765005350112915\n",
      "Step complete! 0.7119956016540527\n",
      "Step complete! 0.5500004291534424\n",
      "Step complete! 0.7480006217956543\n",
      "Step complete! 0.7510077953338623\n",
      "Step complete! 0.7659957408905029\n",
      "Step complete! 0.49654316902160645\n",
      "Step complete! 0.711005687713623\n",
      "Step complete! 0.7269892692565918\n",
      "Step complete! 0.5019993782043457\n",
      "Step complete! 0.5250060558319092\n",
      "Step complete! 0.7079973220825195\n",
      "Step complete! 0.5679981708526611\n",
      "Step complete! 0.56099534034729\n",
      "Step complete! 0.759544849395752\n",
      "Step complete! 0.764000654220581\n",
      "Step complete! 0.5579993724822998\n",
      "Step complete! 0.5399963855743408\n",
      "Step complete! 0.7263808250427246\n",
      "Step complete! 0.7109982967376709\n",
      "Step complete! 0.7259993553161621\n",
      "Step complete! 0.5620055198669434\n",
      "Step complete! 0.5139942169189453\n",
      "Step complete! 0.7410011291503906\n",
      "Step complete! 0.49699974060058594\n",
      "Step complete! 0.5300018787384033\n",
      "Step complete! 0.7419998645782471\n",
      "Step complete! 0.7465109825134277\n",
      "Step complete! 0.5430006980895996\n",
      "Step complete! 0.547997236251831\n",
      "Step complete! 0.5570027828216553\n",
      "Step complete! 0.5669996738433838\n",
      "Step complete! 0.538001298904419\n",
      "Step complete! 0.685997724533081\n",
      "Step complete! 0.4870007038116455\n",
      "Step complete! 0.5215103626251221\n",
      "Step complete! 0.7330029010772705\n",
      "Step complete! 0.5280027389526367\n",
      "Step complete! 0.7249996662139893\n",
      "Step complete! 0.6999998092651367\n",
      "Step complete! 0.5129997730255127\n",
      "Step complete! 0.7535197734832764\n",
      "Step complete! 0.5169951915740967\n",
      "Step complete! 0.5610096454620361\n",
      "Step complete! 0.5439929962158203\n",
      "Step complete! 0.5880029201507568\n",
      "Step complete! 0.7859940528869629\n",
      "Step complete! 0.5620014667510986\n",
      "Step complete! 0.5540080070495605\n",
      "Step complete! 0.7019999027252197\n",
      "Step complete! 0.5000009536743164\n",
      "Step complete! 0.5279972553253174\n",
      "Step complete! 0.7013447284698486\n",
      "Step complete! 0.7399938106536865\n",
      "Step complete! 0.5240004062652588\n",
      "Step complete! 0.5759949684143066\n",
      "Step complete! 0.7350013256072998\n",
      "Step complete! 0.5520026683807373\n",
      "Step complete! 0.718001127243042\n",
      "Step complete! 0.5599987506866455\n",
      "Step complete! 0.5260124206542969\n",
      "Step complete! 0.5379974842071533\n",
      "Step complete! 0.7074418067932129\n",
      "Step complete! 0.7089982032775879\n",
      "Step complete! 0.5049996376037598\n",
      "Step complete! 0.5189919471740723\n",
      "Step complete! 0.5189998149871826\n",
      "Step complete! 0.5160017013549805\n",
      "Step complete! 0.5260009765625\n",
      "Step complete! 0.5189995765686035\n",
      "Step complete! 0.7579994201660156\n",
      "Step complete! 0.5350024700164795\n",
      "Step complete! 0.7700014114379883\n",
      "Step complete! 0.7349984645843506\n",
      "Step complete! 0.5600018501281738\n",
      "Step complete! 0.5390040874481201\n",
      "Step complete! 0.5159943103790283\n",
      "Step complete! 0.5295119285583496\n",
      "Step complete! 0.7189998626708984\n",
      "Step complete! 0.7370002269744873\n",
      "Step complete! 0.5310013294219971\n",
      "Step complete! 0.7169990539550781\n",
      "Step complete! 0.7279982566833496\n",
      "Step complete! 0.5310020446777344\n",
      "Step complete! 0.7125122547149658\n",
      "Step complete! 0.7380008697509766\n",
      "Step complete! 0.5680077075958252\n",
      "Step complete! 0.7999968528747559\n",
      "Step complete! 0.7139992713928223\n",
      "Step complete! 0.5660028457641602\n",
      "Step complete! 0.5080010890960693\n",
      "Step complete! 0.717522144317627\n",
      "Step complete! 0.4900033473968506\n",
      "Step complete! 0.7119879722595215\n",
      "Step complete! 0.7200005054473877\n",
      "Step complete! 0.7271671295166016\n",
      "Step complete! 0.47300195693969727\n",
      "Step complete! 0.6969869136810303\n",
      "Step complete! 0.7250006198883057\n",
      "Step complete! 0.7259957790374756\n",
      "Step complete! 0.5369973182678223\n",
      "Step complete! 0.574998140335083\n",
      "Step complete! 0.7415242195129395\n",
      "Step complete! 0.7380023002624512\n",
      "Step complete! 0.7107901573181152\n",
      "Step complete! 0.6849966049194336\n",
      "Step complete! 0.6980030536651611\n",
      "Step complete! 0.47699761390686035\n",
      "Step complete! 0.7140011787414551\n",
      "Step complete! 0.4940042495727539\n",
      "Step complete! 0.7166049480438232\n",
      "Step complete! 0.7619919776916504\n",
      "Step complete! 0.5500006675720215\n",
      "Step complete! 0.5239999294281006\n",
      "Step complete! 0.7419989109039307\n",
      "Step complete! 0.7390079498291016\n",
      "Step complete! 0.5299983024597168\n",
      "Step complete! 0.6280038356781006\n",
      "Step complete! 0.7019970417022705\n",
      "Step complete! 0.5339994430541992\n",
      "Step complete! 0.727644681930542\n",
      "Step complete! 0.5170018672943115\n",
      "Step complete! 0.6989974975585938\n",
      "Step complete! 0.5190012454986572\n",
      "Step complete! 0.49500155448913574\n",
      "Step complete! 0.5439975261688232\n",
      "Step complete! 0.7430002689361572\n",
      "Step complete! 0.7390053272247314\n",
      "Step complete! 0.7629997730255127\n",
      "Step complete! 0.7800083160400391\n",
      "Step complete! 0.5699999332427979\n",
      "Step complete! 0.5099985599517822\n",
      "Step complete! 0.699998140335083\n",
      "Step complete! 0.6880011558532715\n",
      "Step complete! 0.7440054416656494\n",
      "Step complete! 0.7049908638000488\n",
      "Step complete! 0.6850001811981201\n",
      "Step complete! 0.6845347881317139\n",
      "Step complete! 0.7370049953460693\n",
      "Step complete! 0.5249991416931152\n",
      "Step complete! 0.716987133026123\n",
      "Step complete! 0.7309999465942383\n",
      "Step complete! 0.5140013694763184\n",
      "Step complete! 0.5380027294158936\n",
      "Step complete! 0.5310008525848389\n",
      "Step complete! 0.7005100250244141\n",
      "Step complete! 0.5099990367889404\n",
      "Step complete! 0.667001485824585\n",
      "Step complete! 0.7219974994659424\n",
      "Step complete! 0.7320077419281006\n",
      "Step complete! 0.5410137176513672\n",
      "Step complete! 0.4899923801422119\n",
      "Step complete! 0.6970634460449219\n",
      "Step complete! 0.5539944171905518\n",
      "Step complete! 0.5335221290588379\n",
      "Step complete! 0.7399983406066895\n",
      "Step complete! 0.7440028190612793\n",
      "Step complete! 0.7499997615814209\n",
      "Step complete! 0.5290005207061768\n",
      "Step complete! 0.5214183330535889\n",
      "Step complete! 0.736994743347168\n",
      "Step complete! 0.5280017852783203\n",
      "Step complete! 0.6980001926422119\n",
      "Step complete! 0.6890006065368652\n",
      "Step complete! 0.711998462677002\n",
      "Step complete! 0.5090029239654541\n",
      "Step complete! 0.5080029964447021\n",
      "Step complete! 0.7545180320739746\n",
      "Step complete! 0.542996883392334\n",
      "Step complete! 0.7509982585906982\n",
      "Step complete! 0.5590119361877441\n",
      "Step complete! 0.5939919948577881\n",
      "Step complete! 0.5440008640289307\n",
      "Step complete! 0.5420079231262207\n",
      "Step complete! 0.7162179946899414\n",
      "Step complete! 0.5029995441436768\n",
      "Step complete! 0.724998950958252\n",
      "Step complete! 0.512000322341919\n",
      "Step complete! 0.5115184783935547\n",
      "Step complete! 0.5099961757659912\n",
      "Step complete! 0.7140166759490967\n",
      "Step complete! 0.5320000648498535\n",
      "Step complete! 0.5560009479522705\n",
      "Step complete! 0.8159997463226318\n",
      "Step complete! 0.7619998455047607\n",
      "Step complete! 0.9065239429473877\n",
      "Step complete! 0.5609960556030273\n",
      "Step complete! 0.7379989624023438\n",
      "Step complete! 0.5429906845092773\n",
      "Step complete! 0.7040011882781982\n",
      "Step complete! 0.7389984130859375\n",
      "Step complete! 0.5190093517303467\n",
      "Step complete! 0.763988733291626\n",
      "Step complete! 0.9135217666625977\n",
      "Step complete! 0.5639927387237549\n",
      "Step complete! 0.7769956588745117\n",
      "Step complete! 0.7810006141662598\n",
      "Step complete! 0.5550062656402588\n",
      "Step complete! 0.6170012950897217\n",
      "Step complete! 0.7645342350006104\n",
      "Step complete! 0.567000150680542\n",
      "Step complete! 0.7029910087585449\n",
      "Step complete! 0.5389871597290039\n",
      "Step complete! 0.7269957065582275\n",
      "Step complete! 0.7770099639892578\n",
      "Step complete! 0.7619915008544922\n",
      "Step complete! 0.7609963417053223\n",
      "Step complete! 0.5635361671447754\n",
      "Step complete! 0.5669980049133301\n",
      "Step complete! 0.78299880027771\n",
      "Step complete! 0.5640048980712891\n",
      "Step complete! 0.7440030574798584\n",
      "Step complete! 0.5680007934570312\n",
      "Step complete! 0.6115288734436035\n",
      "Step complete! 0.5620007514953613\n",
      "Step complete! 0.7499997615814209\n",
      "Step complete! 0.5290000438690186\n",
      "Step complete! 0.5070018768310547\n",
      "Step complete! 0.5509951114654541\n",
      "Step complete! 0.5320053100585938\n",
      "Step complete! 0.6200010776519775\n",
      "Step complete! 0.5250124931335449\n",
      "Step complete! 0.8165040016174316\n",
      "Step complete! 0.5890059471130371\n",
      "Step complete! 0.7850167751312256\n",
      "Step complete! 0.5750048160552979\n",
      "Step complete! 0.6079981327056885\n",
      "Step complete! 0.5840003490447998\n",
      "Step complete! 0.5499958992004395\n",
      "Step complete! 0.7805166244506836\n",
      "Step complete! 0.5699982643127441\n",
      "Step complete! 0.5499982833862305\n",
      "Step complete! 0.7670040130615234\n",
      "Step complete! 0.5329971313476562\n",
      "Step complete! 0.7639956474304199\n",
      "Step complete! 0.5550022125244141\n",
      "Step complete! 0.7659957408905029\n",
      "Step complete! 0.7685203552246094\n",
      "Step complete! 0.78299880027771\n",
      "Step complete! 0.7949984073638916\n",
      "Step complete! 0.7739982604980469\n",
      "Step complete! 0.7720057964324951\n",
      "Step complete! 0.5705158710479736\n",
      "Step complete! 0.5649983882904053\n",
      "Step complete! 0.7700016498565674\n",
      "Step complete! 0.7301700115203857\n",
      "Step complete! 0.7739958763122559\n",
      "Step complete! 0.7130012512207031\n",
      "Step complete! 0.5779976844787598\n",
      "Step complete! 0.5600049495697021\n",
      "Step complete! 0.5839872360229492\n",
      "Step complete! 0.806999921798706\n",
      "Step complete! 0.777001142501831\n",
      "Step complete! 0.5675194263458252\n",
      "Step complete! 0.5579979419708252\n",
      "Step complete! 0.5259909629821777\n",
      "Step complete! 0.5279977321624756\n",
      "Step complete! 0.5390007495880127\n",
      "Step complete! 0.7549872398376465\n",
      "Step complete! 0.5665180683135986\n",
      "Step complete! 0.6370029449462891\n",
      "Step complete! 0.7489993572235107\n",
      "Step complete! 0.5260179042816162\n",
      "Step complete! 0.5580015182495117\n",
      "Step complete! 0.564000129699707\n",
      "Step complete! 0.5850043296813965\n",
      "Step complete! 0.594010591506958\n",
      "Step complete! 0.8020000457763672\n",
      "Step complete! 0.603518009185791\n",
      "Step complete! 0.7389957904815674\n",
      "Step complete! 0.6009984016418457\n",
      "Step complete! 0.7450037002563477\n",
      "Step complete! 0.5260064601898193\n",
      "Step complete! 0.5260002613067627\n",
      "Step complete! 0.738999605178833\n",
      "Step complete! 0.8195037841796875\n",
      "Step complete! 0.6239995956420898\n",
      "Step complete! 0.5939993858337402\n",
      "Step complete! 0.7710011005401611\n",
      "Step complete! 0.6010074615478516\n",
      "Step complete! 0.6140007972717285\n",
      "Step complete! 0.5949990749359131\n",
      "Step complete! 0.5775249004364014\n",
      "Step complete! 0.7659971714019775\n",
      "Step complete! 0.7639930248260498\n",
      "Step complete! 0.765000581741333\n",
      "Step complete! 0.7559990882873535\n",
      "Step complete! 0.7379958629608154\n",
      "Step complete! 0.7335355281829834\n",
      "Step complete! 0.777000904083252\n",
      "Step complete! 0.8009915351867676\n",
      "Step complete! 0.6069960594177246\n",
      "Step complete! 0.7509973049163818\n",
      "Step complete! 0.80999755859375\n",
      "Step complete! 0.5750007629394531\n",
      "Step complete! 0.5319969654083252\n",
      "Step complete! 0.7475202083587646\n",
      "Step complete! 0.5349986553192139\n",
      "Step complete! 0.7360007762908936\n",
      "Step complete! 0.568000316619873\n",
      "Step complete! 0.7589988708496094\n",
      "Step complete! 0.7489979267120361\n",
      "Step complete! 0.5670003890991211\n",
      "Step complete! 0.5989978313446045\n",
      "Step complete! 0.5635266304016113\n",
      "Step complete! 0.5609958171844482\n",
      "Step complete! 0.7970008850097656\n",
      "Step complete! 0.7980072498321533\n",
      "Step complete! 0.5599992275238037\n",
      "Step complete! 0.7470197677612305\n",
      "Step complete! 0.5410013198852539\n",
      "Step complete! 0.7345199584960938\n",
      "Step complete! 0.5159969329833984\n",
      "Step complete! 0.5760042667388916\n",
      "Step complete! 0.7240071296691895\n",
      "Step complete! 0.7169976234436035\n",
      "Step complete! 0.7744483947753906\n",
      "Step complete! 0.5819950103759766\n",
      "Step complete! 0.793997049331665\n",
      "Step complete! 0.7499842643737793\n",
      "Step complete! 0.5610020160675049\n",
      "Step complete! 0.7920002937316895\n",
      "Step complete! 0.5219974517822266\n",
      "Step complete! 0.7385103702545166\n",
      "Step complete! 0.7140026092529297\n",
      "Step complete! 0.7340044975280762\n",
      "Step complete! 0.5088503360748291\n",
      "Step complete! 0.7269947528839111\n",
      "Step complete! 0.5320003032684326\n",
      "Step complete! 0.4945087432861328\n",
      "Step complete! 0.5519993305206299\n",
      "Step complete! 0.7669987678527832\n",
      "Step complete! 0.7749996185302734\n",
      "Step complete! 0.5639991760253906\n",
      "Step complete! 0.7770013809204102\n",
      "Step complete! 0.7409992218017578\n",
      "Step complete! 0.7245199680328369\n",
      "Step complete! 0.7210009098052979\n",
      "Step complete! 0.7259995937347412\n",
      "Step complete! 0.5320007801055908\n",
      "Step complete! 0.5009996891021729\n",
      "Step complete! 0.7105157375335693\n",
      "Step complete! 0.49199962615966797\n",
      "Step complete! 0.530001163482666\n",
      "Step complete! 0.5339992046356201\n",
      "Step complete! 0.7410204410552979\n",
      "Step complete! 0.525998592376709\n",
      "Step complete! 0.7459976673126221\n",
      "Step complete! 0.7630000114440918\n",
      "Step complete! 0.7100012302398682\n",
      "Step complete! 0.5230004787445068\n",
      "Step complete! 0.5125143527984619\n",
      "Step complete! 0.5180020332336426\n",
      "Step complete! 0.6929988861083984\n",
      "Step complete! 0.731013298034668\n",
      "Step complete! 0.7130002975463867\n",
      "Step complete! 0.7200069427490234\n",
      "Step complete! 0.8669955730438232\n",
      "Step complete! 0.7950003147125244\n",
      "Step complete! 0.5529990196228027\n",
      "Step complete! 0.7439980506896973\n",
      "Step complete! 0.510014533996582\n",
      "Step complete! 0.5779969692230225\n",
      "Step complete! 0.7000112533569336\n",
      "Step complete! 0.5369932651519775\n",
      "Step complete! 0.7029969692230225\n",
      "Step complete! 0.5190019607543945\n",
      "Step complete! 0.5159995555877686\n",
      "Step complete! 0.7109992504119873\n",
      "Step complete! 0.7429959774017334\n",
      "Step complete! 0.7169971466064453\n",
      "Step complete! 0.7520010471343994\n",
      "Step complete! 0.7545173168182373\n",
      "Step complete! 0.7360048294067383\n",
      "Step complete! 0.5619907379150391\n",
      "Step complete! 0.7390017509460449\n",
      "Step complete! 0.5339975357055664\n",
      "Step complete! 0.5200028419494629\n",
      "Step complete! 0.7030007839202881\n",
      "Step complete! 0.5045132637023926\n",
      "Step complete! 0.72499680519104\n",
      "Step complete! 0.5080053806304932\n",
      "Step complete! 0.7111079692840576\n",
      "Step complete! 0.7290043830871582\n",
      "Step complete! 0.6030054092407227\n",
      "Step complete! 0.5960025787353516\n",
      "Step complete! 0.7930090427398682\n",
      "Step complete! 0.6249990463256836\n",
      "Step complete! 0.8415234088897705\n",
      "Step complete! 0.8370192050933838\n",
      "Step complete! 0.7579777240753174\n",
      "Step complete! 0.8220038414001465\n",
      "Step complete! 0.6030001640319824\n",
      "Step complete! 0.564000129699707\n",
      "Step complete! 0.5649890899658203\n",
      "Step complete! 0.5375161170959473\n",
      "Step complete! 0.5159993171691895\n",
      "Step complete! 0.7560005187988281\n",
      "Step complete! 0.5219888687133789\n",
      "Step complete! 0.7809975147247314\n",
      "Step complete! 0.7750058174133301\n",
      "Step complete! 0.7729969024658203\n",
      "Step complete! 0.7543299198150635\n",
      "Step complete! 0.6960000991821289\n",
      "Step complete! 0.7189972400665283\n",
      "Step complete! 0.7185192108154297\n",
      "Step complete! 0.5070350170135498\n",
      "Step complete! 0.5209987163543701\n",
      "Step complete! 0.6920044422149658\n",
      "Step complete! 0.7229976654052734\n",
      "Step complete! 0.517991304397583\n",
      "Step complete! 0.5709993839263916\n",
      "Step complete! 0.5400018692016602\n",
      "Step complete! 0.820000171661377\n",
      "Step complete! 0.7519960403442383\n",
      "Step complete! 0.7665150165557861\n",
      "Step complete! 0.5640020370483398\n",
      "Step complete! 0.6899986267089844\n",
      "Step complete! 0.5040051937103271\n",
      "Step complete! 0.5009911060333252\n",
      "Step complete! 0.7219994068145752\n",
      "Step complete! 0.7160000801086426\n",
      "Step complete! 0.54653000831604\n",
      "Step complete! 0.7229948043823242\n",
      "Step complete! 0.5319812297821045\n",
      "Step complete! 0.5460021495819092\n",
      "Step complete! 0.7300035953521729\n",
      "Step complete! 0.7350001335144043\n",
      "Step complete! 0.5345206260681152\n",
      "Step complete! 0.5399973392486572\n",
      "Step complete! 0.7969975471496582\n",
      "Step complete! 0.5600054264068604\n",
      "Step complete! 0.7129969596862793\n",
      "Step complete! 0.5150039196014404\n",
      "Step complete! 0.5130007266998291\n",
      "Step complete! 0.514521598815918\n",
      "Step complete! 0.5909984111785889\n",
      "Step complete! 0.5069987773895264\n",
      "Step complete! 0.7349989414215088\n",
      "Step complete! 0.5840044021606445\n",
      "Step complete! 0.5279982089996338\n",
      "Step complete! 0.8040013313293457\n",
      "Step complete! 0.5730016231536865\n",
      "Step complete! 0.5290017127990723\n",
      "Step complete! 0.5390005111694336\n",
      "Step complete! 0.7330021858215332\n",
      "Step complete! 0.7362301349639893\n",
      "Step complete! 0.51999831199646\n",
      "Step complete! 0.4810032844543457\n",
      "Step complete! 0.6729962825775146\n",
      "Step complete! 0.5289993286132812\n",
      "Step complete! 0.5160019397735596\n",
      "Step complete! 0.5220005512237549\n",
      "Step complete! 0.5020081996917725\n",
      "Step complete! 0.5299930572509766\n",
      "Step complete! 0.7670004367828369\n",
      "Step complete! 0.7979986667633057\n",
      "Step complete! 0.7700221538543701\n",
      "Step complete! 0.5680058002471924\n",
      "Step complete! 0.558997631072998\n",
      "Step complete! 0.5460028648376465\n",
      "Step complete! 0.5209994316101074\n",
      "Step complete! 0.7289981842041016\n",
      "Step complete! 0.7310054302215576\n",
      "Step complete! 0.5815155506134033\n",
      "Step complete! 0.727001428604126\n",
      "Step complete! 0.7170000076293945\n",
      "Step complete! 0.48399877548217773\n",
      "Step complete! 0.5009980201721191\n",
      "Step complete! 0.5250015258789062\n",
      "Step complete! 0.5320005416870117\n",
      "Step complete! 0.5515167713165283\n",
      "Step complete! 0.7269971370697021\n",
      "Step complete! 0.5480062961578369\n",
      "Step complete! 0.5599977970123291\n",
      "Step complete! 0.5270121097564697\n",
      "Step complete! 0.5160017013549805\n",
      "Step complete! 0.5240044593811035\n",
      "Step complete! 0.7049949169158936\n",
      "Step complete! 0.6980011463165283\n",
      "Step complete! 0.5015182495117188\n",
      "Step complete! 0.7909998893737793\n",
      "Step complete! 0.7009997367858887\n",
      "Step complete! 0.5080006122589111\n",
      "Step complete! 0.5649995803833008\n",
      "Step complete! 0.7350013256072998\n",
      "Step complete! 0.7499992847442627\n",
      "Step complete! 0.5545272827148438\n",
      "Step complete! 0.7359917163848877\n",
      "Step complete! 0.7160007953643799\n",
      "Step complete! 0.723996639251709\n",
      "Step complete! 0.7390005588531494\n",
      "Step complete! 0.5280025005340576\n",
      "Step complete! 0.517998218536377\n",
      "Step complete! 0.7119300365447998\n",
      "Step complete! 0.6960010528564453\n",
      "Step complete! 0.4979984760284424\n",
      "Step complete! 0.53999924659729\n",
      "Step complete! 0.5510015487670898\n",
      "Step complete! 0.7380030155181885\n",
      "Step complete! 0.7249956130981445\n",
      "Step complete! 0.8449983596801758\n",
      "Step complete! 0.5229997634887695\n",
      "Step complete! 0.7099981307983398\n",
      "Step complete! 0.7170062065124512\n",
      "Step complete! 0.5230062007904053\n",
      "Step complete! 0.5349996089935303\n",
      "Step complete! 0.49599528312683105\n",
      "Step complete! 0.6965270042419434\n",
      "Step complete! 0.49500060081481934\n",
      "Step complete! 0.7079973220825195\n",
      "Step complete! 0.5430023670196533\n",
      "Step complete! 0.5330004692077637\n",
      "Step complete! 0.5789906978607178\n",
      "Step complete! 0.7709982395172119\n",
      "Step complete! 0.5240013599395752\n",
      "Step complete! 0.5455172061920166\n",
      "Step complete! 0.5500121116638184\n",
      "Step complete! 0.48200011253356934\n",
      "Step complete! 0.5110037326812744\n",
      "Step complete! 0.7191259860992432\n",
      "Step complete! 0.5199985504150391\n",
      "Step complete! 0.5029971599578857\n",
      "Step complete! 0.5180010795593262\n",
      "Step complete! 0.7100012302398682\n",
      "Step complete! 0.5200002193450928\n",
      "Step complete! 0.7105221748352051\n",
      "Step complete! 0.7760097980499268\n",
      "Step complete! 0.5459885597229004\n",
      "Step complete! 0.5279989242553711\n",
      "Step complete! 0.7950031757354736\n",
      "Step complete! 0.761998176574707\n",
      "Step complete! 0.6910085678100586\n",
      "Step complete! 0.5300006866455078\n",
      "Step complete! 0.7375228404998779\n",
      "Step complete! 0.5039961338043213\n",
      "Step complete! 0.7210030555725098\n",
      "Step complete! 0.6879973411560059\n",
      "Step complete! 0.6919994354248047\n",
      "Step complete! 0.508007287979126\n",
      "Step complete! 0.5309925079345703\n",
      "Step complete! 0.7468750476837158\n",
      "Step complete! 0.7960009574890137\n",
      "Step complete! 0.7329978942871094\n",
      "Step complete! 0.7529928684234619\n",
      "Step complete! 0.7359976768493652\n",
      "Step complete! 0.5210094451904297\n",
      "Step complete! 0.715003252029419\n",
      "Step complete! 0.5129969120025635\n",
      "Step complete! 0.7000000476837158\n",
      "Step complete! 0.5210113525390625\n",
      "Step complete! 0.5089967250823975\n",
      "Step complete! 0.5100016593933105\n",
      "Step complete! 0.4955172538757324\n",
      "Step complete! 0.5199978351593018\n",
      "Step complete! 0.7370045185089111\n",
      "Step complete! 0.7379982471466064\n",
      "Step complete! 0.5349893569946289\n",
      "Step complete! 0.8220131397247314\n",
      "Step complete! 0.7179923057556152\n",
      "Step complete! 0.707526683807373\n",
      "Step complete! 0.7270004749298096\n",
      "Step complete! 0.4929988384246826\n",
      "Step complete! 0.5020010471343994\n",
      "Step complete! 0.6958048343658447\n",
      "Step complete! 0.5020105838775635\n",
      "Step complete! 0.6979966163635254\n",
      "Step complete! 0.7430024147033691\n",
      "Step complete! 0.7459976673126221\n",
      "Step complete! 0.5490169525146484\n",
      "Step complete! 0.5529985427856445\n",
      "Step complete! 0.7369999885559082\n",
      "Step complete! 0.530001163482666\n",
      "Step complete! 0.5239970684051514\n",
      "Step complete! 0.523998498916626\n",
      "Step complete! 0.5500023365020752\n",
      "Step complete! 0.711000919342041\n",
      "Step complete! 0.5160114765167236\n",
      "Step complete! 0.47700071334838867\n",
      "Step complete! 0.531001091003418\n",
      "Step complete! 0.4870016574859619\n",
      "Step complete! 0.5225131511688232\n",
      "Step complete! 0.7270002365112305\n",
      "Step complete! 0.7190003395080566\n",
      "Step complete! 0.5709998607635498\n",
      "Step complete! 0.5129995346069336\n",
      "Step complete! 0.697007417678833\n",
      "Step complete! 0.7319996356964111\n",
      "Step complete! 0.46700000762939453\n",
      "Step complete! 0.48052000999450684\n",
      "Step complete! 0.676567554473877\n",
      "Step complete! 0.5010051727294922\n",
      "Step complete! 0.7039966583251953\n",
      "Step complete! 0.48799943923950195\n",
      "Step complete! 0.4790005683898926\n",
      "Step complete! 0.7006175518035889\n",
      "Step complete! 0.51800537109375\n",
      "Step complete! 0.521003007888794\n",
      "Step complete! 0.5419971942901611\n",
      "Step complete! 0.7290019989013672\n",
      "Step complete! 0.5299973487854004\n",
      "Step complete! 0.5590128898620605\n",
      "Step complete! 0.5280022621154785\n",
      "Step complete! 0.7409958839416504\n",
      "Step complete! 0.6649994850158691\n",
      "Step complete! 0.7190008163452148\n",
      "Step complete! 0.7420141696929932\n",
      "Step complete! 0.6805496215820312\n",
      "Step complete! 0.7129993438720703\n",
      "Step complete! 0.5019941329956055\n",
      "Step complete! 0.477001428604126\n",
      "Step complete! 0.49899935722351074\n",
      "Step complete! 0.7440028190612793\n",
      "Step complete! 0.5180082321166992\n",
      "Step complete! 0.5309948921203613\n",
      "Step complete! 0.5319974422454834\n",
      "Step complete! 0.7329976558685303\n",
      "Step complete! 0.5130014419555664\n",
      "Step complete! 0.6760010719299316\n",
      "Step complete! 0.469038724899292\n",
      "Step complete! 0.5110034942626953\n",
      "Step complete! 0.48301267623901367\n",
      "Step complete! 0.5359876155853271\n",
      "Step complete! 0.4929969310760498\n",
      "Step complete! 0.6949987411499023\n",
      "Step complete! 0.6935272216796875\n",
      "Step complete! 0.531003475189209\n",
      "Step complete! 0.5300028324127197\n",
      "Step complete! 0.5370001792907715\n",
      "Step complete! 0.7379989624023438\n",
      "Step complete! 0.5610158443450928\n",
      "Step complete! 0.5409994125366211\n",
      "Step complete! 0.7315230369567871\n",
      "Step complete! 0.4829981327056885\n",
      "Step complete! 0.5130012035369873\n",
      "Step complete! 0.49300169944763184\n",
      "Step complete! 0.7089993953704834\n",
      "Step complete! 0.6994166374206543\n",
      "Step complete! 0.6609959602355957\n",
      "Step complete! 0.48900389671325684\n",
      "Step complete! 0.4850013256072998\n",
      "Step complete! 0.7160015106201172\n",
      "Step complete! 0.7639989852905273\n",
      "Step complete! 0.7339992523193359\n",
      "Step complete! 0.7279994487762451\n",
      "Step complete! 0.549999475479126\n",
      "Step complete! 0.7145190238952637\n",
      "Step complete! 0.5660004615783691\n",
      "Step complete! 0.49199867248535156\n",
      "Step complete! 0.48799800872802734\n",
      "Step complete! 0.48000192642211914\n",
      "Step complete! 0.5009684562683105\n",
      "Step complete! 0.4780106544494629\n",
      "Step complete! 0.701988697052002\n",
      "Step complete! 0.4680037498474121\n",
      "Step complete! 0.7035179138183594\n",
      "Step complete! 0.7449994087219238\n",
      "Step complete! 0.5269994735717773\n",
      "Step complete! 0.7440011501312256\n",
      "Step complete! 0.7230010032653809\n",
      "Step complete! 0.5169994831085205\n",
      "Step complete! 0.583998441696167\n",
      "Step complete! 0.5170018672943115\n",
      "Step complete! 0.7195117473602295\n",
      "Step complete! 0.5309410095214844\n",
      "Step complete! 0.7449979782104492\n",
      "Step complete! 0.48000001907348633\n",
      "Step complete! 0.7270021438598633\n",
      "Step complete! 0.48999857902526855\n",
      "Step complete! 0.5020003318786621\n",
      "Step complete! 0.7001709938049316\n",
      "Step complete! 0.5910027027130127\n",
      "Step complete! 0.5310018062591553\n",
      "Step complete! 0.5579953193664551\n",
      "Step complete! 0.5290014743804932\n",
      "Step complete! 0.7330002784729004\n",
      "Step complete! 0.5119938850402832\n",
      "Step complete! 0.5119967460632324\n",
      "Step complete! 0.5070071220397949\n",
      "Step complete! 0.6899986267089844\n",
      "Step complete! 0.49300169944763184\n",
      "Step complete! 0.481001615524292\n",
      "Step complete! 0.6959977149963379\n",
      "Step complete! 0.6970007419586182\n",
      "Step complete! 0.45725083351135254\n",
      "Step complete! 0.7130091190338135\n",
      "Step complete! 0.5150032043457031\n",
      "Step complete! 0.5529963970184326\n",
      "Step complete! 0.7500009536743164\n",
      "Step complete! 0.734997034072876\n",
      "Step complete! 0.5610084533691406\n",
      "Step complete! 0.6355347633361816\n",
      "Step complete! 1.6110024452209473\n",
      "Step complete! 0.5119976997375488\n",
      "Step complete! 0.6860167980194092\n",
      "Step complete! 0.6935441493988037\n",
      "Step complete! 0.7604186534881592\n",
      "Step complete! 0.6710026264190674\n",
      "Step complete! 0.4570016860961914\n",
      "Step complete! 0.7459902763366699\n",
      "Step complete! 0.7259900569915771\n",
      "Step complete! 0.5119988918304443\n",
      "Step complete! 0.7400002479553223\n",
      "Step complete! 0.5070056915283203\n",
      "Step complete! 0.7119936943054199\n",
      "Step complete! 0.5020017623901367\n",
      "Step complete! 0.503000020980835\n",
      "Step complete! 0.7431991100311279\n",
      "Step complete! 0.5040004253387451\n",
      "Step complete! 0.5109992027282715\n",
      "Step complete! 0.707998514175415\n",
      "Step complete! 0.7230079174041748\n",
      "Step complete! 0.7345130443572998\n",
      "Step complete! 0.7619938850402832\n",
      "Step complete! 0.5210011005401611\n",
      "Step complete! 0.5419948101043701\n",
      "Step complete! 0.7569983005523682\n",
      "Step complete! 0.7760114669799805\n",
      "Step complete! 0.5700078010559082\n",
      "Step complete! 0.7015259265899658\n",
      "Step complete! 0.726999044418335\n",
      "Step complete! 0.6878960132598877\n",
      "Step complete! 0.48099684715270996\n",
      "Step complete! 0.4690077304840088\n",
      "Step complete! 0.6760845184326172\n",
      "Step complete! 0.47701096534729004\n",
      "Step complete! 0.68912672996521\n",
      "Step complete! 0.7400000095367432\n",
      "Step complete! 0.7590053081512451\n",
      "Step complete! 0.7219998836517334\n",
      "Step complete! 0.5210001468658447\n",
      "Step complete! 0.692511796951294\n",
      "Step complete! 0.7579996585845947\n",
      "Step complete! 0.6570005416870117\n",
      "Step complete! 0.4849991798400879\n",
      "Step complete! 0.49500083923339844\n",
      "Step complete! 0.7084341049194336\n",
      "Step complete! 0.46556973457336426\n",
      "Step complete! 0.496997594833374\n",
      "Step complete! 0.7059974670410156\n",
      "Step complete! 0.49100494384765625\n",
      "Step complete! 0.4895343780517578\n",
      "Step complete! 0.5209987163543701\n",
      "Step complete! 0.529996395111084\n",
      "Step complete! 0.508002758026123\n",
      "Step complete! 0.5470023155212402\n",
      "Step complete! 0.5449957847595215\n",
      "Step complete! 0.7089986801147461\n",
      "Step complete! 0.5039947032928467\n",
      "Step complete! 0.4940013885498047\n",
      "Step complete! 0.6914801597595215\n",
      "Step complete! 0.5020022392272949\n",
      "Step complete! 0.7109956741333008\n",
      "Step complete! 0.6950013637542725\n",
      "Step complete! 0.49100494384765625\n",
      "Step complete! 0.6769990921020508\n",
      "Step complete! 0.7239985466003418\n",
      "Step complete! 0.5360009670257568\n",
      "Step complete! 0.7540023326873779\n",
      "Step complete! 0.7529985904693604\n",
      "Step complete! 0.7170009613037109\n",
      "Step complete! 0.5299973487854004\n",
      "Step complete! 0.7139990329742432\n",
      "Step complete! 0.6994035243988037\n",
      "Step complete! 0.49699831008911133\n",
      "Step complete! 0.6980018615722656\n",
      "Step complete! 0.48591184616088867\n",
      "Step complete! 0.5069968700408936\n",
      "Step complete! 0.4889957904815674\n",
      "Step complete! 0.7020010948181152\n",
      "Step complete! 0.4699978828430176\n",
      "Step complete! 0.5480036735534668\n",
      "Step complete! 0.5320034027099609\n",
      "Step complete! 0.7730026245117188\n",
      "Step complete! 0.7625308036804199\n",
      "Step complete! 0.5280003547668457\n",
      "Step complete! 0.7180004119873047\n",
      "Step complete! 0.4649999141693115\n",
      "Step complete! 0.7510030269622803\n",
      "Step complete! 0.4889960289001465\n",
      "Step complete! 0.6869845390319824\n",
      "Step complete! 0.48552513122558594\n",
      "Step complete! 0.661003828048706\n",
      "Step complete! 0.7136447429656982\n",
      "Step complete! 0.5010006427764893\n",
      "Step complete! 0.4920017719268799\n",
      "Step complete! 0.5499997138977051\n",
      "Step complete! 0.7239947319030762\n",
      "Step complete! 0.7159988880157471\n",
      "Step complete! 0.5360004901885986\n",
      "Step complete! 0.505000114440918\n",
      "Step complete! 0.5170001983642578\n",
      "Step complete! 0.5079975128173828\n",
      "Step complete! 0.6920022964477539\n",
      "Step complete! 0.6817634105682373\n",
      "Step complete! 0.7479937076568604\n",
      "Step complete! 0.7000031471252441\n",
      "Step complete! 0.5309982299804688\n",
      "Step complete! 0.6935381889343262\n",
      "Step complete! 0.4889998435974121\n",
      "Step complete! 0.7480013370513916\n",
      "Step complete! 0.5629982948303223\n",
      "Step complete! 0.7600104808807373\n",
      "Step complete! 0.7349910736083984\n",
      "Step complete! 0.7120003700256348\n",
      "Step complete! 0.5369997024536133\n",
      "Step complete! 0.47300004959106445\n",
      "Step complete! 0.6508188247680664\n",
      "Step complete! 0.6689989566802979\n",
      "Step complete! 0.46700143814086914\n",
      "Step complete! 0.6739966869354248\n",
      "Step complete! 0.7600007057189941\n",
      "Step complete! 0.7309978008270264\n",
      "Step complete! 0.5050177574157715\n",
      "Step complete! 0.7249987125396729\n",
      "Step complete! 0.5455219745635986\n",
      "Step complete! 0.508988618850708\n",
      "Step complete! 0.5730025768280029\n",
      "Step complete! 0.5129997730255127\n",
      "Step complete! 0.547001838684082\n",
      "Step complete! 0.7159974575042725\n",
      "Step complete! 0.7190017700195312\n",
      "Step complete! 0.4969961643218994\n",
      "Step complete! 0.6995236873626709\n",
      "Step complete! 0.6829984188079834\n",
      "Step complete! 0.4629993438720703\n",
      "Step complete! 0.5120017528533936\n",
      "Step complete! 0.5019996166229248\n",
      "Step complete! 0.5129997730255127\n",
      "Step complete! 0.7349991798400879\n",
      "Step complete! 0.7325167655944824\n",
      "Step complete! 0.5209987163543701\n",
      "Step complete! 0.5830025672912598\n",
      "Step complete! 0.5269954204559326\n",
      "Step complete! 0.5180106163024902\n",
      "Step complete! 0.703998327255249\n",
      "Step complete! 0.6729791164398193\n",
      "Step complete! 0.6980030536651611\n",
      "Step complete! 0.6760008335113525\n",
      "Step complete! 0.4875218868255615\n",
      "Step complete! 0.6860029697418213\n",
      "Step complete! 0.4656238555908203\n",
      "Step complete! 0.6919982433319092\n",
      "Step complete! 0.5129983425140381\n",
      "Step complete! 0.58599853515625\n",
      "Step complete! 0.767000675201416\n",
      "Step complete! 0.5420022010803223\n",
      "Step complete! 0.7139995098114014\n",
      "Step complete! 0.51300048828125\n",
      "Step complete! 0.5159912109375\n",
      "Step complete! 0.6820003986358643\n",
      "Step complete! 0.6939976215362549\n",
      "Step complete! 0.6860017776489258\n",
      "Step complete! 0.6779561042785645\n",
      "Step complete! 0.47037434577941895\n",
      "Step complete! 0.6890065670013428\n",
      "Step complete! 0.4909937381744385\n",
      "Step complete! 0.4930088520050049\n",
      "Step complete! 0.517998218536377\n",
      "Step complete! 0.735001802444458\n",
      "Step complete! 0.7245125770568848\n",
      "Step complete! 0.5460021495819092\n",
      "Step complete! 0.7390027046203613\n",
      "Step complete! 0.7000017166137695\n",
      "Step complete! 0.6789982318878174\n",
      "Step complete! 0.6700172424316406\n",
      "Step complete! 0.49699950218200684\n",
      "Step complete! 0.7145237922668457\n",
      "Step complete! 0.4819943904876709\n",
      "Step complete! 0.4699981212615967\n",
      "Step complete! 0.6749980449676514\n",
      "Step complete! 0.7040185928344727\n",
      "Step complete! 0.7569983005523682\n",
      "Step complete! 0.5500009059906006\n",
      "Step complete! 0.6900026798248291\n",
      "Step complete! 0.7779982089996338\n",
      "Step complete! 0.532512903213501\n",
      "Step complete! 0.5489974021911621\n",
      "Step complete! 0.7129864692687988\n",
      "Step complete! 0.5000026226043701\n",
      "Step complete! 0.7780008316040039\n",
      "Step complete! 0.4980010986328125\n",
      "Step complete! 0.5019986629486084\n",
      "Step complete! 0.5235309600830078\n",
      "Step complete! 0.7510008811950684\n",
      "Step complete! 0.5219976902008057\n",
      "Step complete! 0.5719985961914062\n",
      "Step complete! 0.7400033473968506\n",
      "Step complete! 0.7400045394897461\n",
      "Step complete! 0.7260031700134277\n",
      "Step complete! 0.7245235443115234\n",
      "Step complete! 0.6889989376068115\n",
      "Step complete! 0.6965663433074951\n",
      "Step complete! 0.5080010890960693\n",
      "Step complete! 0.7140078544616699\n",
      "Step complete! 0.6985671520233154\n",
      "Step complete! 0.49700188636779785\n",
      "Step complete! 0.4840047359466553\n",
      "Step complete! 0.4799954891204834\n",
      "Step complete! 0.48600077629089355\n",
      "Step complete! 0.695986270904541\n",
      "Step complete! 0.7250001430511475\n",
      "Step complete! 0.5240106582641602\n",
      "Step complete! 0.7330029010772705\n",
      "Step complete! 0.7270054817199707\n",
      "Step complete! 0.5369939804077148\n",
      "Step complete! 0.49799346923828125\n",
      "Step complete! 0.49900221824645996\n",
      "Step complete! 0.501997709274292\n",
      "Step complete! 0.49266505241394043\n",
      "Step complete! 0.7800030708312988\n",
      "Step complete! 0.7090015411376953\n",
      "Step complete! 0.52699875831604\n",
      "Step complete! 0.7090048789978027\n",
      "Step complete! 0.7469987869262695\n",
      "Step complete! 0.5409984588623047\n",
      "Step complete! 0.5069992542266846\n",
      "Step complete! 0.5319995880126953\n",
      "Step complete! 0.555001974105835\n",
      "Step complete! 0.71799635887146\n",
      "Step complete! 0.5009982585906982\n",
      "Step complete! 0.6820001602172852\n",
      "Step complete! 0.6629986763000488\n",
      "Step complete! 0.5100016593933105\n",
      "Step complete! 0.6759982109069824\n",
      "Step complete! 0.6850001811981201\n",
      "Step complete! 0.6899981498718262\n",
      "Step complete! 0.698516845703125\n",
      "Step complete! 0.705998420715332\n",
      "Step complete! 0.550992488861084\n",
      "Step complete! 0.7360026836395264\n",
      "Step complete! 0.5079982280731201\n",
      "Step complete! 0.7945156097412109\n",
      "Step complete! 0.5119979381561279\n",
      "Step complete! 0.7110025882720947\n",
      "Step complete! 0.6959989070892334\n",
      "Step complete! 0.517998218536377\n",
      "Step complete! 0.7089977264404297\n",
      "Step complete! 0.6820027828216553\n",
      "Step complete! 0.48399829864501953\n",
      "Step complete! 0.6963493824005127\n",
      "Step complete! 0.740997314453125\n",
      "Step complete! 0.5430014133453369\n",
      "Step complete! 0.5679976940155029\n",
      "Step complete! 0.7240023612976074\n",
      "Step complete! 0.7369983196258545\n",
      "Step complete! 0.7225179672241211\n",
      "Step complete! 0.5039951801300049\n",
      "Step complete! 0.7240040302276611\n",
      "Step complete! 0.6909992694854736\n",
      "Step complete! 0.6680011749267578\n",
      "Step complete! 0.7020053863525391\n",
      "Step complete! 0.6660101413726807\n",
      "Step complete! 0.48000192642211914\n",
      "Step complete! 0.6742024421691895\n",
      "Step complete! 0.5239963531494141\n",
      "Step complete! 0.49900150299072266\n",
      "Step complete! 0.7069990634918213\n",
      "Step complete! 0.5239994525909424\n",
      "Step complete! 0.5299983024597168\n",
      "Step complete! 0.7449996471405029\n",
      "Step complete! 0.4929993152618408\n",
      "Step complete! 0.6687591075897217\n",
      "Step complete! 0.4649984836578369\n",
      "Step complete! 0.46300411224365234\n",
      "Step complete! 0.6860027313232422\n",
      "Step complete! 0.679997444152832\n",
      "Step complete! 0.47099900245666504\n",
      "Step complete! 0.6940031051635742\n",
      "Step complete! 0.4909980297088623\n",
      "Step complete! 0.7829999923706055\n",
      "Step complete! 0.5280020236968994\n",
      "Step complete! 0.5510027408599854\n",
      "Step complete! 0.5159914493560791\n",
      "Step complete! 0.5149979591369629\n",
      "Step complete! 0.5239973068237305\n",
      "Step complete! 0.7239990234375\n",
      "Step complete! 0.4650092124938965\n",
      "Step complete! 0.4759981632232666\n",
      "Step complete! 0.6690011024475098\n",
      "Step complete! 0.6969962120056152\n",
      "Step complete! 0.6690030097961426\n",
      "Step complete! 0.47499942779541016\n",
      "Step complete! 0.6719968318939209\n",
      "Step complete! 0.6925113201141357\n",
      "Step complete! 0.5099992752075195\n",
      "Step complete! 0.5390019416809082\n",
      "Step complete! 0.5520012378692627\n",
      "Step complete! 0.7129993438720703\n",
      "Step complete! 0.5029993057250977\n",
      "Step complete! 0.5310027599334717\n",
      "Step complete! 0.5070064067840576\n",
      "Step complete! 0.4735136032104492\n",
      "Step complete! 0.4779999256134033\n",
      "Step complete! 0.5497019290924072\n",
      "Step complete! 0.6680004596710205\n",
      "Step complete! 0.6820015907287598\n",
      "Step complete! 0.4586513042449951\n",
      "Step complete! 0.4760007858276367\n",
      "Step complete! 0.46700000762939453\n",
      "Step complete! 0.48699951171875\n",
      "Step complete! 0.516998291015625\n",
      "Step complete! 0.7200026512145996\n",
      "Step complete! 0.7119998931884766\n",
      "Step complete! 0.732001781463623\n",
      "Step complete! 0.501997709274292\n",
      "Step complete! 0.7130029201507568\n",
      "Step complete! 0.4659993648529053\n",
      "Step complete! 0.48699951171875\n",
      "Step complete! 0.46790027618408203\n",
      "Step complete! 0.5030019283294678\n",
      "Step complete! 0.49557018280029297\n",
      "Step complete! 0.5340011119842529\n",
      "Step complete! 0.5150015354156494\n",
      "Step complete! 0.6979994773864746\n",
      "Step complete! 0.4549999237060547\n",
      "Step complete! 0.715996503829956\n",
      "Step complete! 0.7249960899353027\n",
      "Step complete! 0.7070014476776123\n",
      "Step complete! 0.731001615524292\n",
      "Step complete! 0.7139987945556641\n",
      "Step complete! 0.6950018405914307\n",
      "Step complete! 0.4721362590789795\n",
      "Step complete! 0.681100606918335\n",
      "Step complete! 0.4710097312927246\n",
      "Step complete! 0.6717274188995361\n",
      "Step complete! 0.686002254486084\n",
      "Step complete! 0.4909954071044922\n",
      "Step complete! 0.4640023708343506\n",
      "Step complete! 0.6919987201690674\n",
      "Step complete! 0.7329990863800049\n",
      "Step complete! 0.7499983310699463\n",
      "Step complete! 0.5950016975402832\n",
      "Step complete! 0.5360000133514404\n",
      "Step complete! 0.7309989929199219\n",
      "Step complete! 0.6690049171447754\n",
      "Step complete! 0.7096004486083984\n",
      "Step complete! 0.665001630783081\n",
      "Step complete! 0.47398900985717773\n",
      "Step complete! 0.6540439128875732\n",
      "Step complete! 0.5010015964508057\n",
      "Step complete! 0.4799981117248535\n",
      "Step complete! 0.5150012969970703\n",
      "Step complete! 0.4589991569519043\n",
      "Step complete! 0.5259974002838135\n",
      "Step complete! 0.7539901733398438\n",
      "Step complete! 0.522000789642334\n",
      "Step complete! 0.5240085124969482\n",
      "Step complete! 0.7269926071166992\n",
      "Step complete! 0.518998384475708\n",
      "Step complete! 0.6820001602172852\n",
      "Step complete! 0.4590013027191162\n",
      "Step complete! 0.7460017204284668\n",
      "Step complete! 0.7215182781219482\n",
      "Step complete! 0.6690011024475098\n",
      "Step complete! 0.6820180416107178\n",
      "Step complete! 0.46600818634033203\n",
      "Step complete! 0.6759939193725586\n",
      "Step complete! 0.7410001754760742\n",
      "Step complete! 0.7199945449829102\n",
      "Step complete! 0.5060005187988281\n",
      "Step complete! 0.5325124263763428\n",
      "Step complete! 0.5630016326904297\n",
      "Step complete! 0.7109971046447754\n",
      "Step complete! 0.49900150299072266\n",
      "Step complete! 0.4779973030090332\n",
      "Step complete! 0.5020029544830322\n",
      "Step complete! 0.7500002384185791\n",
      "Step complete! 0.6960694789886475\n",
      "Step complete! 0.4865245819091797\n",
      "Step complete! 0.6691339015960693\n",
      "Step complete! 0.46899914741516113\n",
      "Step complete! 0.5040004253387451\n",
      "Step complete! 0.48600292205810547\n",
      "Step complete! 0.5629985332489014\n",
      "Step complete! 0.7200028896331787\n",
      "Step complete! 0.5019974708557129\n",
      "Step complete! 0.5429985523223877\n",
      "Step complete! 0.6940014362335205\n",
      "Step complete! 0.7599995136260986\n",
      "Step complete! 0.47952818870544434\n",
      "Step complete! 0.4690425395965576\n",
      "Step complete! 0.502000093460083\n",
      "Step complete! 0.481001615524292\n",
      "Step complete! 0.4649970531463623\n",
      "Step complete! 0.4767920970916748\n",
      "Step complete! 0.7079966068267822\n",
      "Step complete! 0.6850011348724365\n",
      "Step complete! 0.4829998016357422\n",
      "Step complete! 0.7254056930541992\n",
      "Step complete! 0.5310018062591553\n",
      "Step complete! 0.5139997005462646\n",
      "Step complete! 0.5469989776611328\n",
      "Step complete! 0.7160000801086426\n",
      "Step complete! 0.6149997711181641\n",
      "Step complete! 0.5000009536743164\n",
      "Step complete! 0.6579997539520264\n",
      "Step complete! 0.7440013885498047\n",
      "Step complete! 0.4740030765533447\n",
      "Step complete! 0.6869971752166748\n",
      "Step complete! 0.6769981384277344\n",
      "Step complete! 0.4909937381744385\n",
      "Step complete! 0.69000244140625\n",
      "Step complete! 0.503997802734375\n",
      "Step complete! 0.7229981422424316\n",
      "Step complete! 0.5150008201599121\n",
      "Step complete! 0.7039990425109863\n",
      "Step complete! 0.52699875831604\n",
      "Step complete! 0.5045168399810791\n",
      "Step complete! 0.5179955959320068\n",
      "Step complete! 0.45700597763061523\n",
      "Step complete! 0.46401500701904297\n",
      "Step complete! 0.6723031997680664\n",
      "Step complete! 0.4950127601623535\n",
      "Step complete! 0.5050017833709717\n",
      "Step complete! 0.703998327255249\n",
      "Step complete! 0.7096614837646484\n",
      "Step complete! 0.7030000686645508\n",
      "Step complete! 0.5490007400512695\n",
      "Step complete! 0.5299992561340332\n",
      "Step complete! 0.7149972915649414\n",
      "Step complete! 0.7090098857879639\n",
      "Step complete! 0.5030002593994141\n",
      "Step complete! 0.7640011310577393\n",
      "Step complete! 0.6859986782073975\n",
      "Step complete! 0.6600019931793213\n",
      "Step complete! 0.45400047302246094\n",
      "Step complete! 0.6660010814666748\n",
      "Step complete! 0.46601128578186035\n",
      "Step complete! 0.6850004196166992\n",
      "Step complete! 0.4479982852935791\n",
      "Step complete! 0.49300527572631836\n",
      "Step complete! 0.4995291233062744\n",
      "Step complete! 0.5369973182678223\n",
      "Step complete! 0.7149989604949951\n",
      "Step complete! 0.7280020713806152\n",
      "Step complete! 0.7240009307861328\n",
      "Step complete! 0.7320060729980469\n",
      "Step complete! 0.5029985904693604\n",
      "Step complete! 0.48900389671325684\n",
      "Step complete! 0.6775550842285156\n",
      "Step complete! 0.49263525009155273\n",
      "Step complete! 0.6799983978271484\n",
      "Step complete! 0.6950023174285889\n",
      "Step complete! 0.4739973545074463\n",
      "Step complete! 0.6890037059783936\n",
      "Step complete! 0.5000042915344238\n",
      "Step complete! 0.724999189376831\n",
      "Step complete! 0.524000883102417\n",
      "Step complete! 0.515998125076294\n",
      "Step complete! 0.7290170192718506\n",
      "Step complete! 0.7239985466003418\n",
      "Step complete! 0.5260014533996582\n",
      "Step complete! 0.5570142269134521\n",
      "Step complete! 0.490009069442749\n",
      "Step complete! 0.4829988479614258\n",
      "Step complete! 0.4980025291442871\n",
      "Step complete! 0.5000014305114746\n",
      "Step complete! 0.48052096366882324\n",
      "Step complete! 0.6999998092651367\n",
      "Step complete! 0.6969966888427734\n",
      "Step complete! 0.4870016574859619\n",
      "Step complete! 0.7580080032348633\n",
      "Step complete! 0.5739994049072266\n",
      "Step complete! 0.7690131664276123\n",
      "Step complete! 0.5730013847351074\n",
      "Step complete! 0.5435049533843994\n",
      "Step complete! 0.7086203098297119\n",
      "Step complete! 0.5079984664916992\n",
      "Step complete! 0.49100208282470703\n",
      "Step complete! 0.6970005035400391\n",
      "Step complete! 0.6959981918334961\n",
      "Step complete! 0.6891095638275146\n",
      "Step complete! 0.7409923076629639\n",
      "Step complete! 0.7057573795318604\n",
      "Step complete! 0.5350015163421631\n",
      "Step complete! 0.5039956569671631\n",
      "Step complete! 0.5260002613067627\n",
      "Step complete! 0.5450019836425781\n",
      "Step complete! 0.4799997806549072\n",
      "Step complete! 0.7059974670410156\n",
      "Step complete! 0.5300002098083496\n",
      "Step complete! 0.5419971942901611\n",
      "Step complete! 0.4969980716705322\n",
      "Step complete! 0.4860086441040039\n",
      "Step complete! 0.47899913787841797\n",
      "Step complete! 0.4986255168914795\n",
      "Step complete! 0.47899961471557617\n",
      "Step complete! 0.49700236320495605\n",
      "Step complete! 0.650996208190918\n",
      "Step complete! 0.470001220703125\n",
      "Step complete! 0.5090010166168213\n",
      "Step complete! 0.5119993686676025\n",
      "Step complete! 0.7289984226226807\n",
      "Step complete! 0.695000410079956\n",
      "Step complete! 0.7230019569396973\n",
      "Step complete! 0.4869973659515381\n",
      "Step complete! 0.7259981632232666\n",
      "Step complete! 0.6702325344085693\n",
      "Step complete! 0.46957850456237793\n",
      "Step complete! 0.69700026512146\n",
      "Step complete! 0.48200035095214844\n",
      "Step complete! 0.6859982013702393\n",
      "Step complete! 0.4929986000061035\n",
      "Step complete! 0.6900012493133545\n",
      "Step complete! 0.5020003318786621\n",
      "Step complete! 0.5189952850341797\n",
      "Step complete! 0.5340096950531006\n",
      "Step complete! 0.5160017013549805\n",
      "Step complete! 0.7329986095428467\n",
      "Step complete! 0.50901198387146\n",
      "Step complete! 0.7349858283996582\n",
      "Step complete! 0.5259990692138672\n",
      "Step complete! 0.6900012493133545\n",
      "Step complete! 0.47599220275878906\n",
      "Step complete! 0.5029995441436768\n",
      "Step complete! 0.47300291061401367\n",
      "Step complete! 0.748995304107666\n",
      "Step complete! 0.4759993553161621\n",
      "Step complete! 0.6830019950866699\n",
      "Step complete! 0.48851966857910156\n",
      "Step complete! 0.7009992599487305\n",
      "Step complete! 0.7160003185272217\n",
      "Step complete! 0.5400040149688721\n",
      "Step complete! 0.5249934196472168\n",
      "Step complete! 0.710991382598877\n",
      "Step complete! 0.7540116310119629\n",
      "Step complete! 0.4929933547973633\n",
      "Step complete! 0.6855301856994629\n",
      "Step complete! 0.6800057888031006\n",
      "Step complete! 0.4760136604309082\n",
      "Step complete! 0.48400139808654785\n",
      "Step complete! 0.511899471282959\n",
      "Step complete! 0.688164472579956\n",
      "Step complete! 0.5480046272277832\n",
      "Step complete! 0.5389997959136963\n",
      "Step complete! 0.7239992618560791\n",
      "Step complete! 0.7025148868560791\n",
      "Step complete! 0.713001012802124\n",
      "Step complete! 0.5240011215209961\n",
      "Step complete! 0.537999153137207\n",
      "Step complete! 0.5199966430664062\n",
      "Step complete! 0.5139992237091064\n",
      "Step complete! 0.7709999084472656\n",
      "Step complete! 0.51251220703125\n",
      "Step complete! 0.5950002670288086\n",
      "Step complete! 0.5010018348693848\n",
      "Step complete! 0.4810028076171875\n",
      "Step complete! 0.513770580291748\n",
      "Step complete! 0.4729952812194824\n",
      "Step complete! 0.7160131931304932\n",
      "Step complete! 0.6960008144378662\n",
      "Step complete! 0.5470032691955566\n",
      "Step complete! 0.5139980316162109\n",
      "Step complete! 0.5450000762939453\n",
      "Step complete! 0.540996789932251\n",
      "Step complete! 0.7369983196258545\n",
      "Step complete! 0.7700021266937256\n",
      "Step complete! 0.7089998722076416\n",
      "Step complete! 0.6870017051696777\n",
      "Step complete! 0.5190026760101318\n",
      "Step complete! 0.4769937992095947\n",
      "Step complete! 0.7010045051574707\n",
      "Step complete! 0.4920017719268799\n",
      "Step complete! 0.49751901626586914\n",
      "Step complete! 0.4890012741088867\n",
      "Step complete! 0.7010073661804199\n",
      "Step complete! 0.5339956283569336\n",
      "Step complete! 0.6110014915466309\n",
      "Step complete! 0.5719985961914062\n",
      "Step complete! 0.5839993953704834\n",
      "Step complete! 0.7380008697509766\n",
      "Step complete! 0.6030051708221436\n",
      "Step complete! 0.6725163459777832\n",
      "Step complete! 0.8020002841949463\n",
      "Step complete! 0.5815443992614746\n",
      "Step complete! 0.5659985542297363\n",
      "Step complete! 0.7849981784820557\n",
      "Step complete! 0.5240011215209961\n",
      "Step complete! 0.5539999008178711\n",
      "Step complete! 0.5359988212585449\n",
      "Step complete! 0.7799999713897705\n",
      "Step complete! 0.5890028476715088\n",
      "Step complete! 0.797999382019043\n",
      "Step complete! 0.7889978885650635\n",
      "Step complete! 0.7769999504089355\n",
      "Step complete! 0.7539985179901123\n",
      "Step complete! 0.5945243835449219\n",
      "Step complete! 0.5440034866333008\n",
      "Step complete! 0.534996747970581\n",
      "Step complete! 0.5470037460327148\n",
      "Step complete! 0.750774621963501\n",
      "Step complete! 0.7789955139160156\n",
      "Step complete! 0.5139977931976318\n",
      "Step complete! 0.7649991512298584\n",
      "Step complete! 0.5630009174346924\n",
      "Step complete! 0.5690007209777832\n",
      "Step complete! 0.7899987697601318\n",
      "Step complete! 0.7609982490539551\n",
      "Step complete! 0.5520031452178955\n",
      "Step complete! 0.7419953346252441\n",
      "Step complete! 0.7630038261413574\n",
      "Step complete! 0.7239992618560791\n",
      "Step complete! 0.7384583950042725\n",
      "Step complete! 0.5340142250061035\n",
      "Step complete! 0.5450005531311035\n",
      "Step complete! 0.5439980030059814\n",
      "Step complete! 0.5759963989257812\n",
      "Step complete! 0.5860037803649902\n",
      "Step complete! 0.5859918594360352\n",
      "Step complete! 0.7659966945648193\n",
      "Step complete! 0.6295225620269775\n",
      "Step complete! 0.5930042266845703\n",
      "Step complete! 0.5969946384429932\n",
      "Step complete! 0.5730080604553223\n",
      "Step complete! 0.5289971828460693\n",
      "Step complete! 0.5749969482421875\n",
      "Step complete! 0.531005859375\n",
      "Step complete! 0.7510027885437012\n",
      "Step complete! 0.7700014114379883\n",
      "Step complete! 0.7285125255584717\n",
      "Step complete! 0.7180023193359375\n",
      "Step complete! 0.7599978446960449\n",
      "Step complete! 0.7770006656646729\n",
      "Step complete! 0.5700056552886963\n",
      "Step complete! 0.801006555557251\n",
      "Step complete! 0.6035184860229492\n",
      "Step complete! 0.560999870300293\n",
      "Step complete! 0.7399957180023193\n",
      "Step complete! 0.5680050849914551\n",
      "Step complete! 0.648993730545044\n",
      "Step complete! 0.7939984798431396\n",
      "Step complete! 0.5259974002838135\n",
      "Step complete! 0.5740039348602295\n",
      "Step complete! 0.7465271949768066\n",
      "Step complete! 0.7659957408905029\n",
      "Step complete! 0.5669984817504883\n",
      "Step complete! 0.5609993934631348\n",
      "Step complete! 0.6329994201660156\n",
      "Step complete! 0.5770001411437988\n",
      "Step complete! 0.5489983558654785\n",
      "Step complete! 0.751507043838501\n",
      "Step complete! 0.5070028305053711\n",
      "Step complete! 0.7889983654022217\n",
      "Step complete! 0.5739984512329102\n",
      "Step complete! 0.7619996070861816\n",
      "Step complete! 0.5179986953735352\n",
      "Step complete! 0.7700011730194092\n",
      "Step complete! 0.5229973793029785\n",
      "Step complete! 0.7835142612457275\n",
      "Step complete! 0.7810139656066895\n",
      "Step complete! 0.5479860305786133\n",
      "Step complete! 0.5899789333343506\n",
      "Step complete! 0.5649988651275635\n",
      "Step complete! 0.7869951725006104\n",
      "Step complete! 0.5560002326965332\n",
      "Step complete! 0.5545127391815186\n",
      "Step complete! 0.5599899291992188\n",
      "Step complete! 0.5279929637908936\n",
      "Step complete! 0.7264394760131836\n",
      "Step complete! 0.7537798881530762\n",
      "Step complete! 0.7020089626312256\n",
      "Step complete! 0.5589950084686279\n",
      "Step complete! 0.627000093460083\n",
      "Step complete! 0.7850003242492676\n",
      "Step complete! 0.7820069789886475\n",
      "Step complete! 0.8089993000030518\n",
      "Step complete! 0.5940020084381104\n",
      "Step complete! 0.5660011768341064\n",
      "Step complete! 0.7769980430603027\n",
      "Step complete! 0.753002405166626\n",
      "Step complete! 0.5789990425109863\n",
      "Step complete! 0.5260043144226074\n",
      "Step complete! 0.572998046875\n",
      "Step complete! 0.5499999523162842\n",
      "Step complete! 0.7920095920562744\n",
      "Step complete! 0.554999828338623\n",
      "Step complete! 0.7520086765289307\n",
      "Step complete! 0.8149914741516113\n",
      "Step complete! 0.5539982318878174\n",
      "Step complete! 0.6100025177001953\n",
      "Step complete! 0.5535190105438232\n",
      "Step complete! 0.5540006160736084\n",
      "Step complete! 0.5169980525970459\n",
      "Step complete! 0.7610008716583252\n",
      "Step complete! 0.7360007762908936\n",
      "Step complete! 0.7179982662200928\n",
      "Step complete! 0.7222042083740234\n",
      "Step complete! 0.7239973545074463\n",
      "Step complete! 0.5260038375854492\n",
      "Step complete! 0.5589838027954102\n",
      "Step complete! 0.7785205841064453\n",
      "Step complete! 0.5399971008300781\n",
      "Step complete! 0.7630012035369873\n",
      "Step complete! 0.7879993915557861\n",
      "Step complete! 0.6370017528533936\n",
      "Step complete! 0.742997407913208\n",
      "Step complete! 0.7329959869384766\n",
      "Step complete! 0.517540454864502\n",
      "Step complete! 0.7190029621124268\n",
      "Step complete! 0.7460014820098877\n",
      "Step complete! 0.5030043125152588\n",
      "Step complete! 0.5399980545043945\n",
      "Step complete! 0.5609996318817139\n",
      "Step complete! 0.660996675491333\n",
      "Step complete! 0.6395268440246582\n",
      "Step complete! 0.812204122543335\n",
      "Step complete! 0.5460011959075928\n",
      "Step complete! 0.8389990329742432\n",
      "Step complete! 0.6389963626861572\n",
      "Step complete! 0.7289988994598389\n",
      "Step complete! 0.5330114364624023\n",
      "Step complete! 0.7200074195861816\n",
      "Step complete! 0.735999345779419\n",
      "Step complete! 0.49399876594543457\n",
      "Step complete! 0.7105135917663574\n",
      "Step complete! 0.5109965801239014\n",
      "Step complete! 0.5069963932037354\n",
      "Step complete! 0.5640013217926025\n",
      "Step complete! 0.5460007190704346\n",
      "Step complete! 0.5499935150146484\n",
      "Step complete! 0.5340120792388916\n",
      "Step complete! 0.537996768951416\n",
      "Step complete! 0.7509996891021729\n",
      "Step complete! 0.5355117321014404\n",
      "Step complete! 0.7070040702819824\n",
      "Step complete! 0.5369956493377686\n",
      "Step complete! 0.7120041847229004\n",
      "Step complete! 0.7129979133605957\n",
      "Step complete! 0.7059998512268066\n",
      "Step complete! 0.48499512672424316\n",
      "Step complete! 0.8025195598602295\n",
      "Step complete! 0.758004903793335\n",
      "Step complete! 0.7429962158203125\n",
      "Step complete! 0.7259993553161621\n",
      "Step complete! 0.7420003414154053\n",
      "Step complete! 0.774998664855957\n",
      "Step complete! 0.7060024738311768\n",
      "Step complete! 0.523517370223999\n",
      "Step complete! 0.5209965705871582\n",
      "Step complete! 0.6990108489990234\n",
      "Step complete! 0.707988977432251\n",
      "Step complete! 0.5090031623840332\n",
      "Step complete! 0.5009982585906982\n",
      "Step complete! 0.514514684677124\n",
      "Step complete! 0.734687089920044\n",
      "Step complete! 0.744002103805542\n",
      "Step complete! 0.7469985485076904\n",
      "Step complete! 0.5419979095458984\n",
      "Step complete! 0.5410008430480957\n",
      "Step complete! 0.551002025604248\n",
      "Step complete! 0.539996862411499\n",
      "Step complete! 0.5429990291595459\n",
      "Step complete! 0.5170016288757324\n",
      "Step complete! 0.5059986114501953\n",
      "Step complete! 0.7110073566436768\n",
      "Step complete! 0.5166409015655518\n",
      "Step complete! 0.7220015525817871\n",
      "Step complete! 0.7259960174560547\n",
      "Step complete! 0.5129971504211426\n",
      "Step complete! 0.7210001945495605\n",
      "Step complete! 0.5500013828277588\n",
      "Step complete! 0.5460009574890137\n",
      "Step complete! 0.7489979267120361\n",
      "Step complete! 0.7380034923553467\n",
      "Step complete! 0.7269985675811768\n",
      "Step complete! 0.7130002975463867\n",
      "Step complete! 0.49399447441101074\n",
      "Step complete! 0.7180063724517822\n",
      "Step complete! 0.5129899978637695\n",
      "Step complete! 0.7220067977905273\n",
      "Step complete! 0.6894235610961914\n",
      "Step complete! 0.49799609184265137\n",
      "Step complete! 0.7334482669830322\n",
      "Step complete! 0.5259959697723389\n",
      "Step complete! 0.7411618232727051\n",
      "Step complete! 0.7249982357025146\n",
      "Step complete! 0.7540020942687988\n",
      "Step complete! 0.7600018978118896\n",
      "Step complete! 0.7039923667907715\n",
      "Step complete! 0.712411642074585\n",
      "Step complete! 0.5059959888458252\n",
      "Step complete! 0.73600172996521\n",
      "Step complete! 0.4700014591217041\n",
      "Step complete! 0.5049967765808105\n",
      "Step complete! 0.6939985752105713\n",
      "Step complete! 0.6880004405975342\n",
      "Step complete! 0.7360005378723145\n",
      "Step complete! 0.7260007858276367\n",
      "Step complete! 0.5850019454956055\n",
      "Step complete! 0.7415180206298828\n",
      "Step complete! 0.5919957160949707\n",
      "Step complete! 0.5279946327209473\n",
      "Step complete! 0.6959991455078125\n",
      "Step complete! 0.7100009918212891\n",
      "Step complete! 0.6840009689331055\n",
      "Step complete! 0.6729998588562012\n",
      "Step complete! 0.5629856586456299\n",
      "Step complete! 0.6949999332427979\n",
      "Step complete! 0.4880034923553467\n",
      "Step complete! 0.6795175075531006\n",
      "Step complete! 0.5260031223297119\n",
      "Step complete! 0.5230000019073486\n",
      "Step complete! 0.5349986553192139\n",
      "Step complete! 0.72300124168396\n",
      "Step complete! 0.5480077266693115\n",
      "Step complete! 0.7379941940307617\n",
      "Step complete! 0.7179951667785645\n",
      "Step complete! 0.6915163993835449\n",
      "Step complete! 0.49900221824645996\n",
      "Step complete! 0.4660060405731201\n",
      "Step complete! 0.49900054931640625\n",
      "Step complete! 0.6190040111541748\n",
      "Step complete! 0.5519955158233643\n",
      "Step complete! 0.47602295875549316\n",
      "Step complete! 0.49100327491760254\n",
      "Step complete! 0.4879939556121826\n",
      "Step complete! 0.5315356254577637\n",
      "Step complete! 0.7149982452392578\n",
      "Step complete! 0.7140026092529297\n",
      "Step complete! 0.7062163352966309\n",
      "Step complete! 0.7270066738128662\n",
      "Step complete! 0.5309896469116211\n",
      "Step complete! 0.4810037612915039\n",
      "Step complete! 0.663618803024292\n",
      "Step complete! 0.6740097999572754\n",
      "Step complete! 0.5190005302429199\n",
      "Step complete! 0.4700016975402832\n",
      "Step complete! 0.4569973945617676\n",
      "Step complete! 0.6600520610809326\n",
      "Step complete! 0.6899971961975098\n",
      "Step complete! 0.5150015354156494\n",
      "Step complete! 0.5070011615753174\n",
      "Step complete! 0.720001220703125\n",
      "Step complete! 0.7050008773803711\n",
      "Step complete! 0.5460011959075928\n",
      "Step complete! 0.7259976863861084\n",
      "Step complete! 0.6919960975646973\n",
      "Step complete! 0.49550580978393555\n",
      "Step complete! 0.7289998531341553\n",
      "Step complete! 0.5309991836547852\n",
      "Step complete! 0.6980023384094238\n",
      "Step complete! 0.47699785232543945\n",
      "Step complete! 0.4579966068267822\n",
      "Step complete! 0.5020387172698975\n",
      "Step complete! 0.7135195732116699\n",
      "Step complete! 0.7279996871948242\n",
      "Step complete! 0.7288515567779541\n",
      "Step complete! 0.49000120162963867\n",
      "Step complete! 0.7370038032531738\n",
      "Step complete! 0.496995210647583\n",
      "Step complete! 0.5080032348632812\n",
      "Step complete! 0.6885702610015869\n",
      "Step complete! 0.447537899017334\n",
      "Step complete! 0.65598464012146\n",
      "Step complete! 0.7328503131866455\n",
      "Step complete! 0.6899991035461426\n",
      "Step complete! 0.7100000381469727\n",
      "Step complete! 0.5040042400360107\n",
      "Step complete! 0.48800015449523926\n",
      "Step complete! 0.7290017604827881\n",
      "Step complete! 0.5189988613128662\n",
      "Step complete! 0.7049996852874756\n",
      "Step complete! 0.509998083114624\n",
      "Step complete! 0.7119929790496826\n",
      "Step complete! 0.5110001564025879\n",
      "Step complete! 0.4825162887573242\n",
      "Step complete! 0.6799983978271484\n",
      "Step complete! 0.6870026588439941\n",
      "Step complete! 0.4529995918273926\n",
      "Step complete! 0.6977558135986328\n",
      "Step complete! 0.5019993782043457\n",
      "Step complete! 0.49755096435546875\n",
      "Step complete! 0.47100281715393066\n",
      "Step complete! 0.712003231048584\n",
      "Step complete! 0.5199992656707764\n",
      "Step complete! 0.5299983024597168\n",
      "Step complete! 0.5120000839233398\n",
      "Step complete! 0.5079963207244873\n",
      "Step complete! 0.724999189376831\n",
      "Step complete! 0.5259995460510254\n",
      "Step complete! 0.6780092716217041\n",
      "Step complete! 0.6540067195892334\n",
      "Step complete! 0.4829981327056885\n",
      "Step complete! 0.6569998264312744\n",
      "Step complete! 0.6951074600219727\n",
      "Step complete! 0.49500513076782227\n",
      "Step complete! 0.4659895896911621\n",
      "Step complete! 0.5280077457427979\n",
      "Step complete! 0.6670801639556885\n",
      "Step complete! 0.7279958724975586\n",
      "Step complete! 0.7130024433135986\n",
      "Step complete! 0.5199980735778809\n",
      "Step complete! 0.7550103664398193\n",
      "Step complete! 0.48599839210510254\n",
      "Step complete! 0.7550082206726074\n",
      "Step complete! 0.7109997272491455\n",
      "Step complete! 0.508509635925293\n",
      "Step complete! 0.4979848861694336\n",
      "Step complete! 0.6670005321502686\n",
      "Step complete! 0.7180020809173584\n",
      "Step complete! 0.47022032737731934\n",
      "Step complete! 0.48000288009643555\n",
      "Step complete! 0.6775083541870117\n",
      "Step complete! 0.7059967517852783\n",
      "Step complete! 0.7200005054473877\n",
      "Step complete! 0.5130040645599365\n",
      "Step complete! 0.5219957828521729\n",
      "Step complete! 0.7179989814758301\n",
      "Step complete! 0.7169959545135498\n",
      "Step complete! 0.6829984188079834\n",
      "Step complete! 0.4889960289001465\n",
      "Step complete! 0.4770035743713379\n",
      "Step complete! 0.7460005283355713\n",
      "Step complete! 0.7055168151855469\n",
      "Step complete! 0.7579953670501709\n",
      "Step complete! 0.610001802444458\n",
      "Step complete! 0.5739989280700684\n",
      "Step complete! 0.7239990234375\n",
      "Step complete! 0.7360014915466309\n",
      "Step complete! 0.5189979076385498\n",
      "Step complete! 0.6960070133209229\n",
      "Step complete! 0.7429907321929932\n",
      "Step complete! 0.6763935089111328\n",
      "Step complete! 0.5029973983764648\n",
      "Step complete! 0.672997236251831\n",
      "Step complete! 0.5000002384185791\n",
      "Step complete! 0.6911125183105469\n",
      "Step complete! 0.48601579666137695\n",
      "Step complete! 0.5179858207702637\n",
      "Step complete! 0.4844996929168701\n",
      "Step complete! 0.4969971179962158\n",
      "Step complete! 0.7540132999420166\n",
      "Step complete! 0.686002254486084\n",
      "Step complete! 0.5280008316040039\n",
      "Step complete! 0.5110011100769043\n",
      "Step complete! 0.5409986972808838\n",
      "Step complete! 0.7179999351501465\n",
      "Step complete! 0.48000121116638184\n",
      "Step complete! 0.45651674270629883\n",
      "Step complete! 0.7070791721343994\n",
      "Step complete! 0.6649954319000244\n",
      "Step complete! 0.6880035400390625\n",
      "Step complete! 0.48400092124938965\n",
      "Step complete! 0.6849992275238037\n",
      "Step complete! 0.6860036849975586\n",
      "Step complete! 0.5010011196136475\n",
      "Step complete! 0.5119979381561279\n",
      "Step complete! 0.5385165214538574\n",
      "Step complete! 0.5139932632446289\n",
      "Step complete! 0.5449981689453125\n",
      "Step complete! 0.7740092277526855\n",
      "Step complete! 0.5270006656646729\n",
      "Step complete! 0.6971259117126465\n",
      "Step complete! 0.6880369186401367\n",
      "Step complete! 0.49599623680114746\n",
      "Step complete! 0.6824665069580078\n",
      "Step complete! 0.4959986209869385\n",
      "Step complete! 0.4799976348876953\n",
      "Step complete! 0.596003532409668\n",
      "Step complete! 0.49300050735473633\n",
      "Step complete! 0.4648904800415039\n",
      "Step complete! 0.5039994716644287\n",
      "Step complete! 0.5299968719482422\n",
      "Step complete! 0.7165155410766602\n",
      "Step complete! 0.532996654510498\n",
      "Step complete! 0.7029988765716553\n",
      "Step complete! 0.708000898361206\n",
      "Step complete! 1.7691855430603027\n",
      "Step complete! 0.675001859664917\n",
      "Step complete! 0.4839975833892822\n",
      "Step complete! 0.6490042209625244\n",
      "Step complete! 0.7055213451385498\n",
      "Step complete! 0.7676215171813965\n",
      "Step complete! 0.7210016250610352\n",
      "Step complete! 0.598996639251709\n",
      "Step complete! 0.711003303527832\n",
      "Step complete! 0.7419967651367188\n",
      "Step complete! 0.5040006637573242\n",
      "Step complete! 0.6885006427764893\n",
      "Step complete! 0.472994327545166\n",
      "Step complete! 0.44899988174438477\n",
      "Step complete! 0.6940381526947021\n",
      "Step complete! 0.48000001907348633\n",
      "Step complete! 0.49499940872192383\n",
      "Step complete! 0.46100783348083496\n",
      "Step complete! 0.6836562156677246\n",
      "Step complete! 0.6699974536895752\n",
      "Step complete! 0.49500322341918945\n",
      "Step complete! 0.5620026588439941\n",
      "Step complete! 0.5369951725006104\n",
      "Step complete! 0.7300019264221191\n",
      "Step complete! 0.5360004901885986\n",
      "Step complete! 0.6960020065307617\n",
      "Step complete! 0.5079984664916992\n",
      "Step complete! 0.4747459888458252\n",
      "Step complete! 0.6899983882904053\n",
      "Step complete! 0.6850039958953857\n",
      "Step complete! 0.4910004138946533\n",
      "Step complete! 0.5030026435852051\n",
      "Step complete! 0.6970014572143555\n",
      "Step complete! 0.6959977149963379\n",
      "Step complete! 0.7520017623901367\n",
      "Step complete! 0.50699782371521\n",
      "Step complete! 0.7080004215240479\n",
      "Step complete! 0.7779972553253174\n",
      "Step complete! 0.5180001258850098\n",
      "Step complete! 0.5209989547729492\n",
      "Step complete! 0.7250053882598877\n",
      "Step complete! 0.6810023784637451\n",
      "Step complete! 0.48357248306274414\n",
      "Step complete! 0.4559934139251709\n",
      "Step complete! 0.719001293182373\n",
      "Step complete! 0.6910006999969482\n",
      "Step complete! 0.6420013904571533\n",
      "Step complete! 0.6967160701751709\n",
      "Step complete! 0.722008466720581\n",
      "Step complete! 0.5029935836791992\n",
      "Step complete! 0.7199995517730713\n",
      "Step complete! 0.706002950668335\n",
      "Step complete! 0.4869956970214844\n",
      "Step complete! 0.4840061664581299\n",
      "Step complete! 0.5050041675567627\n",
      "Step complete! 0.4549999237060547\n",
      "Step complete! 0.6820025444030762\n",
      "Step complete! 0.4949984550476074\n",
      "Step complete! 0.6640009880065918\n",
      "Step complete! 0.4770028591156006\n",
      "Step complete! 0.47499990463256836\n",
      "Step complete! 0.46399617195129395\n",
      "Step complete! 0.6919994354248047\n",
      "Step complete! 0.7280001640319824\n",
      "Step complete! 0.7179958820343018\n",
      "Step complete! 0.5030019283294678\n",
      "Step complete! 0.7009997367858887\n",
      "Step complete! 0.7279975414276123\n",
      "Step complete! 0.4995267391204834\n",
      "Step complete! 0.6670007705688477\n",
      "Step complete! 0.4550018310546875\n",
      "Step complete! 0.6926841735839844\n",
      "Step complete! 0.6799964904785156\n",
      "Step complete! 0.47800421714782715\n",
      "Step complete! 0.6495480537414551\n",
      "Step complete! 0.443997859954834\n",
      "Step complete! 0.6629993915557861\n",
      "Step complete! 0.7050013542175293\n",
      "Step complete! 0.5109982490539551\n",
      "Step complete! 0.7230010032653809\n",
      "Step complete! 0.4820058345794678\n",
      "Step complete! 0.5010006427764893\n",
      "Step complete! 0.518000602722168\n",
      "Step complete! 0.5119967460632324\n",
      "Step complete! 0.6900641918182373\n",
      "Step complete! 0.6810224056243896\n",
      "Step complete! 0.6759977340698242\n",
      "Step complete! 0.659998893737793\n",
      "Step complete! 0.5170023441314697\n",
      "Step complete! 0.5110006332397461\n",
      "Step complete! 0.7000000476837158\n",
      "Step complete! 0.6948442459106445\n",
      "Step complete! 0.5020101070404053\n",
      "Step complete! 0.7275152206420898\n",
      "Step complete! 0.5110011100769043\n",
      "Step complete! 0.7380023002624512\n",
      "Step complete! 0.5189993381500244\n",
      "Step complete! 0.49100327491760254\n",
      "Step complete! 0.7409982681274414\n",
      "Step complete! 0.6620030403137207\n",
      "Step complete! 0.47899937629699707\n",
      "Step complete! 0.4440000057220459\n",
      "Step complete! 0.45499658584594727\n",
      "Step complete! 0.6661787033081055\n",
      "Step complete! 0.4693329334259033\n",
      "Step complete! 0.7108111381530762\n",
      "Step complete! 0.6870012283325195\n",
      "Step complete! 0.7299990653991699\n",
      "Step complete! 0.7100014686584473\n",
      "Step complete! 0.5049965381622314\n",
      "Step complete! 0.7199933528900146\n",
      "Step complete! 0.5200009346008301\n",
      "Step complete! 0.4850013256072998\n",
      "Step complete! 0.6721174716949463\n",
      "Step complete! 0.6690902709960938\n",
      "Step complete! 0.4680025577545166\n",
      "Step complete! 0.674004077911377\n",
      "Step complete! 0.510998010635376\n",
      "Step complete! 0.47500061988830566\n",
      "Step complete! 0.68399977684021\n",
      "Step complete! 0.6450145244598389\n",
      "Step complete! 0.7256894111633301\n",
      "Step complete! 0.5499966144561768\n",
      "Step complete! 0.739999532699585\n",
      "Step complete! 0.5049996376037598\n",
      "Step complete! 0.724825382232666\n",
      "Step complete! 0.5250000953674316\n",
      "Step complete! 0.485994815826416\n",
      "Step complete! 0.4500010013580322\n",
      "Step complete! 0.476001501083374\n",
      "Step complete! 0.6699924468994141\n",
      "Step complete! 0.489001989364624\n",
      "Step complete! 0.4440021514892578\n",
      "Step complete! 0.49199485778808594\n",
      "Step complete! 0.4400036334991455\n",
      "Step complete! 0.6829757690429688\n",
      "Step complete! 0.5265152454376221\n",
      "Step complete! 0.5760016441345215\n",
      "Step complete! 0.4839935302734375\n",
      "Step complete! 0.7300007343292236\n",
      "Step complete! 0.5460007190704346\n",
      "Step complete! 0.5059983730316162\n",
      "Step complete! 0.49899864196777344\n",
      "Step complete! 0.7065274715423584\n",
      "Step complete! 0.6499991416931152\n",
      "Step complete! 0.7060031890869141\n",
      "Step complete! 0.6609959602355957\n",
      "Step complete! 0.4680018424987793\n",
      "Step complete! 0.7109951972961426\n",
      "Step complete! 0.7039892673492432\n",
      "Step complete! 0.5160038471221924\n",
      "Step complete! 0.7005212306976318\n",
      "Step complete! 0.7330000400543213\n",
      "Step complete! 0.7630016803741455\n",
      "Step complete! 0.507012128829956\n",
      "Step complete! 0.5490036010742188\n",
      "Step complete! 0.4989957809448242\n",
      "Step complete! 0.5170018672943115\n",
      "Step complete! 0.4819905757904053\n",
      "Step complete! 0.6725237369537354\n",
      "Step complete! 0.6820015907287598\n",
      "Step complete! 0.667999267578125\n",
      "Step complete! 0.6860003471374512\n",
      "Step complete! 0.7060019969940186\n",
      "Step complete! 0.5000007152557373\n",
      "Step complete! 0.4860117435455322\n",
      "Step complete! 0.5185110569000244\n",
      "Step complete! 0.6870005130767822\n",
      "Step complete! 0.5129988193511963\n",
      "Step complete! 0.5100135803222656\n",
      "Step complete! 0.5059888362884521\n",
      "Step complete! 0.6539990901947021\n",
      "Step complete! 0.49199771881103516\n",
      "Step complete! 0.45099759101867676\n",
      "Step complete! 0.7010009288787842\n",
      "Step complete! 0.6815845966339111\n",
      "Step complete! 0.49199914932250977\n",
      "Step complete! 0.684589147567749\n",
      "Step complete! 0.6589968204498291\n",
      "Step complete! 0.5049984455108643\n",
      "Step complete! 0.5150103569030762\n",
      "Step complete! 0.5469944477081299\n",
      "Step complete! 0.7379951477050781\n",
      "Step complete! 0.7369999885559082\n",
      "Step complete! 0.742999792098999\n",
      "Step complete! 0.7269985675811768\n",
      "Step complete! 0.7109997272491455\n",
      "Step complete! 0.6660017967224121\n",
      "Step complete! 0.7779943943023682\n",
      "Step complete! 0.6923110485076904\n",
      "Step complete! 0.6580016613006592\n",
      "Step complete! 0.4839937686920166\n",
      "Step complete! 0.49201345443725586\n",
      "Step complete! 0.7060034275054932\n",
      "Step complete! 0.7089986801147461\n",
      "Step complete! 0.7049994468688965\n",
      "Step complete! 0.7045085430145264\n",
      "Step complete! 0.6950032711029053\n",
      "Step complete! 0.4849998950958252\n",
      "Step complete! 0.6749870777130127\n",
      "Step complete! 0.6879997253417969\n",
      "Step complete! 0.6744346618652344\n",
      "Step complete! 0.698000431060791\n",
      "Step complete! 0.7205610275268555\n",
      "Step complete! 0.5019991397857666\n",
      "Step complete! 0.49500083923339844\n",
      "Step complete! 0.5579979419708252\n",
      "Step complete! 0.5430011749267578\n",
      "Step complete! 0.5629985332489014\n",
      "Step complete! 0.5180034637451172\n",
      "Step complete! 0.7909958362579346\n",
      "Step complete! 0.5290029048919678\n",
      "Step complete! 0.5209989547729492\n",
      "Step complete! 0.483367919921875\n",
      "Step complete! 0.69000244140625\n",
      "Step complete! 0.4847085475921631\n",
      "Step complete! 0.4909958839416504\n",
      "Step complete! 0.7010030746459961\n",
      "Step complete! 0.5200011730194092\n",
      "Step complete! 0.6489999294281006\n",
      "Step complete! 0.49199724197387695\n",
      "Step complete! 0.7360033988952637\n",
      "Step complete! 0.5180034637451172\n",
      "Step complete! 0.5249953269958496\n",
      "Step complete! 0.55299973487854\n",
      "Step complete! 0.51300048828125\n",
      "Step complete! 0.5350005626678467\n",
      "Step complete! 0.5659997463226318\n",
      "Step complete! 0.7159993648529053\n",
      "Step complete! 0.48000383377075195\n",
      "Step complete! 0.5080022811889648\n",
      "Step complete! 0.675999641418457\n",
      "Step complete! 0.48799824714660645\n",
      "Step complete! 0.48600244522094727\n",
      "Step complete! 0.49000096321105957\n",
      "Step complete! 0.47899866104125977\n",
      "Step complete! 0.4849984645843506\n",
      "Step complete! 0.5330281257629395\n",
      "Step complete! 0.5150082111358643\n",
      "Step complete! 0.5429983139038086\n",
      "Step complete! 0.7210068702697754\n",
      "Step complete! 0.554997444152832\n",
      "Step complete! 0.7770047187805176\n",
      "Step complete! 0.7279996871948242\n",
      "Step complete! 0.7010025978088379\n",
      "Step complete! 0.6641263961791992\n",
      "Step complete! 0.4799985885620117\n",
      "Step complete! 0.49601197242736816\n",
      "Step complete! 0.48753786087036133\n",
      "Step complete! 0.6649997234344482\n",
      "Step complete! 0.4980003833770752\n",
      "Step complete! 0.4869999885559082\n",
      "Step complete! 0.529003381729126\n",
      "Step complete! 0.7059974670410156\n",
      "Step complete! 0.5310022830963135\n",
      "Step complete! 0.48900604248046875\n",
      "Step complete! 0.7250032424926758\n",
      "Step complete! 0.5049974918365479\n",
      "Step complete! 0.5379984378814697\n",
      "Step complete! 0.5100030899047852\n",
      "Step complete! 0.6890268325805664\n",
      "Step complete! 0.7079808712005615\n",
      "Step complete! 0.5060021877288818\n",
      "Step complete! 0.5089986324310303\n",
      "Step complete! 0.4980018138885498\n",
      "Step complete! 0.5049970149993896\n",
      "Step complete! 0.760350227355957\n",
      "Step complete! 0.46400022506713867\n",
      "Step complete! 0.7259974479675293\n",
      "Step complete! 0.5120012760162354\n",
      "Step complete! 0.7159991264343262\n",
      "Step complete! 0.5290021896362305\n",
      "Step complete! 0.7479996681213379\n",
      "Step complete! 0.7049965858459473\n",
      "Step complete! 0.5240011215209961\n",
      "Step complete! 0.6900126934051514\n",
      "Step complete! 0.7525110244750977\n",
      "Step complete! 0.6867246627807617\n",
      "Step complete! 0.6804170608520508\n",
      "Step complete! 0.5910482406616211\n",
      "Step complete! 0.7009978294372559\n",
      "Step complete! 0.7220039367675781\n",
      "Step complete! 0.7309982776641846\n",
      "Step complete! 0.5289967060089111\n",
      "Step complete! 0.7020022869110107\n",
      "Step complete! 0.5440003871917725\n",
      "Step complete! 0.6989991664886475\n",
      "Step complete! 0.6957061290740967\n",
      "Step complete! 0.68367600440979\n",
      "Step complete! 0.6719934940338135\n",
      "Step complete! 0.47100234031677246\n",
      "Step complete! 0.48599910736083984\n",
      "Step complete! 0.5239999294281006\n",
      "Step complete! 0.6890020370483398\n",
      "Step complete! 0.695002555847168\n",
      "Step complete! 0.7169947624206543\n",
      "Step complete! 0.5210046768188477\n",
      "Step complete! 0.508997917175293\n",
      "Step complete! 0.7410011291503906\n",
      "Step complete! 0.717998743057251\n",
      "Step complete! 0.513005256652832\n",
      "Step complete! 0.47199463844299316\n",
      "Step complete! 0.6789987087249756\n",
      "Step complete! 0.47099924087524414\n",
      "Step complete! 0.4830033779144287\n",
      "Step complete! 0.6869988441467285\n",
      "Step complete! 0.695929765701294\n",
      "Step complete! 0.48299670219421387\n",
      "Step complete! 0.6930017471313477\n",
      "Step complete! 0.5519979000091553\n",
      "Step complete! 0.7054939270019531\n",
      "Step complete! 0.5240013599395752\n",
      "Step complete! 0.7569973468780518\n",
      "Step complete! 0.6990022659301758\n",
      "Step complete! 0.6959981918334961\n",
      "Step complete! 0.6689984798431396\n",
      "Step complete! 0.4610002040863037\n",
      "Step complete! 0.6745717525482178\n",
      "Step complete! 0.6969988346099854\n",
      "Step complete! 0.6649990081787109\n",
      "Step complete! 0.4980027675628662\n",
      "Step complete! 0.47499656677246094\n",
      "Step complete! 0.7460014820098877\n",
      "Step complete! 0.4949982166290283\n",
      "Step complete! 0.7449979782104492\n",
      "Step complete! 0.7390100955963135\n",
      "Step complete! 0.5269989967346191\n",
      "Step complete! 0.5300002098083496\n",
      "Step complete! 0.5070018768310547\n",
      "Step complete! 0.6789965629577637\n",
      "Step complete! 0.4890024662017822\n",
      "Step complete! 0.5000045299530029\n",
      "Step complete! 0.5330042839050293\n",
      "Step complete! 0.49498772621154785\n",
      "Step complete! 0.5230123996734619\n",
      "Step complete! 0.47098803520202637\n",
      "Step complete! 0.6980016231536865\n",
      "Step complete! 0.44952893257141113\n",
      "Step complete! 0.7216343879699707\n",
      "Step complete! 0.5030014514923096\n",
      "Step complete! 0.7219984531402588\n",
      "Step complete! 0.7080001831054688\n",
      "Step complete! 0.5600075721740723\n",
      "Step complete! 0.515998125076294\n",
      "Step complete! 0.4940035343170166\n",
      "Step complete! 0.4920015335083008\n",
      "Step complete! 0.4399898052215576\n",
      "Step complete! 0.6803431510925293\n",
      "Step complete! 0.658118724822998\n",
      "Step complete! 0.514002799987793\n",
      "Step complete! 0.49399447441101074\n",
      "Step complete! 0.6575627326965332\n",
      "Step complete! 0.6809849739074707\n",
      "Step complete! 0.7159972190856934\n",
      "Step complete! 0.5070009231567383\n",
      "Step complete! 0.5219967365264893\n",
      "Step complete! 0.48201608657836914\n",
      "Step complete! 0.7130022048950195\n",
      "Step complete! 0.6989998817443848\n",
      "Step complete! 0.4739968776702881\n",
      "Step complete! 0.506011962890625\n",
      "Step complete! 0.504997730255127\n",
      "Step complete! 0.6770083904266357\n",
      "Step complete! 0.7119991779327393\n",
      "Step complete! 0.5000011920928955\n",
      "Step complete! 0.4909985065460205\n",
      "Step complete! 0.48599958419799805\n",
      "Step complete! 0.4561586380004883\n",
      "Step complete! 0.5009975433349609\n",
      "Step complete! 0.702998161315918\n",
      "Step complete! 0.7249996662139893\n",
      "Step complete! 0.4940023422241211\n",
      "Step complete! 0.49100565910339355\n",
      "Step complete! 0.4969971179962158\n",
      "Step complete! 0.5195209980010986\n",
      "Step complete! 0.510000467300415\n",
      "Step complete! 0.4349982738494873\n",
      "Step complete! 0.4750058650970459\n",
      "Step complete! 0.6999936103820801\n",
      "Step complete! 0.69100022315979\n",
      "Step complete! 0.4740021228790283\n",
      "Step complete! 0.49100232124328613\n",
      "Step complete! 0.46200132369995117\n",
      "Step complete! 0.4779956340789795\n",
      "Step complete! 0.471529483795166\n",
      "Step complete! 0.5329992771148682\n",
      "Step complete! 0.5230038166046143\n",
      "Step complete! 0.7199966907501221\n",
      "Step complete! 0.7083413600921631\n",
      "Step complete! 0.6900005340576172\n",
      "Step complete! 0.7129995822906494\n",
      "Step complete! 0.6629989147186279\n",
      "Step complete! 0.46599888801574707\n",
      "Step complete! 0.48900318145751953\n",
      "Step complete! 0.6709966659545898\n",
      "Step complete! 0.693997859954834\n",
      "Step complete! 0.46799707412719727\n",
      "Step complete! 0.47152042388916016\n",
      "Step complete! 0.483994722366333\n",
      "Step complete! 0.6529960632324219\n",
      "Step complete! 0.7190070152282715\n",
      "Step complete! 0.5320022106170654\n",
      "Step complete! 0.7099921703338623\n",
      "Step complete! 0.5190000534057617\n",
      "Step complete! 0.7360002994537354\n",
      "Step complete! 0.5015132427215576\n",
      "Step complete! 0.6460018157958984\n",
      "Step complete! 0.46999549865722656\n",
      "Step complete! 0.4558711051940918\n",
      "Step complete! 0.48599910736083984\n",
      "Step complete! 0.6840100288391113\n",
      "Step complete! 0.48299646377563477\n",
      "Step complete! 0.45652055740356445\n",
      "Step complete! 0.4650003910064697\n",
      "Step complete! 0.6710171699523926\n",
      "Step complete! 0.7233796119689941\n",
      "Step complete! 0.5199928283691406\n",
      "Step complete! 0.501004695892334\n",
      "Step complete! 0.5170011520385742\n",
      "Step complete! 0.7149989604949951\n",
      "Step complete! 0.5220019817352295\n",
      "Step complete! 0.4699985980987549\n",
      "Step complete! 0.47399353981018066\n",
      "Step complete! 0.45400094985961914\n",
      "Step complete! 0.6900005340576172\n",
      "Step complete! 0.7259984016418457\n",
      "Step complete! 0.6405148506164551\n",
      "Step complete! 0.4980006217956543\n",
      "Step complete! 0.4629981517791748\n",
      "Step complete! 0.4910013675689697\n",
      "Step complete! 0.7010231018066406\n",
      "Step complete! 0.5209910869598389\n",
      "Step complete! 0.5600006580352783\n",
      "Step complete! 0.7200040817260742\n",
      "Step complete! 0.5019962787628174\n",
      "Step complete! 0.7335150241851807\n",
      "Step complete! 0.6929991245269775\n",
      "Step complete! 0.48000097274780273\n",
      "Step complete! 0.4809989929199219\n",
      "Step complete! 0.4660019874572754\n",
      "Step complete! 0.6889972686767578\n",
      "Step complete! 0.709083080291748\n",
      "Step complete! 0.44299793243408203\n",
      "Step complete! 0.5089447498321533\n",
      "Step complete! 0.4865148067474365\n",
      "Step complete! 0.7050001621246338\n",
      "Step complete! 0.7199978828430176\n",
      "Step complete! 0.7131891250610352\n",
      "Step complete! 0.5259969234466553\n",
      "Step complete! 0.5350015163421631\n",
      "Step complete! 0.505002498626709\n",
      "Step complete! 0.6710000038146973\n",
      "Step complete! 0.45999908447265625\n",
      "Step complete! 0.6899981498718262\n",
      "Step complete! 0.4425201416015625\n",
      "Step complete! 0.6830005645751953\n",
      "Step complete! 0.4669976234436035\n",
      "Step complete! 0.7073776721954346\n",
      "Step complete! 0.46599674224853516\n",
      "Step complete! 0.49700474739074707\n",
      "Step complete! 0.7209980487823486\n",
      "Step complete! 0.5049989223480225\n",
      "Step complete! 0.7138059139251709\n",
      "Step complete! 0.7145195007324219\n",
      "Step complete! 0.5190012454986572\n",
      "Step complete! 0.5530004501342773\n",
      "Step complete! 0.4770016670227051\n",
      "Step complete! 0.6889958381652832\n",
      "Step complete! 0.6950020790100098\n",
      "Step complete! 0.6750011444091797\n",
      "Step complete! 0.7315249443054199\n",
      "Step complete! 0.46199965476989746\n",
      "Step complete! 0.6829991340637207\n",
      "Step complete! 0.704002857208252\n",
      "Step complete! 0.7290012836456299\n",
      "Step complete! 0.7280023097991943\n",
      "Step complete! 0.5309994220733643\n",
      "Step complete! 0.5360078811645508\n",
      "Step complete! 0.7199869155883789\n",
      "Step complete! 0.684532880783081\n",
      "Step complete! 0.683997631072998\n",
      "Step complete! 0.7053210735321045\n",
      "Step complete! 0.720001220703125\n",
      "Step complete! 0.4680008888244629\n",
      "Step complete! 0.762000560760498\n",
      "Step complete! 0.4909987449645996\n",
      "Step complete! 0.492002010345459\n",
      "Step complete! 0.7129998207092285\n",
      "Step complete! 0.5089993476867676\n",
      "Step complete! 0.49600672721862793\n",
      "Step complete! 0.5369939804077148\n",
      "Step complete! 0.5210044384002686\n",
      "Step complete! 0.5429933071136475\n",
      "Step complete! 0.5265135765075684\n",
      "Step complete! 0.6980006694793701\n",
      "Step complete! 0.693000316619873\n",
      "Step complete! 0.49599409103393555\n",
      "Step complete! 0.4889979362487793\n",
      "Step complete! 0.5200014114379883\n",
      "Step complete! 0.4649980068206787\n",
      "Step complete! 0.5079984664916992\n",
      "Step complete! 0.47899889945983887\n",
      "Step complete! 0.5255296230316162\n",
      "Step complete! 0.7279994487762451\n",
      "Step complete! 0.5229969024658203\n",
      "Step complete! 0.5450055599212646\n",
      "Step complete! 0.5270025730133057\n",
      "Step complete! 0.7639966011047363\n",
      "Step complete! 0.5439996719360352\n",
      "Step complete! 0.6899988651275635\n",
      "Step complete! 0.7485167980194092\n",
      "Step complete! 0.496523380279541\n",
      "Step complete! 0.47213101387023926\n",
      "Step complete! 0.527998685836792\n",
      "Step complete! 0.534998893737793\n",
      "Step complete! 0.5070137977600098\n",
      "Step complete! 0.570589542388916\n",
      "Step complete! 0.7070126533508301\n",
      "Step complete! 0.7970106601715088\n",
      "Step complete! 0.5570063591003418\n",
      "Step complete! 0.7469964027404785\n",
      "Step complete! 0.5220003128051758\n",
      "Step complete! 0.5859997272491455\n",
      "Step complete! 0.7749998569488525\n",
      "Step complete! 0.5649995803833008\n",
      "Step complete! 0.7200007438659668\n",
      "Step complete! 0.5220015048980713\n",
      "Step complete! 0.7670016288757324\n",
      "Step complete! 0.7159993648529053\n",
      "Step complete! 0.7320029735565186\n",
      "Step complete! 0.7099981307983398\n",
      "Step complete! 0.7780148983001709\n",
      "Step complete! 0.5739865303039551\n",
      "Step complete! 0.5999984741210938\n",
      "Step complete! 0.5920028686523438\n",
      "Step complete! 0.5799977779388428\n",
      "Step complete! 0.5625355243682861\n",
      "Step complete! 0.7570016384124756\n",
      "Step complete! 0.5259995460510254\n",
      "Step complete! 0.7230002880096436\n",
      "Step complete! 0.7299997806549072\n",
      "Step complete! 0.699000358581543\n",
      "Step complete! 0.7119960784912109\n",
      "Step complete! 0.5635266304016113\n",
      "Step complete! 0.5150039196014404\n",
      "Step complete! 0.5490052700042725\n",
      "Step complete! 0.7887020111083984\n",
      "Step complete! 0.7680044174194336\n",
      "Step complete! 0.8409981727600098\n",
      "Step complete! 0.5960001945495605\n",
      "Step complete! 0.5810012817382812\n",
      "Step complete! 0.8090112209320068\n",
      "Step complete! 0.575998067855835\n",
      "Step complete! 0.5070042610168457\n",
      "Step complete! 0.7409942150115967\n",
      "Step complete! 0.7740006446838379\n",
      "Step complete! 0.738001823425293\n",
      "Step complete! 0.5640056133270264\n",
      "Step complete! 0.5289967060089111\n",
      "Step complete! 0.7389998435974121\n",
      "Step complete! 0.7415213584899902\n",
      "Step complete! 0.7779989242553711\n",
      "Step complete! 0.5219995975494385\n",
      "Step complete! 0.5510044097900391\n",
      "Step complete! 0.7969982624053955\n",
      "Step complete! 0.7250015735626221\n",
      "Step complete! 0.528022289276123\n",
      "Step complete! 0.5399940013885498\n",
      "Step complete! 0.5225441455841064\n",
      "Step complete! 0.7600007057189941\n",
      "Step complete! 0.735999584197998\n",
      "Step complete! 0.5130043029785156\n",
      "Step complete! 0.7330071926116943\n",
      "Step complete! 0.7799983024597168\n",
      "Step complete! 0.7559981346130371\n",
      "Step complete! 0.7825281620025635\n",
      "Step complete! 0.7500145435333252\n",
      "Step complete! 0.5729882717132568\n",
      "Step complete! 0.5740070343017578\n",
      "Step complete! 0.5739955902099609\n",
      "Step complete! 0.731987476348877\n",
      "Step complete! 0.5239901542663574\n",
      "Step complete! 0.5235154628753662\n",
      "Step complete! 0.544001579284668\n",
      "Step complete! 0.7459969520568848\n",
      "Step complete! 0.5520000457763672\n",
      "Step complete! 0.5190005302429199\n",
      "Step complete! 0.5959975719451904\n",
      "Step complete! 0.8270001411437988\n",
      "Step complete! 0.7775213718414307\n",
      "Step complete! 0.5549988746643066\n",
      "Step complete! 0.6030170917510986\n",
      "Step complete! 0.5989992618560791\n",
      "Step complete! 0.7930045127868652\n",
      "Step complete! 0.7505266666412354\n",
      "Step complete! 0.5519967079162598\n",
      "Step complete! 0.5509989261627197\n",
      "Step complete! 0.5230011940002441\n",
      "Step complete! 0.5479898452758789\n",
      "Step complete! 0.5400009155273438\n",
      "Step complete! 0.5619983673095703\n",
      "Step complete! 0.7729973793029785\n",
      "Step complete! 0.7850003242492676\n",
      "Step complete! 0.5460009574890137\n",
      "Step complete! 0.5535295009613037\n",
      "Step complete! 0.7999982833862305\n",
      "Step complete! 0.5780031681060791\n",
      "Step complete! 0.7350020408630371\n",
      "Step complete! 0.7470102310180664\n",
      "Step complete! 0.7259860038757324\n",
      "Step complete! 0.5169978141784668\n",
      "Step complete! 0.7665383815765381\n",
      "Step complete! 0.5239965915679932\n",
      "Step complete! 0.5069937705993652\n",
      "Step complete! 0.5519979000091553\n",
      "Step complete! 0.7279977798461914\n",
      "Step complete! 0.5700027942657471\n",
      "Step complete! 0.7519974708557129\n",
      "Step complete! 0.5580036640167236\n",
      "Step complete! 0.7530002593994141\n",
      "Step complete! 0.7495110034942627\n",
      "Step complete! 0.49199891090393066\n",
      "Step complete! 0.540999174118042\n",
      "Step complete! 0.5140242576599121\n",
      "Step complete! 0.5649964809417725\n",
      "Step complete! 0.5279920101165771\n",
      "Step complete! 0.6980006694793701\n",
      "Step complete! 0.7465202808380127\n",
      "Step complete! 0.7089977264404297\n",
      "Step complete! 0.572002649307251\n",
      "Step complete! 0.7739822864532471\n",
      "Step complete! 0.5670006275177002\n",
      "Step complete! 0.5769991874694824\n",
      "Step complete! 0.5600101947784424\n",
      "Step complete! 0.7650034427642822\n",
      "Step complete! 0.7835228443145752\n",
      "Step complete! 0.5379958152770996\n",
      "Step complete! 0.5239975452423096\n",
      "Step complete! 0.7308616638183594\n",
      "Step complete! 0.6310105323791504\n",
      "Step complete! 0.7140014171600342\n",
      "Step complete! 0.5375232696533203\n",
      "Step complete! 0.5200002193450928\n",
      "Step complete! 0.5669987201690674\n",
      "Step complete! 0.7840051651000977\n",
      "Step complete! 0.7819962501525879\n",
      "Step complete! 0.7940025329589844\n",
      "Step complete! 0.760993242263794\n",
      "Step complete! 0.7234349250793457\n",
      "Step complete! 0.721001148223877\n",
      "Step complete! 0.5229952335357666\n",
      "Step complete! 0.49300456047058105\n",
      "Step complete! 0.5259990692138672\n",
      "Step complete! 0.5110032558441162\n",
      "Step complete! 0.7245137691497803\n",
      "Step complete! 0.5339977741241455\n",
      "Step complete! 0.7469973564147949\n",
      "Step complete! 0.5409996509552002\n",
      "Step complete! 0.5720007419586182\n",
      "Step complete! 0.777015209197998\n",
      "Step complete! 0.5630004405975342\n",
      "Step complete! 0.53299880027771\n",
      "Step complete! 0.5545191764831543\n",
      "Step complete! 0.4939992427825928\n",
      "Step complete! 0.7369985580444336\n",
      "Step complete! 0.5220010280609131\n",
      "Step complete! 0.5209991931915283\n",
      "Step complete! 0.6720004081726074\n",
      "Step complete! 0.6870131492614746\n",
      "Step complete! 0.7300295829772949\n",
      "Step complete! 0.7065191268920898\n",
      "Step complete! 0.5260007381439209\n",
      "Step complete! 0.747002124786377\n",
      "Step complete! 0.7420012950897217\n",
      "Step complete! 0.5489997863769531\n",
      "Step complete! 0.5479919910430908\n",
      "Step complete! 0.5519974231719971\n",
      "Step complete! 0.5080018043518066\n",
      "Step complete! 0.6815228462219238\n",
      "Step complete! 0.6980030536651611\n",
      "Step complete! 0.6858878135681152\n",
      "Step complete! 0.5080006122589111\n",
      "Step complete! 0.7110016345977783\n",
      "Step complete! 0.7490074634552002\n",
      "Step complete! 0.790015459060669\n",
      "Step complete! 0.5519969463348389\n",
      "Step complete! 0.494002103805542\n",
      "Step complete! 0.7360086441040039\n",
      "Step complete! 0.7350013256072998\n",
      "Step complete! 0.7279994487762451\n",
      "Step complete! 0.7200000286102295\n",
      "Step complete! 0.7289996147155762\n",
      "Step complete! 0.5270018577575684\n",
      "Step complete! 0.7120037078857422\n",
      "Step complete! 0.6999971866607666\n",
      "Step complete! 0.5120010375976562\n",
      "Step complete! 0.523998498916626\n",
      "Step complete! 0.5005190372467041\n",
      "Step complete! 0.5689988136291504\n",
      "Step complete! 0.7390096187591553\n",
      "Step complete! 0.7300026416778564\n",
      "Step complete! 0.5439975261688232\n",
      "Step complete! 0.5220048427581787\n",
      "Step complete! 0.5330164432525635\n",
      "Step complete! 0.7169947624206543\n",
      "Step complete! 0.49599266052246094\n",
      "Step complete! 0.5325145721435547\n",
      "Step complete! 0.7000000476837158\n",
      "Step complete! 0.5530056953430176\n",
      "Step complete! 0.6819841861724854\n",
      "Step complete! 0.5179996490478516\n",
      "Step complete! 0.5070085525512695\n",
      "Step complete! 0.7139999866485596\n",
      "Step complete! 0.5105280876159668\n",
      "Step complete! 0.7350006103515625\n",
      "Step complete! 0.5200178623199463\n",
      "Step complete! 0.5229842662811279\n",
      "Step complete! 0.5180017948150635\n",
      "Step complete! 0.5339984893798828\n",
      "Step complete! 0.7260005474090576\n",
      "Step complete! 0.502518892288208\n",
      "Step complete! 0.504002571105957\n",
      "Step complete! 0.7356433868408203\n",
      "Step complete! 0.48999595642089844\n",
      "Step complete! 0.7059996128082275\n",
      "Step complete! 0.7213435173034668\n",
      "Step complete! 0.7069911956787109\n",
      "Step complete! 0.7259976863861084\n",
      "Step complete! 0.7449977397918701\n",
      "Step complete! 0.5509986877441406\n",
      "Step complete! 0.5200014114379883\n",
      "Step complete! 0.5180001258850098\n",
      "Step complete! 0.5209970474243164\n",
      "Step complete! 0.6920034885406494\n",
      "Step complete! 0.6969993114471436\n",
      "Step complete! 0.4729938507080078\n",
      "Step complete! 0.6800003051757812\n",
      "Step complete! 0.49499988555908203\n",
      "Step complete! 0.518000602722168\n",
      "Step complete! 0.499004602432251\n",
      "Step complete! 0.7219831943511963\n",
      "Step complete! 0.4850029945373535\n",
      "Step complete! 0.5739991664886475\n",
      "Step complete! 0.7400085926055908\n",
      "Step complete! 0.7205207347869873\n",
      "Step complete! 0.7219994068145752\n",
      "Step complete! 0.7139999866485596\n",
      "Step complete! 0.5049998760223389\n",
      "Step complete! 0.7089946269989014\n",
      "Step complete! 0.47899532318115234\n",
      "Step complete! 0.7035174369812012\n",
      "Step complete! 0.4986691474914551\n",
      "Step complete! 0.49099302291870117\n",
      "Step complete! 0.6999940872192383\n",
      "Step complete! 0.4680008888244629\n",
      "Step complete! 0.6949985027313232\n",
      "Step complete! 0.764005184173584\n",
      "Step complete! 0.753995418548584\n",
      "Step complete! 0.5929994583129883\n",
      "Step complete! 0.5250000953674316\n",
      "Step complete! 0.7479932308197021\n",
      "Step complete! 0.6969988346099854\n",
      "Step complete! 0.5000011920928955\n",
      "Step complete! 0.6780383586883545\n",
      "Step complete! 0.4980010986328125\n",
      "Step complete! 0.7119121551513672\n",
      "Step complete! 0.5249996185302734\n",
      "Step complete! 0.6945188045501709\n",
      "Step complete! 0.5269906520843506\n",
      "Step complete! 0.5247673988342285\n",
      "Step complete! 0.7490189075469971\n",
      "Step complete! 0.5139806270599365\n",
      "Step complete! 0.5520000457763672\n",
      "Step complete! 0.7329890727996826\n",
      "Step complete! 0.7629997730255127\n",
      "Step complete! 0.49445104598999023\n",
      "Step complete! 0.6829967498779297\n",
      "Step complete! 0.7060096263885498\n",
      "Step complete! 0.4999861717224121\n",
      "Step complete! 0.5099985599517822\n",
      "Step complete! 0.4719963073730469\n",
      "Step complete! 0.5400009155273438\n",
      "Step complete! 0.681002140045166\n",
      "Step complete! 0.7159993648529053\n",
      "Step complete! 0.7430033683776855\n",
      "Step complete! 0.7139968872070312\n",
      "Step complete! 0.6990020275115967\n",
      "Step complete! 0.7260010242462158\n",
      "Step complete! 0.5209982395172119\n",
      "Step complete! 0.49199795722961426\n",
      "Step complete! 0.49018001556396484\n",
      "Step complete! 0.5430023670196533\n",
      "Step complete! 0.492999792098999\n",
      "Step complete! 0.5069947242736816\n",
      "Step complete! 0.4830014705657959\n",
      "Step complete! 0.719273567199707\n",
      "Step complete! 0.6909968852996826\n",
      "Step complete! 0.6873893737792969\n",
      "Step complete! 0.6890008449554443\n",
      "Step complete! 0.5279960632324219\n",
      "Step complete! 0.7518854141235352\n",
      "Step complete! 0.7049994468688965\n",
      "Step complete! 0.5190029144287109\n",
      "Step complete! 0.5149989128112793\n",
      "Step complete! 0.7139976024627686\n",
      "Step complete! 0.6815164089202881\n",
      "Step complete! 0.5149974822998047\n",
      "Step complete! 0.49300312995910645\n",
      "Step complete! 0.7177097797393799\n",
      "Step complete! 0.46500182151794434\n",
      "Step complete! 0.5970010757446289\n",
      "Step complete! 0.5009984970092773\n",
      "Step complete! 0.6818947792053223\n",
      "Step complete! 0.7279963493347168\n",
      "Step complete! 0.7160019874572754\n",
      "Step complete! 0.7280077934265137\n",
      "Step complete! 0.713994026184082\n",
      "Step complete! 0.5599994659423828\n",
      "Step complete! 0.5309975147247314\n",
      "Step complete! 0.5209999084472656\n",
      "Step complete! 0.7455458641052246\n",
      "Step complete! 0.5080037117004395\n",
      "Step complete! 0.690995454788208\n",
      "Step complete! 0.48000264167785645\n",
      "Step complete! 0.7059979438781738\n",
      "Step complete! 0.5069994926452637\n",
      "Step complete! 0.6739964485168457\n",
      "Step complete! 0.7210023403167725\n",
      "Step complete! 0.519005537033081\n",
      "Step complete! 0.5150008201599121\n",
      "Step complete! 0.7365164756774902\n",
      "Step complete! 0.6990165710449219\n",
      "Step complete! 0.7099850177764893\n",
      "Step complete! 0.6779961585998535\n",
      "Step complete! 0.6764936447143555\n",
      "Step complete! 0.6892852783203125\n",
      "Step complete! 0.6810054779052734\n",
      "Step complete! 0.48800182342529297\n",
      "Step complete! 0.6717195510864258\n",
      "Step complete! 0.689002275466919\n",
      "Step complete! 0.5519936084747314\n",
      "Step complete! 0.5059995651245117\n",
      "Step complete! 0.5179989337921143\n",
      "Step complete! 0.76300048828125\n",
      "Step complete! 0.6819994449615479\n",
      "Step complete! 0.5210037231445312\n",
      "Step complete! 0.6849989891052246\n",
      "Step complete! 0.7070014476776123\n",
      "Step complete! 0.46100425720214844\n",
      "Step complete! 0.5000014305114746\n",
      "Step complete! 0.4957566261291504\n",
      "Step complete! 0.6919829845428467\n",
      "Step complete! 0.4849984645843506\n",
      "Step complete! 0.6820085048675537\n",
      "Step complete! 0.5109972953796387\n",
      "Step complete! 0.5039985179901123\n",
      "Step complete! 0.5109965801239014\n",
      "Step complete! 0.5299961566925049\n",
      "Step complete! 0.530998945236206\n",
      "Step complete! 0.5100116729736328\n",
      "Step complete! 0.542992115020752\n",
      "Step complete! 0.48599886894226074\n",
      "Step complete! 0.7240016460418701\n",
      "Step complete! 0.5129995346069336\n",
      "Step complete! 0.6639993190765381\n",
      "Step complete! 0.5070059299468994\n",
      "Step complete! 0.6539952754974365\n",
      "Step complete! 0.6587786674499512\n",
      "Step complete! 0.4820075035095215\n",
      "Step complete! 0.48999786376953125\n",
      "Step complete! 0.44899940490722656\n",
      "Step complete! 0.45810532569885254\n",
      "Step complete! 0.48999905586242676\n",
      "Step complete! 0.4960033893585205\n",
      "Step complete! 0.5169963836669922\n",
      "Step complete! 0.7269978523254395\n",
      "Step complete! 0.5899999141693115\n",
      "Step complete! 0.5140023231506348\n",
      "Step complete! 0.5229976177215576\n",
      "Step complete! 0.669011116027832\n",
      "Step complete! 0.7010002136230469\n",
      "Step complete! 0.4390091896057129\n",
      "Step complete! 0.5039958953857422\n",
      "Step complete! 0.4490013122558594\n",
      "Step complete! 0.6629977226257324\n",
      "Step complete! 0.6939959526062012\n",
      "Step complete! 0.6339993476867676\n",
      "Step complete! 0.7065231800079346\n",
      "Step complete! 0.6980066299438477\n",
      "Step complete! 0.5140032768249512\n",
      "Step complete! 0.5139944553375244\n",
      "Step complete! 0.7100052833557129\n",
      "Step complete! 0.7059884071350098\n",
      "Step complete! 0.534001350402832\n",
      "Step complete! 0.6389915943145752\n",
      "Step complete! 0.48310303688049316\n",
      "Step complete! 0.45096707344055176\n",
      "Step complete! 0.4739987850189209\n",
      "Step complete! 0.4680061340332031\n",
      "Step complete! 0.47299957275390625\n",
      "Step complete! 0.6833643913269043\n",
      "Step complete! 0.6779987812042236\n",
      "Step complete! 0.6859850883483887\n",
      "Step complete! 0.5049970149993896\n",
      "Step complete! 0.48400139808654785\n",
      "Step complete! 0.6879971027374268\n",
      "Step complete! 0.5140030384063721\n",
      "Step complete! 0.699998140335083\n",
      "Step complete! 0.6679990291595459\n",
      "Step complete! 0.6818869113922119\n",
      "Step complete! 0.5269794464111328\n",
      "Step complete! 0.7410013675689697\n",
      "Step complete! 0.697981595993042\n",
      "Step complete! 0.6850008964538574\n",
      "Step complete! 0.6632299423217773\n",
      "Step complete! 0.4660007953643799\n",
      "Step complete! 0.6750109195709229\n",
      "Step complete! 0.7090425491333008\n",
      "Step complete! 0.73600172996521\n",
      "Step complete! 0.5395312309265137\n",
      "Step complete! 0.7359857559204102\n",
      "Step complete! 0.49300074577331543\n",
      "Step complete! 0.4780006408691406\n",
      "Step complete! 0.45199108123779297\n",
      "Step complete! 0.679002046585083\n",
      "Step complete! 0.6875181198120117\n",
      "Step complete! 0.4640011787414551\n",
      "Step complete! 0.49500131607055664\n",
      "Step complete! 0.6978075504302979\n",
      "Step complete! 0.4609975814819336\n",
      "Step complete! 0.6690075397491455\n",
      "Step complete! 0.5075271129608154\n",
      "Step complete! 0.6780016422271729\n",
      "Step complete! 0.5009994506835938\n",
      "Step complete! 0.7170214653015137\n",
      "Step complete! 0.5120048522949219\n",
      "Step complete! 0.5179932117462158\n",
      "Step complete! 0.4900026321411133\n",
      "Step complete! 0.4719994068145752\n",
      "Step complete! 0.6410689353942871\n",
      "Step complete! 0.6915247440338135\n",
      "Step complete! 0.4770052433013916\n",
      "Step complete! 0.7016792297363281\n",
      "Step complete! 0.659001350402832\n",
      "Step complete! 0.48399853706359863\n",
      "Step complete! 0.7370049953460693\n",
      "Step complete! 0.49799633026123047\n",
      "Step complete! 0.5080006122589111\n",
      "Step complete! 0.48999810218811035\n",
      "Step complete! 0.7080011367797852\n",
      "Step complete! 0.6979987621307373\n",
      "Step complete! 0.6810033321380615\n",
      "Step complete! 0.48799705505371094\n",
      "Step complete! 0.6497941017150879\n",
      "Step complete! 0.48000192642211914\n",
      "Step complete! 0.4480011463165283\n",
      "Step complete! 0.5010135173797607\n",
      "Step complete! 0.4699850082397461\n",
      "Step complete! 0.48699951171875\n",
      "Step complete! 0.6669979095458984\n",
      "Step complete! 0.45903468132019043\n",
      "Step complete! 0.4968559741973877\n",
      "Step complete! 0.5050017833709717\n",
      "Step complete! 0.5360023975372314\n",
      "Step complete! 0.49199700355529785\n",
      "Step complete! 0.7190001010894775\n",
      "Step complete! 0.5139944553375244\n",
      "Step complete! 0.4970095157623291\n",
      "Step complete! 0.45600056648254395\n",
      "Step complete! 0.689000129699707\n",
      "Step complete! 0.6619982719421387\n",
      "Step complete! 0.4850139617919922\n",
      "Step complete! 0.44753241539001465\n",
      "Step complete! 0.4809987545013428\n",
      "Step complete! 0.5054984092712402\n",
      "Step complete! 0.4609973430633545\n",
      "Step complete! 0.4929976463317871\n",
      "Step complete! 0.740004301071167\n",
      "Step complete! 0.7069962024688721\n",
      "Step complete! 0.5069999694824219\n",
      "Step complete! 0.5339987277984619\n",
      "Step complete! 0.5179998874664307\n",
      "Step complete! 0.7409954071044922\n",
      "Step complete! 0.698228120803833\n",
      "Step complete! 0.667992353439331\n",
      "Step complete! 0.6446802616119385\n",
      "Step complete! 0.4590010643005371\n",
      "Step complete! 0.6900038719177246\n",
      "Step complete! 0.6952521800994873\n",
      "Step complete! 0.5060014724731445\n",
      "Step complete! 0.47901082038879395\n",
      "Step complete! 0.6959857940673828\n",
      "Step complete! 0.7239997386932373\n",
      "Step complete! 1.7760004997253418\n",
      "Step complete! 0.693000316619873\n",
      "Step complete! 0.5189921855926514\n",
      "Step complete! 0.6980164051055908\n",
      "Step complete! 0.6789968013763428\n",
      "Step complete! 0.48296356201171875\n",
      "Step complete! 0.6470005512237549\n",
      "Step complete! 0.6799983978271484\n",
      "Step complete! 0.6810567378997803\n",
      "Step complete! 0.47000646591186523\n",
      "Step complete! 0.7330014705657959\n",
      "Step complete! 0.7085287570953369\n",
      "Step complete! 0.5099973678588867\n",
      "Step complete! 0.6990022659301758\n",
      "Step complete! 0.5239994525909424\n",
      "Step complete! 0.5019998550415039\n",
      "Step complete! 0.6950123310089111\n",
      "Step complete! 0.47699761390686035\n",
      "Step complete! 0.6639981269836426\n",
      "Step complete! 0.47600317001342773\n",
      "Step complete! 0.5145254135131836\n",
      "Step complete! 0.5049996376037598\n",
      "Step complete! 0.5029973983764648\n",
      "Step complete! 0.4869992733001709\n",
      "Step complete! 0.7030000686645508\n",
      "Step complete! 0.48000216484069824\n",
      "Step complete! 0.5139992237091064\n",
      "Step complete! 0.6610000133514404\n",
      "Step complete! 0.5235202312469482\n",
      "Step complete! 0.5409936904907227\n",
      "Step complete! 0.6961321830749512\n",
      "Step complete! 0.5089988708496094\n",
      "Step complete! 0.685002326965332\n",
      "Step complete! 0.5018723011016846\n",
      "Step complete! 0.4829978942871094\n",
      "Step complete! 0.7210016250610352\n",
      "Step complete! 0.5005254745483398\n",
      "Step complete! 0.5220029354095459\n",
      "Step complete! 0.5410103797912598\n",
      "Step complete! 0.6876752376556396\n",
      "Step complete! 0.4909999370574951\n",
      "Step complete! 0.7209980487823486\n",
      "Step complete! 0.7310006618499756\n",
      "Step complete! 0.5349981784820557\n",
      "Step complete! 0.4920012950897217\n",
      "Step complete! 0.7259960174560547\n",
      "Step complete! 0.5139989852905273\n",
      "Step complete! 0.5219995975494385\n",
      "Step complete! 0.5340003967285156\n",
      "Step complete! 0.5350003242492676\n",
      "Step complete! 0.6823444366455078\n",
      "Step complete! 0.5080010890960693\n",
      "Step complete! 0.485001802444458\n",
      "Step complete! 0.6800103187561035\n",
      "Step complete! 0.4909994602203369\n",
      "Step complete! 0.6696646213531494\n",
      "Step complete! 0.7339997291564941\n",
      "Step complete! 0.5490124225616455\n",
      "Step complete! 0.5309972763061523\n",
      "Step complete! 0.7579987049102783\n",
      "Step complete! 0.7230050563812256\n",
      "Step complete! 0.5410001277923584\n",
      "Step complete! 0.5089991092681885\n",
      "Step complete! 0.6885719299316406\n",
      "Step complete! 0.5040018558502197\n",
      "Step complete! 0.7199985980987549\n",
      "Step complete! 0.6940011978149414\n",
      "Step complete! 0.49100828170776367\n",
      "Step complete! 0.4880790710449219\n",
      "Step complete! 0.7090065479278564\n",
      "Step complete! 0.6885104179382324\n",
      "Step complete! 0.5379981994628906\n",
      "Step complete! 0.7899985313415527\n",
      "Step complete! 0.7419998645782471\n",
      "Step complete! 0.5690000057220459\n",
      "Step complete! 0.5380041599273682\n",
      "Step complete! 0.5249960422515869\n",
      "Step complete! 0.48400306701660156\n",
      "Step complete! 0.4799959659576416\n",
      "Step complete! 0.5209929943084717\n",
      "Step complete! 0.6809978485107422\n",
      "Step complete! 0.5240018367767334\n",
      "Step complete! 0.49100327491760254\n",
      "Step complete! 0.5000007152557373\n",
      "Step complete! 0.482999324798584\n",
      "Step complete! 0.5250101089477539\n",
      "Step complete! 0.5389902591705322\n",
      "Step complete! 0.7710022926330566\n",
      "Step complete! 0.7125048637390137\n",
      "Step complete! 0.764002799987793\n",
      "Step complete! 0.5280108451843262\n",
      "Step complete! 0.7309982776641846\n",
      "Step complete! 0.49700117111206055\n",
      "Step complete! 0.6969990730285645\n",
      "Step complete! 0.49199867248535156\n",
      "Step complete! 0.6989989280700684\n",
      "Step complete! 0.531524658203125\n",
      "Step complete! 0.47599172592163086\n",
      "Step complete! 0.7060165405273438\n",
      "Step complete! 0.5090024471282959\n",
      "Step complete! 0.7260158061981201\n",
      "Step complete! 0.7250010967254639\n",
      "Step complete! 0.5575251579284668\n",
      "Step complete! 0.5419983863830566\n",
      "Step complete! 0.7210025787353516\n",
      "Step complete! 0.7259988784790039\n",
      "Step complete! 0.5510029792785645\n",
      "Step complete! 0.6889994144439697\n",
      "Step complete! 0.7085280418395996\n",
      "Step complete! 0.5140054225921631\n",
      "Step complete! 0.6908228397369385\n",
      "Step complete! 0.5060040950775146\n",
      "Step complete! 0.7629668712615967\n",
      "Step complete! 0.49392175674438477\n",
      "Step complete! 0.674940824508667\n",
      "Step complete! 0.7525150775909424\n",
      "Step complete! 0.7360010147094727\n",
      "Step complete! 0.5269992351531982\n",
      "Step complete! 0.5419976711273193\n",
      "Step complete! 0.5209925174713135\n",
      "Step complete! 0.729135274887085\n",
      "Step complete! 0.7233593463897705\n",
      "Step complete! 0.5109975337982178\n",
      "Step complete! 0.4849982261657715\n",
      "Step complete! 0.674004077911377\n",
      "Step complete! 0.46699976921081543\n",
      "Step complete! 0.5069959163665771\n",
      "Step complete! 0.46899938583374023\n",
      "Step complete! 0.6960005760192871\n",
      "Step complete! 0.7076561450958252\n",
      "Step complete! 0.5029993057250977\n",
      "Step complete! 0.7389981746673584\n",
      "Step complete! 0.7310020923614502\n",
      "Step complete! 0.5280003547668457\n",
      "Step complete! 0.5490000247955322\n",
      "Step complete! 0.5270020961761475\n",
      "Step complete! 0.7220003604888916\n",
      "Step complete! 0.6849970817565918\n",
      "Step complete! 0.6699979305267334\n",
      "Step complete! 0.7025322914123535\n",
      "Step complete! 0.4709913730621338\n",
      "Step complete! 0.5110001564025879\n",
      "Step complete! 0.6673321723937988\n",
      "Step complete! 0.4709963798522949\n",
      "Step complete! 0.5399973392486572\n",
      "Step complete! 0.72300124168396\n",
      "Step complete! 0.5449986457824707\n",
      "Step complete! 0.6969985961914062\n",
      "Step complete! 0.70200514793396\n",
      "Step complete! 0.5079948902130127\n",
      "Step complete! 0.6729962825775146\n",
      "Step complete! 0.7160272598266602\n",
      "Step complete! 0.48355579376220703\n",
      "Step complete! 0.4999828338623047\n",
      "Step complete! 0.6740233898162842\n",
      "Step complete! 0.48900771141052246\n",
      "Step complete! 0.6969954967498779\n",
      "Step complete! 0.7130486965179443\n",
      "Step complete! 0.5119969844818115\n",
      "Step complete! 0.5209996700286865\n",
      "Step complete! 0.7099850177764893\n",
      "Step complete! 0.7395191192626953\n",
      "Step complete! 0.5109994411468506\n",
      "Step complete! 0.7249987125396729\n",
      "Step complete! 0.4889986515045166\n",
      "Step complete! 0.5099999904632568\n",
      "Step complete! 0.48799920082092285\n",
      "Step complete! 0.7089993953704834\n",
      "Step complete! 0.6963920593261719\n",
      "Step complete! 0.6849977970123291\n",
      "Step complete! 0.6857345104217529\n",
      "Step complete! 0.7650048732757568\n",
      "Step complete! 0.6099984645843506\n",
      "Step complete! 0.5229990482330322\n",
      "Step complete! 0.7890048027038574\n",
      "Step complete! 0.5789966583251953\n",
      "Step complete! 0.5340027809143066\n",
      "Step complete! 0.5019960403442383\n",
      "Step complete! 0.6939184665679932\n",
      "Step complete! 0.6650021076202393\n",
      "Step complete! 0.6770009994506836\n",
      "Step complete! 0.5009970664978027\n",
      "Step complete! 0.5015358924865723\n",
      "Step complete! 0.5279965400695801\n",
      "Step complete! 0.6729991436004639\n",
      "Step complete! 0.5140011310577393\n",
      "Step complete! 0.751023530960083\n",
      "Step complete! 0.5439772605895996\n",
      "Step complete! 0.7390036582946777\n",
      "Step complete! 0.575995922088623\n",
      "Step complete! 0.5750019550323486\n",
      "Step complete! 0.7410016059875488\n",
      "Step complete! 0.7245090007781982\n",
      "Step complete! 0.5540013313293457\n",
      "Step complete! 0.6970093250274658\n",
      "Step complete! 0.7109866142272949\n",
      "Step complete! 0.7420012950897217\n",
      "Step complete! 0.7590034008026123\n",
      "Step complete! 0.5740053653717041\n",
      "Step complete! 0.7045202255249023\n",
      "Step complete! 0.573998212814331\n",
      "Step complete! 0.4739990234375\n",
      "Step complete! 0.6980135440826416\n",
      "Step complete! 1.0820024013519287\n",
      "I don't want to play anymore!\n",
      "Loop complete!\n",
      "Time spent: 3360.61283493042 seconds\n",
      "Number of steps: 5450\n"
     ]
    }
   ],
   "source": [
    "# LEAGUE OF LEGENDS\n",
    "\n",
    "from win32gui import GetWindowText, GetForegroundWindow # To make sure she's playing your game\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "game_window = GetWindowText(GetForegroundWindow())\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "reward = 0.\n",
    "\n",
    "for step in range(dataset.steps):\n",
    "\n",
    "    if GetWindowText(GetForegroundWindow()) != game_window:\n",
    "        print(\"I don't want to play anymore!\")\n",
    "        break\n",
    "\n",
    "    start_step = time()\n",
    "\n",
    "    frame = dataset._grab_frame() # Getting environment state\n",
    "\n",
    "    cmds = hakisa()\n",
    "\n",
    "    #print(cmds[0])\n",
    "\n",
    "    command = dataset.get_command(cmds[0].cpu().numpy(), cmds[1].cpu().numpy(), cmds[2].cpu().numpy())\n",
    "\n",
    "    #print(command)\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    # No Need for thresholding/grayscale here\n",
    "\n",
    "    kda = dataset.get_consequences(1, 1632, 1762-1632, 30-1, tesseract_config='--psm 6')\n",
    "\n",
    "    kills, deaths, assists = preprocess_LoL_KDA(kda)\n",
    "\n",
    "    farm = dataset.get_consequences(1, 1775, 1823-1775, 30-1, tesseract_config='--psm 6')\n",
    "\n",
    "    farm = preprocess_LoL_farm(farm)\n",
    "    \n",
    "    farm = farm/8.\n",
    "\n",
    "    reward += (farm+kills+assists)/(2**deaths)\n",
    "\n",
    "    del kda, kills, deaths, assists, farm\n",
    "\n",
    "    dataset.create_memory(frame, keys=command, values=(cmds[0].cpu(), cmds[1].cpu(), cmds[2].cpu()), reward=reward)\n",
    "\n",
    "    end_step = time()\n",
    "\n",
    "    print(f\"Step complete! {end_step-start_step}\")\n",
    "\n",
    "    if step == (dataset.steps - 1):\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en.wav', winsound.SND_FILENAME) # Because yes\n",
    "\n",
    "end = time()\n",
    "print(f\"Loop complete!\\nTime spent: {end-start} seconds\\nNumber of steps: {step+1}\")\n",
    "\n",
    "del frame, command, reward, step, start, end\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_rest_1_en.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([[[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [137., 182., 161.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 178., 173., 142.],\n",
      "          [176., 173., 142.,  ..., 139., 183., 162.]],\n",
      "\n",
      "         [[136., 181., 160.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 179., 174., 143.],\n",
      "          [178., 173., 142.,  ..., 139., 183., 163.],\n",
      "          ...,\n",
      "          [183., 152., 157.,  ..., 136., 186., 120.],\n",
      "          [135., 185., 118.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 180., 143., 151.]],\n",
      "\n",
      "         [[185., 151., 155.,  ..., 134., 185., 119.],\n",
      "          [136., 186., 119.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 182., 149., 153.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]]], device='cuda:0'), ('key', 'Down', 'left'), (tensor([0]), tensor([0]), tensor([2])), 0.1012827559), (tensor([[[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [138., 182., 148.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 180., 181., 149.],\n",
      "          [180., 182., 149.,  ..., 136., 181., 162.]],\n",
      "\n",
      "         [[137., 181., 158.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 182., 182., 150.],\n",
      "          [181., 180., 147.,  ..., 137., 181., 162.],\n",
      "          ...,\n",
      "          [185., 149., 153.,  ..., 123., 181.,  98.],\n",
      "          [122., 179., 100.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 186., 152., 155.]],\n",
      "\n",
      "         [[186., 147., 153.,  ..., 123., 181., 104.],\n",
      "          [125., 182., 103.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 186., 150., 154.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]]], device='cuda:0'), ('key', 'Up', 'right'), (tensor([0]), tensor([1]), tensor([3])), 0.1012831971), (tensor([[[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [109., 165., 101.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 178., 172., 143.],\n",
      "          [179., 174., 145.,  ..., 113., 168., 105.]],\n",
      "\n",
      "         [[108., 165., 100.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 179., 174., 145.],\n",
      "          [181., 173., 145.,  ..., 112., 168., 104.],\n",
      "          ...,\n",
      "          [187., 159., 158.,  ..., 128., 182., 104.],\n",
      "          [129., 181., 107.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 184., 156., 157.]],\n",
      "\n",
      "         [[187., 157., 157.,  ..., 127., 181., 100.],\n",
      "          [127., 180., 100.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 184., 154., 156.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]]], device='cuda:0'), ('key', 'Down', 'x'), (tensor([0]), tensor([0]), tensor([5])), 0.1012836372), (tensor([[[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [112., 166., 119.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 165., 121., 118.],\n",
      "          [168., 118., 115.,  ..., 112., 167., 112.]],\n",
      "\n",
      "         [[111., 165., 118.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 167., 127., 120.],\n",
      "          [169., 123., 118.,  ..., 112., 167., 113.],\n",
      "          ...,\n",
      "          [187., 159., 158.,  ..., 133., 184., 144.],\n",
      "          [150., 194., 126.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 186., 157., 157.]],\n",
      "\n",
      "         [[187., 157., 158.,  ..., 130., 183., 130.],\n",
      "          [142., 190., 113.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 186., 158., 157.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]]], device='cuda:0'), ('key', 'Up', 'down'), (tensor([0]), tensor([1]), tensor([1])), 0.1012840894), (tensor([[[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          ...,\n",
      "          [126., 170., 140.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 235., 143., 113.],\n",
      "          [195., 111., 111.,  ..., 128., 169., 137.]],\n",
      "\n",
      "         [[124., 168., 135.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 236., 146., 113.],\n",
      "          [198., 120.,  92.,  ..., 102., 179., 133.],\n",
      "          ...,\n",
      "          [211., 148., 152.,  ..., 159., 196., 154.],\n",
      "          [158., 194., 156.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 235., 162., 148.]],\n",
      "\n",
      "         [[212., 145., 151.,  ..., 158., 196., 155.],\n",
      "          [159., 195., 155.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ..., 234., 160., 147.],\n",
      "          ...,\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "          [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]]], device='cuda:0'), ('key', 'Up', 'z'), (tensor([0]), tensor([1]), tensor([4])), 0.1012845335)]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.memory[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_save = dataset.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.memory = memory_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21048/1600089776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Unfortunately, this is where we got the CUDA RuntimeError. Try using resize in Dataset creator and adjust Hakisa accordingly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_data_for_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21048/1248123685.py\u001b[0m in \u001b[0;36mcreate_data_for_study\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Reward got in that step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Now converting to float here to avoid numpy.dtype == object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "# Unfortunately, this is where we got the CUDA RuntimeError. Try using resize in Dataset creator and adjust Hakisa accordingly.\n",
    "\n",
    "dataset.create_data_for_study()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "hakisa.mode = 'Study'\n",
    "costs = []\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = None\n",
    "start_epoch = 0\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hakisa # in order to get memory for the action vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to continue the studying phase.\n",
    "\n",
    "params = torch.load(f'Hakisa/Hakisa_checkpoint.tar')\n",
    "start_epoch = params['Epoch'] + 1\n",
    "hakisa.load_state_dict(params['Hakisa_params'])\n",
    "lr = params['Hakisa_LR']\n",
    "\n",
    "del params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action2Vec(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    The Vectorizer model will assign vectors to each action1 and each action2 according to its context.\n",
    "    In NLP, the context is determined by the position of certain word according to other words.\n",
    "\n",
    "    For us, we could determine the context according to the game state(the frame) and the command used in that state.\n",
    "\n",
    "    But it might be interesting to use other metrics for context, such as HP, MP, Power, Aura, Score...\n",
    "\n",
    "    In order to correctly get the context, we'll be using feature extraction with Conv2Ds on the frames.\n",
    "    This context(or the features extracted from the frames) is gonna be used to condition the action vector.\n",
    "\n",
    "\n",
    "    Game Frame ------> Feature Extraction (Conv2D + MaxPool) ----> Context\n",
    "    O-H action ------> FCC layer --------------------------------> some output?\n",
    "\n",
    "    concatenation(Context, some output) ---> FCC layer ----------> Vector\n",
    "    '''\n",
    "\n",
    "    def __init__(self, command_type, actions1, actions2, evaluate=False):\n",
    "\n",
    "        super(Action2Vec, self).__init__()\n",
    "\n",
    "        self.command_type = len(command_type) # For initialization, the length is what matters.\n",
    "        self.actions1 = len(actions1)\n",
    "        self.actions2 = len(actions2)\n",
    "\n",
    "        self.evaluate = evaluate\n",
    "\n",
    "        # Considering a frame size 200x200x3\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 100, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(100)\n",
    "        self.conv2 = torch.nn.Conv2d(100, 200, kernel_size=3, stride=1, padding=1, bias=False) # 200x200\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(200)\n",
    "        self.pool2x2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # 100x100\n",
    "        self.conv3 = torch.nn.Conv2d(200, 400, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(400)\n",
    "        self.conv4 = torch.nn.Conv2d(400, 600, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(600)\n",
    "        # Add pool 2x2 ---> 50x50\n",
    "        self.conv5 = torch.nn.Conv2d(600, 800, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv6 = torch.nn.Conv2d(800, 1000, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm6 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 25x25\n",
    "        self.conv7 = torch.nn.Conv2d(1000, 1200, kernel_size=4, stride=1, bias=False) # 22x22\n",
    "        self.batchnorm7 = torch.nn.BatchNorm2d(1200)\n",
    "        self.conv8 = torch.nn.Conv2d(1200, 1000, kernel_size=3, stride=1, bias=False) # 20x20\n",
    "        self.batchnorm8 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 10x10\n",
    "        self.conv9 = torch.nn.Conv2d(1000, 800, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm9 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv10 = torch.nn.Conv2d(800, 400, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm10 = torch.nn.BatchNorm2d(400)\n",
    "        # Add pool 2x2 ---> 5x5\n",
    "        self.neuron_frames = torch.nn.Linear(400*5*5, 200*2*2, bias=False)\n",
    "\n",
    "        self.neuron_command_type1 = torch.nn.Linear(self.command_type, 200*2*2, bias=False) # The command type will be used to condition the actions\n",
    "        self.neuron_actions1A = torch.nn.Linear(self.actions1, 200*2*2, bias=False)\n",
    "        self.neuron_actions2A = torch.nn.Linear(self.actions2, 200*2*2, bias=False)\n",
    "\n",
    "        #self.neuron_actions1B = torch.nn.Linear(200*2*6, self.actions1, bias=False)\n",
    "        #self.neuron_actions2B = torch.nn.Linear(200*2*6, self.actions2, bias=False)\n",
    "\n",
    "        self.neuron_actions1B = torch.nn.Linear(200*2*6, self.actions1, bias=False)\n",
    "        self.neuron_actions2B = torch.nn.Linear(200*2*6, self.actions2, bias=False)\n",
    "\n",
    "        self.layer_normA = torch.nn.LayerNorm(200*2*6)\n",
    "        self.layer_normB = torch.nn.LayerNorm(200*2*6)\n",
    "\n",
    "        self.leakyrelu = torch.nn.LeakyReLU(0.25)\n",
    "        #self.softmax = torch.nn.LogSoftmax(-1) # Won't be used ----> Already included in Pytorch's Cross Entropy Loss\n",
    "\n",
    "    def forward(self, game_frame, encoded_command_type, encoded_action1, encoded_action2):\n",
    "\n",
    "        if self.evaluate == False:\n",
    "\n",
    "            x = self.conv1(game_frame)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm2(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv3(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm3(x)\n",
    "            x = self.conv4(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm4(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv5(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm5(x)\n",
    "            x = self.conv6(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm6(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv7(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm7(x)\n",
    "            x = self.conv8(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm8(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv9(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm9(x)\n",
    "            x = self.conv10(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm10(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "            context = self.neuron_frames(x) # (Batch, 200*2*2)\n",
    "\n",
    "            encoded_command_type = self.neuron_command_type1(encoded_command_type) # (Batch, 200*2*2)\n",
    "\n",
    "            context = torch.cat((context, encoded_command_type), -1) # (Batch, 200*2*4)\n",
    "\n",
    "            x = self.neuron_actions1A(encoded_action1)\n",
    "            \n",
    "            x = torch.cat((context, x), -1) # (Batch, 200*2*6)\n",
    "\n",
    "            x = self.layer_normA(x)\n",
    "\n",
    "            output1 = self.neuron_actions1B(x)\n",
    "\n",
    "            x = self.neuron_actions2A(encoded_action2)\n",
    "\n",
    "            x = torch.cat((context, x), -1)\n",
    "\n",
    "            x = self.layer_normB(x)\n",
    "\n",
    "            output2 = self.neuron_actions2B(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "            return output1, output2\n",
    "        \n",
    "        else:\n",
    "\n",
    "            context = self.neuron_frames(game_frame) # (Batch, 200*2*2)\n",
    "\n",
    "            encoded_command_type = self.neuron_command_type1(encoded_command_type) # (Batch, 200*2*2)\n",
    "\n",
    "            context = torch.cat((context, encoded_command_type), -1) # (Batch, 200*2*4)\n",
    "\n",
    "            x = self.neuron_actions1A(encoded_action1)\n",
    "            \n",
    "            x = torch.cat((context, x), -1) # (Batch, 200*2*6)\n",
    "\n",
    "            x = self.layer_normA(x)\n",
    "\n",
    "            output1 = self.neuron_actions1B(x)\n",
    "\n",
    "            x = self.neuron_actions2A(encoded_action2)\n",
    "\n",
    "            x = torch.cat((context, x), -1)\n",
    "\n",
    "            x = self.layer_normB(x)\n",
    "\n",
    "            output2 = self.neuron_actions2B(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "            return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action2vec_model = Action2Vec(command_type, actions1, actions2, evaluate=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4, 12])\n",
      "tensor([[[-7.3424e-02,  8.0671e-01,  1.4318e+00, -1.6218e+00,  1.0770e+00,\n",
      "          -4.7115e-01, -9.6227e-01, -4.2135e-01, -7.5728e-01,  6.6081e-01,\n",
      "           1.2435e+00, -1.8217e+00],\n",
      "         [ 1.3928e-01, -1.7373e-01, -3.0021e-01,  5.6300e-01,  5.5077e-01,\n",
      "          -6.5603e-01,  1.6192e-01, -1.5003e+00,  1.3025e+00, -2.2533e-01,\n",
      "           3.4540e-01, -1.7650e+00],\n",
      "         [ 1.7457e-01, -1.7303e-01, -9.4185e-01,  1.3543e-01, -5.0638e-01,\n",
      "          -2.8714e-01, -1.3042e+00, -8.6903e-01,  1.2033e-01, -3.5175e-01,\n",
      "           7.0597e-01, -2.0232e+00],\n",
      "         [-9.3823e-01, -1.0557e+00,  6.5460e-01, -8.4984e-01,  1.5193e-01,\n",
      "           1.8275e+00,  6.1295e-01, -5.7714e-01,  2.8467e-01, -7.0772e-01,\n",
      "           9.4966e-01, -3.1844e-01]],\n",
      "\n",
      "        [[ 1.7457e-01, -1.7303e-01, -9.4185e-01,  1.3543e-01, -5.0638e-01,\n",
      "          -2.8714e-01, -1.3042e+00, -8.6903e-01,  1.2033e-01, -3.5175e-01,\n",
      "           7.0597e-01, -2.0232e+00],\n",
      "         [ 5.5620e-01,  4.0630e-01,  1.0135e+00, -6.4356e-01,  4.0434e-02,\n",
      "           1.1591e+00, -1.1131e+00, -1.1889e+00, -6.2569e-01,  7.1078e-01,\n",
      "          -5.6807e-01,  3.7234e-01],\n",
      "         [ 1.3928e-01, -1.7373e-01, -3.0021e-01,  5.6300e-01,  5.5077e-01,\n",
      "          -6.5603e-01,  1.6192e-01, -1.5003e+00,  1.3025e+00, -2.2533e-01,\n",
      "           3.4540e-01, -1.7650e+00],\n",
      "         [-8.7502e-01, -4.4550e-01,  5.7963e-01,  4.5923e-01,  8.1140e-02,\n",
      "          -2.0433e-01, -1.6604e-03,  1.2490e+00, -1.0607e+00,  1.6588e+00,\n",
      "          -6.0911e-01, -1.9058e-01]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "teste = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "\n",
    "print(teste.size())\n",
    "\n",
    "embed = torch.nn.Embedding(10, 12)\n",
    "\n",
    "output = embed(teste)\n",
    "\n",
    "print(output.size())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(action2vec_model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, gamma=0.1)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "grads = []\n",
    "\n",
    "#epochs = 10000\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100\tCurrent Loss: 5.0539937019348145\tCurrent Learning Rate: 1e-05\n",
      "Gradients Average: 0.00040838593849912286\n",
      "10/100\tCurrent Loss: 0.010125158354640007\tCurrent Learning Rate: 1e-05\n",
      "Gradients Average: 3.2291990237354185e-07\n",
      "20/100\tCurrent Loss: 0.003002977930009365\tCurrent Learning Rate: 1e-05\n",
      "Gradients Average: 8.554692385587259e-07\n",
      "30/100\tCurrent Loss: 0.002604961395263672\tCurrent Learning Rate: 1e-05\n",
      "Gradients Average: -3.4279639749001944e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11952/1287378341.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0maction2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mencoded_command_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_command_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mencoded_actions1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_actions1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (frames, encoded_command_type, encoded_actions1, encoded_actions2) in enumerate(dataloader):\n",
    "    #for i, (encoded_actions1, encoded_actions2, frames, _, _) in enumerate(dataloader): # return encoded_actions1, encoded_actions2, inputs, labels, rewards\n",
    "        action2vec_model.zero_grad()\n",
    "\n",
    "        frames = frames.to(device)\n",
    "        encoded_command_type = encoded_command_type.to(device)\n",
    "        encoded_actions1 = encoded_actions1.to(device)\n",
    "        encoded_actions2 = encoded_actions2.to(device)\n",
    "\n",
    "        output1, output2 = action2vec_model(frames, encoded_command_type, encoded_actions1, encoded_actions2)\n",
    "\n",
    "        cost1 = loss(output1, encoded_actions1)\n",
    "\n",
    "        cost2 = loss(output2, encoded_actions2)\n",
    "\n",
    "        cost = cost1 + cost2\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        for n, p in action2vec_model.named_parameters():\n",
    "            if 'neuron_frames.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"{epoch}/{epochs}\\tCurrent Loss: {cost.item()}\\tCurrent Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "        print(f\"Gradients Average: {grads[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(encoded_actions1)\n",
    "print(encoded_actions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0144, -4.0533],\n",
      "        [ 3.8853, -3.8473]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "tensor([[-0.9728, -1.4406, -0.7952, -0.2193, -0.8127, -1.3330,  6.6760],\n",
      "        [-0.1510, -1.5077, -1.1029, -0.5130,  7.9366, -1.8635, -2.0265]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output1)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(action2vec_model.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 800])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 800])\n",
      "Dict input maps created successfully!\n",
      "Actions 1 dict length: 2\n",
      "Actions 2 dict length: 7\n",
      "All action maps have been properly fitted by their respective KNN algorithm\n"
     ]
    }
   ],
   "source": [
    "dataset.create_commands_dictionary(action2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 1.9276036024093628, 'Up': 1.9276036024093628}\n",
      "{'up': 0.8760263919830322, 'down': 0.8802969455718994, 'left': 0.8691763877868652, 'right': 0.8644426465034485, 'z': 0.902572512626648, 'x': 0.902572512626648, 'shift': 0.902572512626648}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.key_actions1)\n",
    "print(dataset.key_actions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hakisa = Hakisa(command_type, actions1, actions2, mode='Explore').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/10000\n",
      "Best Loss: 842195.75\tCurrent LR: 0.001\tGradients Average: -2.6702881225637576e-11\n",
      "Predicted Reward: 66.7163314819336\tActual Reward: 100.0\n",
      "Reward loss: 1090.49267578125\n",
      "command_type loss: 0.8958969116210938\taction1_loss: 666362.875\taction2_loss: 174741.46875\n",
      "Actual commands: tensor([[ 1.0000e+00,  3.6876e+02, -1.5074e+02],\n",
      "        [ 1.0000e+00,  1.0869e+03, -5.8274e+02]], device='cuda:0')\n",
      "Predicted commands: (tensor([[ -0.2771,  -1.4188, -14.7499],\n",
      "        [ -1.1669,  -0.3730, -14.4345]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-5.2529],\n",
      "        [-5.2936]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-8.7876],\n",
      "        [-8.8607]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "19/10000\n",
      "Best Loss: 59711.3046875\tCurrent LR: 0.001\tGradients Average: 0.0\n",
      "Predicted Reward: 52.07390594482422\tActual Reward: 100.0\n",
      "Reward loss: 2454.068359375\n",
      "command_type loss: 1.4047632217407227\taction1_loss: 10815.8662109375\taction2_loss: 46439.96484375\n",
      "Actual commands: tensor([[   1.0000, -148.6313, -312.0424],\n",
      "        [   0.0000,  -38.3934,   15.6011]], device='cuda:0')\n",
      "Predicted commands: (tensor([[ -0.0687,  -2.7124, -12.3779],\n",
      "        [ -0.0971,  -2.4080,  -5.9708]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-5.3229],\n",
      "        [-5.3112]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-8.1791],\n",
      "        [-7.7873]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "20/10000\n",
      "Best Loss: 543335.125\tCurrent LR: 0.001\tGradients Average: 6.103515609590104e-11\n",
      "Predicted Reward: 66.99288940429688\tActual Reward: 0.0\n",
      "Reward loss: 2271.91064453125\n",
      "command_type loss: 7.700611591339111\taction1_loss: 454175.5625\taction2_loss: 86879.9296875\n",
      "Actual commands: tensor([[   2.0000, -694.3178,   23.2058],\n",
      "        [   1.0000, -665.9456,  408.2139]], device='cuda:0')\n",
      "Predicted commands: (tensor([[ -2.9895,  -0.0516, -15.3601],\n",
      "        [ -3.2133,  -0.0411, -11.0328]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-6.2043],\n",
      "        [-6.5133]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-7.7602],\n",
      "        [-7.4794]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "21/10000\n",
      "Best Loss: 262429.65625\tCurrent LR: 0.001\tGradients Average: -5.722046100831157e-12\n",
      "Predicted Reward: 63.69624328613281\tActual Reward: 70.0\n",
      "Reward loss: 25.026798248291016\n",
      "command_type loss: 0.01470327191054821\taction1_loss: 51690.53125\taction2_loss: 210714.078125\n",
      "Actual commands: tensor([[   1.0000, -285.0955, -223.7391],\n",
      "        [   1.0000, -165.7855,  603.8935]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-4.7952e+00, -8.3040e-03, -1.4584e+01],\n",
      "        [-3.8689e+00, -2.1103e-02, -1.7267e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-6.0955],\n",
      "        [-5.9729]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-8.4008],\n",
      "        [-8.5259]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "22/10000\n",
      "Best Loss: 799364.5625\tCurrent LR: 0.001\tGradients Average: 2.288818440332463e-11\n",
      "Predicted Reward: 63.673095703125\tActual Reward: 70.0\n",
      "Reward loss: 41.49237060546875\n",
      "command_type loss: 5.920778751373291\taction1_loss: 398759.625\taction2_loss: 400557.53125\n",
      "Actual commands: tensor([[   2.0000,  839.9923,  321.5140],\n",
      "        [   1.0000,  281.6296, -840.5587]], device='cuda:0')\n",
      "Predicted commands: (tensor([[ -1.1002,  -0.4047, -11.6295],\n",
      "        [ -1.6552,  -0.2120, -11.6496]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-5.5826],\n",
      "        [-5.6373]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-8.7348],\n",
      "        [-8.6628]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "23/10000\n",
      "Best Loss: 206515.28125\tCurrent LR: 0.001\tGradients Average: 5.722046100831157e-12\n",
      "Predicted Reward: 62.442604064941406\tActual Reward: 100.0\n",
      "Reward loss: 2847.84912109375\n",
      "command_type loss: 3.676626443862915\taction1_loss: 115755.796875\taction2_loss: 87907.96875\n",
      "Actual commands: tensor([[   1.0000, -273.2119, -110.9733],\n",
      "        [   0.0000, -406.6175,  397.6021]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-6.4150e-04, -7.3532e+00, -1.3964e+01],\n",
      "        [-9.7751e-06, -1.1607e+01, -1.4139e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-6.1040],\n",
      "        [-6.4114]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-9.0881],\n",
      "        [-9.1353]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "24/10000\n",
      "Best Loss: 369738.4375\tCurrent LR: 0.001\tGradients Average: -1.525878902397526e-11\n",
      "Predicted Reward: 54.28589630126953\tActual Reward: 0.0\n",
      "Reward loss: 1621.9930419921875\n",
      "command_type loss: 3.2657930850982666\taction1_loss: 311077.6875\taction2_loss: 57035.453125\n",
      "Actual commands: tensor([[   0.0000, -285.1153, -277.4011],\n",
      "        [   2.0000,  731.6698, -213.3463]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-3.2078e-03, -6.5840e+00, -6.3087e+00],\n",
      "        [-1.9786e-02, -4.0102e+00, -6.5284e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-6.6928],\n",
      "        [-6.3249]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-8.8006],\n",
      "        [-8.5912]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "25/10000\n",
      "Best Loss: 222581.28125\tCurrent LR: 0.001\tGradients Average: 1.7762184084668675e-11\n",
      "Predicted Reward: 58.75045394897461\tActual Reward: 70.0\n",
      "Reward loss: 95.41319274902344\n",
      "command_type loss: 10.895853042602539\taction1_loss: 187023.515625\taction2_loss: 35451.4765625\n",
      "Actual commands: tensor([[   1.0000, -159.0357,  248.7888],\n",
      "        [   1.0000,  585.6582,  -79.0486]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-6.9928e-04, -1.0696e+01, -7.2987e+00],\n",
      "        [-3.1669e-04, -1.1096e+01, -8.1069e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-6.6616],\n",
      "        [-6.6501]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-7.7858],\n",
      "        [-7.8274]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "26/10000\n",
      "Best Loss: 416622.03125\tCurrent LR: 0.001\tGradients Average: -6.103515609590104e-11\n",
      "Predicted Reward: 62.85726547241211\tActual Reward: 100.0\n",
      "Reward loss: 2705.50048828125\n",
      "command_type loss: 7.784028053283691\taction1_loss: 371098.3125\taction2_loss: 42810.4296875\n",
      "Actual commands: tensor([[  2.0000, 147.2334,  34.9917],\n",
      "        [  1.0000, 842.4227, 281.2466]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-5.4037e-03, -5.2260e+00, -1.1160e+01],\n",
      "        [-1.2270e-02, -4.4077e+00, -1.1293e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-5.5795],\n",
      "        [-5.4245]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-8.2521],\n",
      "        [-8.1508]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "27/10000\n",
      "Best Loss: 300208.40625\tCurrent LR: 0.001\tGradients Average: 9.346007995292283e-11\n",
      "Predicted Reward: 63.97594451904297\tActual Reward: 100.0\n",
      "Reward loss: 2317.113037109375\n",
      "command_type loss: 11.23888874053955\taction1_loss: 246579.203125\taction2_loss: 51300.85546875\n",
      "Actual commands: tensor([[   1.0000, -160.7140, -328.1472],\n",
      "        [   0.0000,  681.4183,  -16.6000]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.1991e+01, -1.7789e+01, -6.1989e-06],\n",
      "        [-4.6891e+00, -1.0802e+01, -9.2576e-03]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-2.0983],\n",
      "        [-2.6865]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-7.9501],\n",
      "        [-7.9086]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "28/10000\n",
      "Best Loss: 468560.6875\tCurrent LR: 0.001\tGradients Average: 4.768371461572052e-12\n",
      "Predicted Reward: 60.87807083129883\tActual Reward: 70.0\n",
      "Reward loss: 2276.630615234375\n",
      "command_type loss: 7.065922260284424\taction1_loss: 245616.375\taction2_loss: 220660.625\n",
      "Actual commands: tensor([[   0.0000, -151.0371,  652.8781],\n",
      "        [   2.0000, -694.3178,   23.2058]], device='cuda:0')\n",
      "Predicted commands: (tensor([[ 0.0000e+00, -2.2928e+01, -1.8403e+01],\n",
      "        [-8.3446e-07, -1.5641e+01, -1.4132e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-8.2118],\n",
      "        [-8.1446]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-10.5852],\n",
      "        [-10.5249]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "29/10000\n",
      "Best Loss: 1143593.25\tCurrent LR: 0.001\tGradients Average: 1.220703121918021e-10\n",
      "Predicted Reward: 61.6463508605957\tActual Reward: 100.0\n",
      "Reward loss: 1322.181396484375\n",
      "command_type loss: 6.8651299476623535\taction1_loss: 1027226.375\taction2_loss: 115037.84375\n",
      "Actual commands: tensor([[ 2.0000e+00,  6.8038e+02, -4.7290e+02],\n",
      "        [ 1.0000e+00, -1.2699e+03,  1.0041e+02]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-4.6226e+00, -2.5941e+00, -8.8328e-02],\n",
      "        [-1.7881e-06, -1.3642e+01, -1.4398e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[ 0.7243],\n",
      "        [-7.9292]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-6.0284],\n",
      "        [-9.6128]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "30/10000\n",
      "Best Loss: 1510666.5\tCurrent LR: 0.001\tGradients Average: 6.103515609590104e-11\n",
      "Predicted Reward: 57.88717269897461\tActual Reward: 0.0\n",
      "Reward loss: 3641.616455078125\n",
      "command_type loss: 3.596278429031372\taction1_loss: 1317836.125\taction2_loss: 189185.078125\n",
      "Actual commands: tensor([[   2.0000, 1599.9172, -599.4572],\n",
      "        [   2.0000,  277.1599, -170.1472]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-8.9271e+00, -6.7851e+00, -1.2640e-03],\n",
      "        [-7.6265e-04, -1.1591e+01, -7.1913e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[ 1.7655],\n",
      "        [-8.4679]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-5.5596],\n",
      "        [-9.9729]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "31/10000\n",
      "Best Loss: 4451445.0\tCurrent LR: 0.001\tGradients Average: 1.3427734479876108e-09\n",
      "Predicted Reward: 65.6501693725586\tActual Reward: 70.0\n",
      "Reward loss: 1885.9495849609375\n",
      "command_type loss: 0.024150094017386436\taction1_loss: 4024900.0\taction2_loss: 424659.09375\n",
      "Actual commands: tensor([[ 0.0000e+00,  2.1571e+02, -8.4938e+02],\n",
      "        [ 1.0000e+00,  2.8346e+03, -3.8363e+02]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-8.0286e-03, -4.8839e+00, -7.7540e+00],\n",
      "        [-1.5468e+01, -4.0272e-02, -3.2322e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-10.2129],\n",
      "        [  6.4222]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-10.1052],\n",
      "        [ -2.9389]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "32/10000\n",
      "Best Loss: 1927719.125\tCurrent LR: 0.001\tGradients Average: 0.0\n",
      "Predicted Reward: 46.61275863647461\tActual Reward: 70.0\n",
      "Reward loss: 3174.495849609375\n",
      "command_type loss: 15.182567596435547\taction1_loss: 1618965.75\taction2_loss: 305563.75\n",
      "Actual commands: tensor([[ 1.0000e+00, -2.4871e+02, -6.7463e+02],\n",
      "        [ 1.0000e+00, -1.7993e+03,  4.0375e+02]], device='cuda:0')\n",
      "Predicted commands: (tensor([[ 0.0000e+00, -2.4559e+01, -2.9084e+01],\n",
      "        [-2.1679e-02, -5.8060e+00, -3.9934e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-14.9519],\n",
      "        [-15.0800]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-14.5855],\n",
      "        [-15.1375]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "33/10000\n",
      "Best Loss: 118520.6328125\tCurrent LR: 0.001\tGradients Average: -7.629394338515283e-11\n",
      "Predicted Reward: 48.47563934326172\tActual Reward: 0.0\n",
      "Reward loss: 2283.88720703125\n",
      "command_type loss: 4.171844959259033\taction1_loss: 31398.140625\taction2_loss: 84834.4296875\n",
      "Actual commands: tensor([[   0.0000, -242.7052, -187.0863],\n",
      "        [   1.0000, -108.5914, -383.8391]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-2.7872e-01, -1.4137e+00, -1.3031e+01],\n",
      "        [-3.1442e-04, -8.0650e+00, -2.3314e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-11.0870],\n",
      "        [-12.9394]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[ -9.7229],\n",
      "        [-12.0716]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "34/10000\n",
      "Best Loss: 275568.5625\tCurrent LR: 0.001\tGradients Average: -1.4305115252077893e-12\n",
      "Predicted Reward: 60.24231719970703\tActual Reward: 70.0\n",
      "Reward loss: 1450.071044921875\n",
      "command_type loss: 6.180620403029025e-05\taction1_loss: 134947.421875\taction2_loss: 139171.078125\n",
      "Actual commands: tensor([[   0.0000,   19.4987,  -86.3110],\n",
      "        [   0.0000, -535.5009,  508.0322]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.2349e-04, -8.9991e+00, -2.0217e+01],\n",
      "        [-1.1921e-07, -1.6593e+01, -3.0224e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-16.6761],\n",
      "        [-17.2478]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-13.9742],\n",
      "        [-14.5666]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "35/10000\n",
      "Best Loss: 1277936.0\tCurrent LR: 0.001\tGradients Average: 2.4414061883248905e-09\n",
      "Predicted Reward: 70.7070541381836\tActual Reward: 100.0\n",
      "Reward loss: 470.22637939453125\n",
      "command_type loss: 2.9979474544525146\taction1_loss: 536088.875\taction2_loss: 741373.8125\n",
      "Actual commands: tensor([[ 2.0000e+00,  7.5238e+02, -6.5791e+02],\n",
      "        [ 1.0000e+00,  7.3699e+02,  1.0236e+03]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.2744e+01, -2.5183e-03, -5.9866e+00],\n",
      "        [-8.7056e+00, -9.3353e-03, -4.6966e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[13.4505],\n",
      "        [11.6186]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-0.6141],\n",
      "        [-1.4237]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "36/10000\n",
      "Best Loss: 390202.34375\tCurrent LR: 0.001\tGradients Average: -6.103515470812226e-10\n",
      "Predicted Reward: 68.60709381103516\tActual Reward: 70.0\n",
      "Reward loss: 8.801316261291504\n",
      "command_type loss: 0.002691846340894699\taction1_loss: 15370.279296875\taction2_loss: 374823.25\n",
      "Actual commands: tensor([[   2.0000, -151.0011, -561.3367],\n",
      "        [   2.0000,   -1.9674, -653.8059]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.4568e+01, -5.2374e+00, -5.3285e-03],\n",
      "        [-2.5692e+01, -9.8046e+00, -5.5192e-05]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[22.6409],\n",
      "        [22.3022]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[3.0130],\n",
      "        [2.8187]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "37/10000\n",
      "Best Loss: 702572.6875\tCurrent LR: 0.001\tGradients Average: -6.103515609590104e-11\n",
      "Predicted Reward: 54.987464904785156\tActual Reward: 100.0\n",
      "Reward loss: 2129.47705078125\n",
      "command_type loss: 12.798422813415527\taction1_loss: 521305.5\taction2_loss: 179124.890625\n",
      "Actual commands: tensor([[   1.0000, -349.7452, -592.1829],\n",
      "        [   2.0000, -993.1882, -191.9057]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-2.8549e-02, -3.8133e+00, -5.1045e+00],\n",
      "        [-1.1921e-07, -1.6458e+01, -2.1784e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-25.0445],\n",
      "        [-25.1074]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-19.1164],\n",
      "        [-19.1500]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "38/10000\n",
      "Best Loss: 42869.6484375\tCurrent LR: 0.001\tGradients Average: -1.525878902397526e-11\n",
      "Predicted Reward: 52.12291717529297\tActual Reward: 70.0\n",
      "Reward loss: 1209.1575927734375\n",
      "command_type loss: 20.429269790649414\taction1_loss: 2637.553466796875\taction2_loss: 39002.5078125\n",
      "Actual commands: tensor([[   0.0000,  -53.4139, -114.8114],\n",
      "        [   0.0000,   32.8732,  251.2470]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-2.5422e+01, -5.7859e-04, -7.4551e+00],\n",
      "        [-1.5436e+01, -1.2016e-04, -9.0287e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[14.4763],\n",
      "        [ 7.0657]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-0.5066],\n",
      "        [-3.5852]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "39/10000\n",
      "Best Loss: 3899559.75\tCurrent LR: 0.001\tGradients Average: 9.765624975344167e-10\n",
      "Predicted Reward: 61.66973876953125\tActual Reward: 70.0\n",
      "Reward loss: 370.51318359375\n",
      "command_type loss: 3.352861166000366\taction1_loss: 3535543.5\taction2_loss: 363642.5\n",
      "Actual commands: tensor([[ 0.0000e+00, -9.4600e+02, -2.4819e+02],\n",
      "        [ 2.0000e+00,  2.5083e+03, -8.2640e+02]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.7534e-04, -8.6489e+00, -1.7519e+01],\n",
      "        [-1.7235e+01, -1.2248e-03, -6.7055e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-33.8482],\n",
      "        [ 10.4456]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-26.7232],\n",
      "        [ -2.8503]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "40/10000\n",
      "Best Loss: 880503.5\tCurrent LR: 0.001\tGradients Average: 1.2207030941624453e-09\n",
      "Predicted Reward: 66.61233520507812\tActual Reward: 93.375\n",
      "Reward loss: 370.6183166503906\n",
      "command_type loss: 2.368535041809082\taction1_loss: 677516.0\taction2_loss: 202614.5\n",
      "Actual commands: tensor([[   0.0000,  661.2496, -565.7209],\n",
      "        [   0.0000,  905.1417,  311.9784]], device='cuda:0')\n",
      "Predicted commands: (tensor([[ -1.4259,  -0.2748, -17.1062],\n",
      "        [ -3.3112,  -0.0372, -11.3947]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-32.3855],\n",
      "        [-29.6858]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-24.1774],\n",
      "        [-22.6253]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "41/10000\n",
      "Best Loss: 264939.40625\tCurrent LR: 0.001\tGradients Average: -2.441406243836042e-10\n",
      "Predicted Reward: 68.15301513671875\tActual Reward: 70.0\n",
      "Reward loss: 1692.521484375\n",
      "command_type loss: 0.09200233966112137\taction1_loss: 157371.59375\taction2_loss: 105875.171875\n",
      "Actual commands: tensor([[   1.0000,    2.1914,   47.6439],\n",
      "        [   0.0000, -594.1937, -479.4694]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.8215e+00, -1.7649e-01, -1.1631e+01],\n",
      "        [-7.5110e-03, -4.8951e+00, -2.8368e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-30.1377],\n",
      "        [-34.1062]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-22.2318],\n",
      "        [-24.6423]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "42/10000\n",
      "Best Loss: 1339421.0\tCurrent LR: 0.001\tGradients Average: 7.812499980275334e-09\n",
      "Predicted Reward: 57.96031951904297\tActual Reward: 70.0\n",
      "Reward loss: 192.21533203125\n",
      "command_type loss: 6.030129432678223\taction1_loss: 874780.5\taction2_loss: 464442.21875\n",
      "Actual commands: tensor([[ 0.0000e+00, -1.2667e+03, -9.6739e+02],\n",
      "        [ 1.0000e+00, -4.0044e+02, -7.9204e+01]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.2051e+01, -1.0037e-04, -9.2663e+00],\n",
      "        [-4.6378e+00, -9.7494e-03, -1.0659e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[  2.1635],\n",
      "        [-27.0491]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[ -5.3085],\n",
      "        [-21.8587]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "43/10000\n",
      "Best Loss: 2831479.25\tCurrent LR: 0.001\tGradients Average: -4.57763660310917e-10\n",
      "Predicted Reward: 68.81954956054688\tActual Reward: 0.0\n",
      "Reward loss: 2427.89990234375\n",
      "command_type loss: 6.317379951477051\taction1_loss: 1980310.25\taction2_loss: 848734.5625\n",
      "Actual commands: tensor([[ 0.0000e+00,  1.1793e+03, -6.7467e+02],\n",
      "        [ 1.0000e+00, -1.5147e+03,  1.1549e+03]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-2.5126e-04, -8.2891e+00, -2.6593e+01],\n",
      "        [-5.3279e+01, -1.2635e+01, -3.2186e-06]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-54.9183],\n",
      "        [ 46.5086]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-36.4151],\n",
      "        [ 19.0695]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "44/10000\n",
      "Best Loss: 1094951.875\tCurrent LR: 0.001\tGradients Average: -4.882812487672084e-10\n",
      "Predicted Reward: 53.41499328613281\tActual Reward: 70.0\n",
      "Reward loss: 204.22808837890625\n",
      "command_type loss: 8.76182746887207\taction1_loss: 731605.875\taction2_loss: 363133.03125\n",
      "Actual commands: tensor([[   1.0000, -961.0169,  809.0602],\n",
      "        [   1.0000,  742.2099, -179.9662]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-4.9321e-03, -5.3144e+00, -2.2210e+01],\n",
      "        [-5.0068e-06, -1.2209e+01, -2.8579e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-49.8961],\n",
      "        [-53.4475]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-30.1421],\n",
      "        [-31.6233]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "45/10000\n",
      "Best Loss: 917262.8125\tCurrent LR: 0.001\tGradients Average: -7.934570223078197e-10\n",
      "Predicted Reward: 53.524147033691406\tActual Reward: 100.0\n",
      "Reward loss: 1427.947509765625\n",
      "command_type loss: 9.531632423400879\taction1_loss: 814418.75\taction2_loss: 101406.59375\n",
      "Actual commands: tensor([[   0.0000, -936.0124,  145.2026],\n",
      "        [   1.0000,  864.4449, -447.0116]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.7256e-03, -6.3630e+00, -1.8697e+01],\n",
      "        [ 0.0000e+00, -1.9062e+01, -2.7391e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-59.1911],\n",
      "        [-62.9287]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-32.5223],\n",
      "        [-33.2157]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "46/10000\n",
      "Best Loss: 999859.1875\tCurrent LR: 0.001\tGradients Average: -9.765624975344167e-10\n",
      "Predicted Reward: 25.616323471069336\tActual Reward: 100.0\n",
      "Reward loss: 5488.767578125\n",
      "command_type loss: 10.918931007385254\taction1_loss: 705312.75\taction2_loss: 289046.75\n",
      "Actual commands: tensor([[   0.0000, -870.9489, -399.0640],\n",
      "        [   0.0000,  921.7247,  670.7474]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-8.0553e-03, -4.8256e+00, -1.3737e+01],\n",
      "        [-2.1830e+01, -3.5285e-05, -1.0253e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-77.5023],\n",
      "        [ 37.9444]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-33.4919],\n",
      "        [  4.0761]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "47/10000\n",
      "Best Loss: 2426561.75\tCurrent LR: 0.001\tGradients Average: 1.9531249950688334e-09\n",
      "Predicted Reward: 53.761409759521484\tActual Reward: 100.0\n",
      "Reward loss: 1120.5548095703125\n",
      "command_type loss: 0.20638683438301086\taction1_loss: 2003978.5\taction2_loss: 421462.46875\n",
      "Actual commands: tensor([[ 1.0000e+00, -2.7680e+02, -9.1915e+01],\n",
      "        [ 2.0000e+00,  2.0480e+03,  9.2434e+02]], device='cuda:0')\n",
      "Predicted commands: (tensor([[  -1.0940,   -0.4128,   -5.7105],\n",
      "        [-109.1369,  -22.7010,    0.0000]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-66.7895],\n",
      "        [ 57.0520]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-27.2616],\n",
      "        [  8.5058]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "48/10000\n",
      "Best Loss: 2745009.5\tCurrent LR: 0.001\tGradients Average: 7.324219009063881e-10\n",
      "Predicted Reward: 56.9850959777832\tActual Reward: 0.0\n",
      "Reward loss: 1995.1524658203125\n",
      "command_type loss: 13.800195693969727\taction1_loss: 2660366.25\taction2_loss: 82634.2578125\n",
      "Actual commands: tensor([[ 0.0000e+00,  2.1491e+03,  3.5020e+02],\n",
      "        [ 1.0000e+00, -5.6502e+02, -1.4670e+02]], device='cuda:0')\n",
      "Predicted commands: (tensor([[  0.0000, -18.3685, -19.4203],\n",
      "        [-40.1124, -27.6004,   0.0000]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-80.0316],\n",
      "        [ 28.1648]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-27.7599],\n",
      "        [  3.0070]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "49/10000\n",
      "Best Loss: 383714.9375\tCurrent LR: 0.001\tGradients Average: 2.4557113287304588e-11\n",
      "Predicted Reward: 52.437400817871094\tActual Reward: 99.75\n",
      "Reward loss: 1378.6357421875\n",
      "command_type loss: 1.5253702402114868\taction1_loss: 260015.03125\taction2_loss: 122319.765625\n",
      "Actual commands: tensor([[   1.0000,  403.5256, -454.9829],\n",
      "        [   2.0000, -572.2766,  184.3473]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-2.4496e+01, -3.0507e+00, -4.8480e-02],\n",
      "        [-8.1391e+01, -1.9668e+01,  0.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[64.3458],\n",
      "        [64.1093]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[6.7739],\n",
      "        [7.0899]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "50/10000\n",
      "Best Loss: 935598.25\tCurrent LR: 0.001\tGradients Average: -1.9073485846288207e-11\n",
      "Predicted Reward: 63.290565490722656\tActual Reward: 100.0\n",
      "Reward loss: 688.1207275390625\n",
      "command_type loss: 2.2649717266176594e-06\taction1_loss: 929556.9375\taction2_loss: 5353.1640625\n",
      "Actual commands: tensor([[    2.0000,   -71.8823,   -32.7328],\n",
      "        [    2.0000, -1297.2682,   100.6670]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-4.7642e+01, -1.2323e+01, -4.4107e-06],\n",
      "        [-9.0210e+01, -1.6247e+01, -1.1921e-07]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[59.6132],\n",
      "        [59.8695]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[4.1413],\n",
      "        [3.9891]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "51/10000\n",
      "Best Loss: 227775.078125\tCurrent LR: 0.001\tGradients Average: 0.0\n",
      "Predicted Reward: 42.1243896484375\tActual Reward: 0.0\n",
      "Reward loss: 887.251220703125\n",
      "command_type loss: 8.011682510375977\taction1_loss: 38618.0234375\taction2_loss: 188261.796875\n",
      "Actual commands: tensor([[   1.0000,  166.4255, -292.9720],\n",
      "        [   2.0000,  198.2263, -547.7859]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.1921e-07, -1.6023e+01, -3.0245e+01],\n",
      "        [-8.4793e+01, -8.8780e+00, -1.3947e-04]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-74.0238],\n",
      "        [ 58.8700]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-19.2778],\n",
      "        [  1.4091]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "52/10000\n",
      "Best Loss: 786933.4375\tCurrent LR: 0.001\tGradients Average: 0.0\n",
      "Predicted Reward: 71.95507049560547\tActual Reward: 98.375\n",
      "Reward loss: 350.8395080566406\n",
      "command_type loss: 7.45044469833374\taction1_loss: 768165.4375\taction2_loss: 18409.751953125\n",
      "Actual commands: tensor([[   2.0000, -977.8760, -115.7605],\n",
      "        [   1.0000,  740.0269, -162.9918]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.0699e+02, -1.4517e+01, -4.7684e-07],\n",
      "        [-7.4627e+01, -1.4901e+01, -3.5763e-07]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[55.2765],\n",
      "        [55.2446]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-5.8697],\n",
      "        [-5.6911]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "53/10000\n",
      "Best Loss: 475847.75\tCurrent LR: 0.001\tGradients Average: 1.220703121918021e-10\n",
      "Predicted Reward: 56.11181640625\tActual Reward: 100.0\n",
      "Reward loss: 1038.3138427734375\n",
      "command_type loss: 6.5132856369018555\taction1_loss: 380665.28125\taction2_loss: 94137.6484375\n",
      "Actual commands: tensor([[   2.0000, -257.4665,  415.5294],\n",
      "        [   2.0000, -783.2281, -110.4619]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-6.4724e+01, -6.4620e-02, -2.7714e+00],\n",
      "        [-3.8570e+01, -3.5166e-05, -1.0255e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[45.3554],\n",
      "        [35.0808]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-6.2790],\n",
      "        [-8.7125]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "54/10000\n",
      "Best Loss: 1347559.125\tCurrent LR: 0.001\tGradients Average: 3.6621095045319407e-10\n",
      "Predicted Reward: 56.994773864746094\tActual Reward: 100.0\n",
      "Reward loss: 1882.14013671875\n",
      "command_type loss: 5.3610053062438965\taction1_loss: 1330134.5\taction2_loss: 15537.1396484375\n",
      "Actual commands: tensor([[ 2.0000e+00,  1.9652e+02, -1.2405e+02],\n",
      "        [ 1.0000e+00, -1.5736e+03,  1.2771e+02]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-4.2828e+01, -2.3722e-05, -1.0648e+01],\n",
      "        [-7.8698e+01, -7.4409e-02, -2.6352e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[37.9176],\n",
      "        [49.6819]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-6.2881],\n",
      "        [-3.4633]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "55/10000\n",
      "Best Loss: 1597226.0\tCurrent LR: 0.001\tGradients Average: -1.9531249506599124e-08\n",
      "Predicted Reward: 52.33376693725586\tActual Reward: 0.0\n",
      "Reward loss: 3077.0380859375\n",
      "command_type loss: 0.22013679146766663\taction1_loss: 1377520.5\taction2_loss: 216628.1875\n",
      "Actual commands: tensor([[ 1.0000e+00,  1.0418e+03,  2.8330e+02],\n",
      "        [ 2.0000e+00,  1.3702e+03, -5.9111e+02]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-1.8122e+01, -1.1206e-05, -1.1401e+01],\n",
      "        [-8.2986e+01, -1.0325e+00, -4.4026e-01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[20.0967],\n",
      "        [62.0824]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-10.6032],\n",
      "        [ -2.1492]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "56/10000\n",
      "Best Loss: 1048483.375\tCurrent LR: 0.001\tGradients Average: -6.103515609590104e-11\n",
      "Predicted Reward: 56.45089340209961\tActual Reward: 70.0\n",
      "Reward loss: 109.59571838378906\n",
      "command_type loss: 43.474246978759766\taction1_loss: 659688.5\taction2_loss: 388641.71875\n",
      "Actual commands: tensor([[   2.0000, -822.9576,  325.0014],\n",
      "        [   0.0000,  794.0508, -821.1557]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-6.9434e+01, -1.3396e-02, -4.3195e+00],\n",
      "        [-8.2629e+01, -1.2379e-02, -4.3980e+00]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[65.5831],\n",
      "        [66.1273]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-2.9145],\n",
      "        [-2.7703]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "57/10000\n",
      "Best Loss: 486476.3125\tCurrent LR: 0.001\tGradients Average: 4.57763660310917e-10\n",
      "Predicted Reward: 51.01717758178711\tActual Reward: 74.125\n",
      "Reward loss: 2477.3193359375\n",
      "command_type loss: 2.990238904953003\taction1_loss: 192921.859375\taction2_loss: 291074.15625\n",
      "Actual commands: tensor([[   2.0000, -504.5953,  258.1838],\n",
      "        [   1.0000,   92.3691,  695.3964]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-6.8064e+01, -5.5135e+00, -4.0399e-03],\n",
      "        [-2.5411e-03, -5.9764e+00, -3.8384e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[  78.3165],\n",
      "        [-122.2412]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[  2.7302],\n",
      "        [-23.5555]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n",
      "58/10000\n",
      "Best Loss: 19198.728515625\tCurrent LR: 0.001\tGradients Average: -1.220703121918021e-10\n",
      "Predicted Reward: 66.02079772949219\tActual Reward: 70.0\n",
      "Reward loss: 143.5645751953125\n",
      "command_type loss: 2.4098458290100098\taction1_loss: 16140.9609375\taction2_loss: 2911.79296875\n",
      "Actual commands: tensor([[  0.0000,  19.4987, -86.3110],\n",
      "        [  1.0000, 191.8214,  30.2102]], device='cuda:0')\n",
      "Predicted commands: (tensor([[-4.8197e+00, -8.1021e-03, -2.7737e+01],\n",
      "        [-7.6206e+01,  0.0000e+00, -2.0880e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>), tensor([[-105.2517],\n",
      "        [  62.5184]], device='cuda:0', grad_fn=<MmBackward0>), tensor([[-18.8541],\n",
      "        [ -5.4711]], device='cuda:0', grad_fn=<MmBackward0>))\n",
      "Model saved!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20552/2984029440.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mstudy_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcommand_type_cost\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maction1_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maction2_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mreward_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mstudy_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Python\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Studying loop - Classic supervised learning. Will help Hakisa try to create certain patterns for situations and her reactions.\n",
    "\n",
    "# Can be applied to any game\n",
    "\n",
    "import os\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 100, gamma=0.1)\n",
    "\n",
    "command_type_loss = torch.nn.NLLLoss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    for i, (input_frame, label, reward) in enumerate(dataloader):\n",
    "\n",
    "        label, reward = label.to(device), reward.to(device)\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        cmds, predicted_reward = hakisa(input_frame.to(device))\n",
    "\n",
    "        del input_frame\n",
    "\n",
    "        if len(dataset.command_type) != 1:\n",
    "\n",
    "            command_type_cost = command_type_loss(cmds[0], label[:, 0].long())\n",
    "        \n",
    "        else:\n",
    "            command_type_cost = 0.\n",
    "\n",
    "        action1_loss = mse_loss(cmds[1].view(-1), label[:, 1])\n",
    "        action2_loss = mse_loss(cmds[2].view(-1), label[:, 2])\n",
    "\n",
    "        reward_loss = mse_loss(predicted_reward.view(-1), reward)\n",
    "\n",
    "        study_loss = command_type_cost + action1_loss + action2_loss + reward_loss\n",
    "\n",
    "        study_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        for n, p in hakisa.named_parameters():\n",
    "\n",
    "            if 'neuron1.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        if study_loss.item() < best_loss:\n",
    "\n",
    "            best_loss = study_loss.item()\n",
    "            best_params = hakisa.state_dict()\n",
    "\n",
    "        if i % dataset.memory_size == 0:\n",
    "            print(f\"{epoch}/{epochs}\")\n",
    "            print(f\"Best Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "            print(f\"Predicted Reward: {predicted_reward[0].item()}\\tActual Reward: {reward[0].item()}\")\n",
    "            print(f\"Reward loss: {reward_loss}\")\n",
    "            print(f\"command_type loss: {command_type_cost}\\taction1_loss: {action1_loss}\\taction2_loss: {action2_loss}\")\n",
    "            print(f\"Actual commands: {label}\")\n",
    "            print(f\"Predicted commands: {cmds}\")\n",
    "\n",
    "            if save_path is None:\n",
    "                try:\n",
    "                    os.mkdir(\"Hakisa\")\n",
    "                    save_path = \"Hakisa\"\n",
    "                except:\n",
    "                    save_path = \"Hakisa\"\n",
    "                    \n",
    "            torch.save({\n",
    "                'Epoch': epoch,\n",
    "                'Hakisa_params': best_params,\n",
    "                'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "            }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "            print(\"Model saved!\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saved! You can load it again with\n",
      "open('Hakisa_memory.pkl', 'rb') as f:\n",
      "\tdataset.memory = pickle.load(f)\n",
      "Don't forget to close the file!\n"
     ]
    }
   ],
   "source": [
    "dataset.save_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Hakisa_memory.pkl', 'rb') as f:\n",
    "\n",
    "    dataset.memory = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM: GameplayLoss is making Hakisa's actions as random as when she's in the exploration mode. We have to change that.\n",
    "\n",
    "# Perhaps consider using cumulative rewards in memory?\n",
    "\n",
    "# PROBLEM: GameplayLoss generates gradients that are way too big, making Hakisa generate outputs corresponding only to the both extremes\n",
    "# in the dictionaries (-1 and 1, -7 and 7, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameplayLoss(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gameplay Loss function. Aims to allow backpropagation through scores and, thus, allow\n",
    "    optimization aiming to achieve best gameplay performance.\n",
    "\n",
    "    Have yet to be tested. The idea is to be used during normal playthrough, after study mode.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        model_output: the output generated by the model. Necessary for backpropagation.\n",
    "        reward: the reward obtained. Simply as that. Must have the same size as model_output (try .unsqueeze(-1))\n",
    "\n",
    "    In most games, the lowest possible score one can achieve is 0, while the best score possible is infinite.\n",
    "    So we could simply define a function whose range is [0, inf[ and go on with that.\n",
    "\n",
    "    In order to avoid great numbers and possible likelihood of absurd exploding gradients, we'll use a simple log function and its derivative.\n",
    "    That way, when the performance is too low, you'll get bigger gradients to correct your weights. If it's too big, lower and perhaps vanishing gradients.\n",
    "    This can make the function bad for optimization when the performance is too good, but might give it some help when it's too low.\n",
    "    \n",
    "    Afterall, it's easier to get better when you're bad than when you're a pro.\n",
    "\n",
    "    Of course, there are many possible ways to achieve certain score.\n",
    "    In Jigoku Kisetsukan, to achieve a score of 10,000, you can kill enemies, or simply scrape through bullets without killing anyone.\n",
    "\n",
    "    However, I'm still interested in giving it a chance.\n",
    "\n",
    "    PS: Remember that, when reward < 1, log(reward) will be negative.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, model_output, reward):\n",
    "\n",
    "        print(model_output)\n",
    "\n",
    "        if reward == 0:\n",
    "            reward = 1e-10\n",
    "\n",
    "        reward = torch.tensor(reward, device=device)\n",
    "        reward = reward.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        ctx.save_for_backward(reward, model_output)\n",
    "        \n",
    "        reward = torch.log(reward) # if reward [0,1] ---> reward < 0\n",
    "\n",
    "        return reward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        print(grad_output.size())\n",
    "\n",
    "        reward, model_output = ctx.saved_tensors\n",
    "\n",
    "        # If reward is negative due to log and output is negative, gradient must be positive. If output is positive, gradient must be negative.\n",
    "\n",
    "        if reward < 0:\n",
    "\n",
    "            reward = abs(reward) # reward > 0 ---> gradient > 0\n",
    "\n",
    "        grad_cmd_type = 1/reward\n",
    "        \n",
    "        if model_output[1] < 0: # gradient must be negativa\n",
    "\n",
    "            grad_action1 = -reward\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            grad_action1 = reward\n",
    "        \n",
    "        grad_action1 = 1/grad_action1\n",
    "\n",
    "        if model_output[2] < 0:\n",
    "\n",
    "            grad_action2 = -reward\n",
    "\n",
    "        else:\n",
    "\n",
    "            grad_action2 = reward\n",
    "\n",
    "        grad_action2 = 1/grad_action2\n",
    "\n",
    "\n",
    "        #reward = 1/reward\n",
    "\n",
    "        #reward = -reward\n",
    "\n",
    "        #reward = torch.tensor(reward, device=device)\n",
    "\n",
    "        return grad_output[0] * grad_cmd_type, grad_output[1] * grad_action1, grad_output[2] * grad_action2\n",
    "\n",
    "        return grad_output * reward, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1341.4069, -1227.8274,     0.0000]], device='cuda:0')\n",
      "tensor([[-1341.4069, -1227.8274,     0.0000]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor([[-1341.4069, -1227.8274,     0.0000]], device='cuda:0')\n",
      "tensor([[-3953.3320, -3535.7322,     0.0000]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor([[-3953.3320, -3535.7322,     0.0000]], device='cuda:0')\n",
      "tensor([[-3956.9111, -3538.8691,     0.0000]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor([[-3956.9111, -3538.8691,     0.0000]], device='cuda:0')\n",
      "tensor([[-3957.0632, -3539.0024,     0.0000]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor([[-3957.0632, -3539.0024,     0.0000]], device='cuda:0')\n",
      "tensor([[-3956.9170, -3538.8740,     0.0000]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Current step: 5\n",
      "Best Loss: 22503030259712.0\tCurrent LR: 1\tGradients Average: 498293.0\n",
      "Predicted Reward: -4741257.5\tCurrent Reward: 2478.699951171875\n",
      "('key', 'Down', 'up')\n",
      "tensor([[-3956.9170, -3538.8740,     0.0000]], device='cuda:0')\n",
      "tensor([[    0.0000,  -922.5947, -7924.8667]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(3956.9170, device='cuda:0')\n",
      "tensor([[    0.0000,  -922.5947, -7924.8667]], device='cuda:0')\n",
      "tensor([[    0.0000,  -922.6086, -7924.9917]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor([[    0.0000,  -922.6086, -7924.9917]], device='cuda:0')\n",
      "tensor([[    0.0000,  -922.6079, -7924.9844]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor([[    0.0000,  -922.6079, -7924.9844]], device='cuda:0')\n",
      "tensor([[    0.0000,  -922.6055, -7924.9619]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor([[    0.0000,  -922.6055, -7924.9619]], device='cuda:0')\n",
      "tensor([[    0.0000,  -922.6106, -7925.0044]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Current step: 10\n",
      "Best Loss: 23536922001408.0\tCurrent LR: 1\tGradients Average: -1298.700927734375\n",
      "Predicted Reward: -4848926.5\tCurrent Reward: 2560.02001953125\n",
      "('key', 'Up', 'shift')\n",
      "tensor([[    0.0000,  -922.6106, -7925.0044]], device='cuda:0')\n",
      "tensor([[    0.0000,  -619.6941, -5161.2617]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Playing loop - She learns as she plays\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_learnweaponskill_Rapier_2_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "hakisa.mode = 'Play'\n",
    "\n",
    "reward = 0. # Cumulative reward\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = 'Hakisa'\n",
    "steps = 0\n",
    "save_point = 5 # Also optimization point\n",
    "uncertainty_factor = 0.9 # Also known as gamma or discount factor\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.1)\n",
    "\n",
    "action_quality_loss = torch.nn.NLLLoss()\n",
    "reward_loss = torch.nn.MSELoss()\n",
    "\n",
    "while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame)\n",
    "\n",
    "    else:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame, previous_action=(cmds[0].detach(), cmds[1].detach(), cmds[2].detach()), previous_reward=predicted_reward.detach())\n",
    "\n",
    "    #consequence = 0.0 # for testing\n",
    "\n",
    "    del frame\n",
    "\n",
    "    command = dataset.get_command(cmds[0].detach().cpu().numpy(), cmds[1].detach().cpu().numpy(), cmds[2].detach().cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    score = dataset.get_consequences(1008, 1429, 1723-1429, 1046-1008, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_Jigoku(score)\n",
    "\n",
    "    mult_score = dataset.get_consequences(933, 1536, 1723-1536, 978-933, tesseract_config='--psm 8')\n",
    "\n",
    "    mult_score = preprocess_Jigoku(mult_score)\n",
    "\n",
    "    life = dataset.get_consequences(849, 400, 498-400, 904-849, tesseract_config='--psm 8')\n",
    "\n",
    "    life = preprocess_Jigoku(life)\n",
    "\n",
    "    power = dataset.get_consequences(923, 405, 503-405, 978-923, tesseract_config='--psm 8')\n",
    "\n",
    "    power = preprocess_Jigoku(power)\n",
    "\n",
    "    aura = dataset.get_consequences(1001, 400, 1045-1001, 503-400, tesseract_config='--psm 8')\n",
    "\n",
    "    aura = preprocess_Jigoku(aura)\n",
    "    aura = aura/100\n",
    "\n",
    "    if life == 0:\n",
    "\n",
    "        try:\n",
    "            reward += -(100./(score * mult_score))\n",
    "        \n",
    "        except ZeroDivisionError:\n",
    "            reward += -10.\n",
    "\n",
    "    else:\n",
    "\n",
    "        reward += (score * mult_score) + (power * aura)\n",
    "\n",
    "    del score, mult_score, power, aura, life\n",
    "\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        previous_command_quality = command_quality.detach()\n",
    "\n",
    "    print(previous_command_quality)\n",
    "    print(command_quality)\n",
    "\n",
    "    action_quality_cost = action_quality_loss(previous_command_quality, command_quality.argmax(1).detach()) # Input = (1, Classes), Target = (1)\n",
    "\n",
    "    print(hakisa.neuron_predquality.weight.grad)\n",
    "    print(action_quality_cost)\n",
    "\n",
    "    predicted_reward = predicted_reward * uncertainty_factor\n",
    "\n",
    "    reward_cost = reward_loss(predicted_reward, reward)\n",
    "\n",
    "    gameplay_loss = action_quality_cost + reward_cost\n",
    "\n",
    "    gameplay_loss.backward()\n",
    "\n",
    "    #previous_command_quality = command_quality.clone()\n",
    "    previous_command_quality = command_quality.detach()\n",
    "\n",
    "    del command_quality\n",
    "\n",
    "    #optimizer.step()\n",
    "    #scheduler.step()\n",
    "\n",
    "    for n, p in hakisa.named_parameters():\n",
    "\n",
    "            if 'neuron1.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if gameplay_loss.item() < best_loss:\n",
    "\n",
    "        best_loss = gameplay_loss.item()\n",
    "        best_params = hakisa.state_dict()\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Best Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward}\")\n",
    "        print(command)\n",
    "\n",
    "        torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-3.6690, -1.7459, -3.5979],\n",
       "          [ 3.5231, -5.0422, -4.8364],\n",
       "          [ 3.3817,  3.5961,  3.5995]],\n",
       "\n",
       "         [[-3.0949, -3.2363, -2.9218],\n",
       "          [ 2.5006,  3.3394,  3.0426],\n",
       "          [ 2.0694, -2.1073,  1.4860]],\n",
       "\n",
       "         [[-6.0108, -6.0481, -5.8254],\n",
       "          [-3.2524, -3.0689, -3.2731],\n",
       "          [-3.8773, -3.9044, -3.8974]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4631,  1.3768,  1.5914],\n",
       "          [-1.3735, -1.0785, -0.8438],\n",
       "          [ 0.3630, -1.9996, -1.8737]],\n",
       "\n",
       "         [[ 6.1528,  4.4195,  6.3970],\n",
       "          [-0.8304, -0.7328, -0.7912],\n",
       "          [-5.9235, -1.6246, -3.9086]],\n",
       "\n",
       "         [[-3.7342, -1.5778, -1.6831],\n",
       "          [ 4.4868,  4.6847,  6.4180],\n",
       "          [-1.4684, -1.4709, -1.5685]]],\n",
       "\n",
       "\n",
       "        [[[ 3.3802,  3.2130,  3.3725],\n",
       "          [-2.1761, -2.2826, -2.0845],\n",
       "          [-2.5570, -2.6681, -2.4149]],\n",
       "\n",
       "         [[-4.4082, -4.2114, -4.1907],\n",
       "          [ 0.2591,  0.2851,  0.4691],\n",
       "          [-1.4277, -1.4191, -1.0199]],\n",
       "\n",
       "         [[-3.4173, -3.6357, -3.4503],\n",
       "          [-5.6622, -5.5387, -5.5805],\n",
       "          [-0.6209, -2.3410, -0.4317]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-3.9052, -3.8833, -3.9895],\n",
       "          [ 2.7431,  3.0758,  1.2905],\n",
       "          [ 2.3852,  2.5286,  2.5037]],\n",
       "\n",
       "         [[-3.5696, -1.6702, -3.6847],\n",
       "          [-4.3219, -4.0703, -4.1802],\n",
       "          [ 3.6069,  1.8650,  3.9042]],\n",
       "\n",
       "         [[ 5.1015,  5.1464,  5.4861],\n",
       "          [-1.6671, -1.7785, -3.4167],\n",
       "          [-2.2195, -2.5199, -4.2393]]],\n",
       "\n",
       "\n",
       "        [[[-0.9633,  0.2248,  0.3966],\n",
       "          [-3.4705, -3.4759, -3.5103],\n",
       "          [ 3.5385,  1.4235,  1.4229]],\n",
       "\n",
       "         [[ 5.3376,  3.3929,  3.5645],\n",
       "          [ 5.5697,  5.4070,  5.5657],\n",
       "          [-5.2127, -3.4519, -3.2767]],\n",
       "\n",
       "         [[ 1.8975,  3.9129,  3.8647],\n",
       "          [ 5.5998,  3.2722,  3.4701],\n",
       "          [ 5.9274,  5.7955,  5.8330]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8997,  1.8735,  1.9568],\n",
       "          [ 3.8003,  3.7850,  3.5518],\n",
       "          [-1.4718, -1.4722, -1.5371]],\n",
       "\n",
       "         [[-4.1753, -3.9603, -4.0806],\n",
       "          [ 4.1347,  4.2090,  0.5644],\n",
       "          [ 4.6028,  4.5002,  4.4996]],\n",
       "\n",
       "         [[ 1.7451,  1.8201,  1.5639],\n",
       "          [-4.3649, -4.0523, -4.1531],\n",
       "          [ 4.4407,  4.6768,  4.8287]]]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hakisa.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6338,  1.1568,  1.4793],\n",
       "        [-0.8252,  0.5828,  1.4718],\n",
       "        [ 1.3452, -1.3823, -1.1854]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hakisa.neuron_predquality.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hakisa.neuron_predquality.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BULLET HEAVEN\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_learnweaponskill_Rapier_2_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "hakisa.mode = 'Play'\n",
    "\n",
    "reward = 0. # Cumulative reward\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = 'Hakisa'\n",
    "steps = 0\n",
    "save_point = 5 # Also optimization point\n",
    "uncertainty_factor = 0.9 # Also known as gamma or discount factor\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.1)\n",
    "\n",
    "action_quality_loss = torch.nn.NLLLoss()\n",
    "reward_loss = torch.nn.MSELoss()\n",
    "\n",
    "while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame)\n",
    "\n",
    "    else:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame, previous_action=(cmds[0].detach(), cmds[1].detach(), cmds[2].detach()), previous_reward=predicted_reward.detach())\n",
    "\n",
    "    del frame\n",
    "\n",
    "    command = dataset.get_command(cmds[0].detach().cpu().numpy(), cmds[1].detach().cpu().numpy(), cmds[2].detach().cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    score = dataset.get_consequences(180, 1, 249-1, 213-180, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_BH2(score)\n",
    "\n",
    "    try:\n",
    "\n",
    "        reward += math.log10(score)\n",
    "    \n",
    "    except:\n",
    "\n",
    "        reward += 0.0\n",
    "\n",
    "    del score\n",
    "\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        previous_command_quality = command_quality.detach()\n",
    "\n",
    "    action_quality_cost = action_quality_loss(previous_command_quality, command_quality.argmax(1).detach()) # Input = (1, Classes), Target = (1)\n",
    "\n",
    "    predicted_reward = predicted_reward * uncertainty_factor\n",
    "\n",
    "    reward_cost = reward_loss(predicted_reward, reward)\n",
    "\n",
    "    gameplay_loss = action_quality_cost + reward_cost\n",
    "\n",
    "    gameplay_loss.backward()\n",
    "\n",
    "    previous_command_quality = command_quality.detach()\n",
    "\n",
    "    del command_quality\n",
    "\n",
    "    for n, p in hakisa.named_parameters():\n",
    "\n",
    "            if 'neuron1.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if gameplay_loss.item() < best_loss:\n",
    "\n",
    "        best_loss = gameplay_loss.item()\n",
    "        best_params = hakisa.state_dict()\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Best Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward}\")\n",
    "        print(command)\n",
    "\n",
    "        torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 5\n",
      "Best Loss: 136155479670784.0\tCurrent LR: 1\tGradients Average: 12130466.0\n",
      "Predicted Reward: 11668363.0\tCurrent Reward: -205.0\n",
      "('click', 1671, 70)\n",
      "Current step: 10\n",
      "Best Loss: 1142690152448.0\tCurrent LR: 1\tGradients Average: 2987.968994140625\n",
      "Predicted Reward: 1068556.875\tCurrent Reward: -410.0\n",
      "('rightclick', 497, '9')\n",
      "Current step: 15\n",
      "Best Loss: 2196848246784.0\tCurrent LR: 1\tGradients Average: 4614.93994140625\n",
      "Predicted Reward: 1481561.875\tCurrent Reward: -615.0\n",
      "('rightclick', 90, '9')\n",
      "Current step: 20\n",
      "Best Loss: 104817175298048.0\tCurrent LR: 1\tGradients Average: -13634.1591796875\n",
      "Predicted Reward: -10238846.0\tCurrent Reward: -820.0\n",
      "('rightclick', 90, '9')\n",
      "Current step: 25\n",
      "Best Loss: 322695086473216.0\tCurrent LR: 1\tGradients Average: 3500.263916015625\n",
      "Predicted Reward: 17962690.0\tCurrent Reward: -1025.0\n",
      "('rightclick', 90, '9')\n",
      "Current step: 30\n",
      "Best Loss: 8073134997504.0\tCurrent LR: 1\tGradients Average: -4506.07275390625\n",
      "Predicted Reward: -2842556.25\tCurrent Reward: -1230.0\n",
      "('click', 90, '9')\n",
      "Current step: 35\n",
      "Best Loss: 20150250110976.0\tCurrent LR: 1\tGradients Average: 1911.5909423828125\n",
      "Predicted Reward: 4487468.0\tCurrent Reward: -1435.0\n",
      "('click', 90, '9')\n",
      "Current step: 40\n",
      "Best Loss: 94331197194240.0\tCurrent LR: 1\tGradients Average: -145556.765625\n",
      "Predicted Reward: -9714065.0\tCurrent Reward: -1640.0\n",
      "('click', 90, '9')\n",
      "Current step: 45\n",
      "Best Loss: 1705311469568.0\tCurrent LR: 1\tGradients Average: -3140.357666015625\n",
      "Predicted Reward: -1307720.75\tCurrent Reward: -1845.0\n",
      "('click', 90, '9')\n",
      "Current step: 50\n",
      "Best Loss: 28872626143232.0\tCurrent LR: 1\tGradients Average: -12.311856269836426\n",
      "Predicted Reward: -5375375.5\tCurrent Reward: -2050.0\n",
      "('rightclick', 90, '9')\n",
      "Current step: 55\n",
      "Best Loss: 102291457703936.0\tCurrent LR: 1\tGradients Average: 1448.19921875\n",
      "Predicted Reward: -10116179.0\tCurrent Reward: -2255.0\n",
      "('rightclick', 90, '9')\n",
      "Current step: 60\n",
      "Best Loss: 100035198976000.0\tCurrent LR: 1\tGradients Average: 681.1259155273438\n",
      "Predicted Reward: -10004220.0\tCurrent Reward: -2460.0\n",
      "('rightclick', 90, '9')\n",
      "Current step: 65\n",
      "Best Loss: 30212353949696.0\tCurrent LR: 1\tGradients Average: 28.10895538330078\n",
      "Predicted Reward: -5499241.5\tCurrent Reward: -2665.0\n",
      "('rightclick', 90, '9')\n",
      "Current step: 70\n",
      "Best Loss: 704952795136.0\tCurrent LR: 1\tGradients Average: -6615.1064453125\n",
      "Predicted Reward: 836744.6875\tCurrent Reward: -2870.0\n",
      "('rightclick', 90, '9')\n",
      "Current step: 75\n",
      "Best Loss: 42829048971264.0\tCurrent LR: 1\tGradients Average: 5063.48388671875\n",
      "Predicted Reward: -6547465.5\tCurrent Reward: -3075.0\n",
      "('key', 90, '9')\n",
      "Current step: 80\n",
      "Best Loss: 93075607126016.0\tCurrent LR: 1\tGradients Average: -2844.41455078125\n",
      "Predicted Reward: 9644290.0\tCurrent Reward: -3280.0\n",
      "('key', 90, '9')\n",
      "Current step: 85\n",
      "Best Loss: 22372431872.0\tCurrent LR: 1\tGradients Average: 1337.6002197265625\n",
      "Predicted Reward: -153059.171875\tCurrent Reward: -3485.0\n",
      "('key', 90, '9')\n",
      "Current step: 90\n",
      "Best Loss: 3792116121600.0\tCurrent LR: 1\tGradients Average: 6290.0263671875\n",
      "Predicted Reward: 1943645.625\tCurrent Reward: -3690.0\n",
      "('key', 90, '9')\n",
      "Current step: 95\n",
      "Best Loss: 2393494519808.0\tCurrent LR: 1\tGradients Average: 670.9833984375\n",
      "Predicted Reward: -1550987.25\tCurrent Reward: -3895.0\n",
      "('key', 'press', '9')\n",
      "Current step: 100\n",
      "Best Loss: 14896730210304.0\tCurrent LR: 1\tGradients Average: 1955.6669921875\n",
      "Predicted Reward: -3863728.25\tCurrent Reward: -4100.0\n",
      "('key', 'press', '9')\n",
      "Current step: 105\n",
      "Best Loss: 1111462510592.0\tCurrent LR: 1\tGradients Average: 1699.257568359375\n",
      "Predicted Reward: -1058564.25\tCurrent Reward: -4305.0\n",
      "('key', 'press', '9')\n",
      "Current step: 110\n",
      "Best Loss: 12332706037760.0\tCurrent LR: 1\tGradients Average: 1436.8260498046875\n",
      "Predicted Reward: 3507285.25\tCurrent Reward: -4510.0\n",
      "('key', 90, '9')\n",
      "Current step: 115\n",
      "Best Loss: 834168946688.0\tCurrent LR: 1\tGradients Average: 20136.0234375\n",
      "Predicted Reward: 908613.5\tCurrent Reward: -4715.0\n",
      "('key', 90, '9')\n",
      "Current step: 120\n",
      "Best Loss: 2113963294720.0\tCurrent LR: 1\tGradients Average: -475.3100280761719\n",
      "Predicted Reward: -1458867.5\tCurrent Reward: -4920.0\n",
      "('key', 308, '9')\n",
      "Current step: 125\n",
      "Best Loss: 7113050423296.0\tCurrent LR: 1\tGradients Average: -90.62673950195312\n",
      "Predicted Reward: 2661905.25\tCurrent Reward: -5125.0\n",
      "('key', 1316, '9')\n",
      "Current step: 130\n",
      "Best Loss: 8640329678848.0\tCurrent LR: 1\tGradients Average: -104.28702545166016\n",
      "Predicted Reward: -2944773.75\tCurrent Reward: -5330.0\n",
      "('key', 776, '9')\n",
      "Current step: 135\n",
      "Best Loss: 4281991168000.0\tCurrent LR: 1\tGradients Average: 1136.6324462890625\n",
      "Predicted Reward: 2063762.25\tCurrent Reward: -5535.0\n",
      "('key', 1294, '9')\n",
      "Current step: 140\n",
      "Best Loss: 1774810824704.0\tCurrent LR: 1\tGradients Average: -3645.081787109375\n",
      "Predicted Reward: -1337960.25\tCurrent Reward: -5740.0\n",
      "('key', 1188, '9')\n",
      "Current step: 145\n",
      "Best Loss: 1590671179776.0\tCurrent LR: 1\tGradients Average: 1503.5673828125\n",
      "Predicted Reward: 1255273.125\tCurrent Reward: -5945.0\n",
      "('key', 1248, '9')\n",
      "Current step: 150\n",
      "Best Loss: 119680188416.0\tCurrent LR: 1\tGradients Average: 1290.7760009765625\n",
      "Predicted Reward: 339798.25\tCurrent Reward: -6150.0\n",
      "('key', 1442, '9')\n",
      "Current step: 155\n",
      "Best Loss: 2030857617408.0\tCurrent LR: 1\tGradients Average: -3432.583251953125\n",
      "Predicted Reward: -1431436.625\tCurrent Reward: -6355.0\n",
      "('key', 1500, '9')\n",
      "Current step: 160\n",
      "Best Loss: 1764720902144.0\tCurrent LR: 1\tGradients Average: -2628.8173828125\n",
      "Predicted Reward: -1334988.0\tCurrent Reward: -6560.0\n",
      "('key', 1538, '9')\n",
      "Current step: 165\n",
      "Best Loss: 3042229354496.0\tCurrent LR: 1\tGradients Average: 2364.17333984375\n",
      "Predicted Reward: 1737433.75\tCurrent Reward: -6765.0\n",
      "('key', 1704, '9')\n",
      "Current step: 170\n",
      "Best Loss: 3541013626880.0\tCurrent LR: 1\tGradients Average: 1636.20458984375\n",
      "Predicted Reward: 1874788.125\tCurrent Reward: -6970.0\n",
      "('key', 1761, '9')\n",
      "Current step: 175\n",
      "Best Loss: 161877999616.0\tCurrent LR: 1\tGradients Average: 1578.6646728515625\n",
      "Predicted Reward: 395165.65625\tCurrent Reward: -7175.0\n",
      "('key', 1853, '9')\n",
      "Current step: 180\n",
      "Best Loss: 7093212413952.0\tCurrent LR: 1\tGradients Average: -4215.34375\n",
      "Predicted Reward: -2670688.5\tCurrent Reward: -7380.0\n",
      "('key', 1772, '9')\n",
      "Current step: 185\n",
      "Best Loss: 2006322118656.0\tCurrent LR: 1\tGradients Average: -1906.047119140625\n",
      "Predicted Reward: -1424032.0\tCurrent Reward: -7585.0\n",
      "('key', 1788, '9')\n",
      "Current step: 190\n",
      "Best Loss: 2521076596736.0\tCurrent LR: 1\tGradients Average: 456.51416015625\n",
      "Predicted Reward: 1579999.875\tCurrent Reward: -7790.0\n",
      "('key', 'press', '9')\n",
      "Current step: 195\n",
      "Best Loss: 4795144339456.0\tCurrent LR: 1\tGradients Average: -619.874755859375\n",
      "Predicted Reward: 2181786.75\tCurrent Reward: -7995.0\n",
      "('key', 'press', '9')\n",
      "Current step: 200\n",
      "Best Loss: 1107559972864.0\tCurrent LR: 1\tGradients Average: -455.9424743652344\n",
      "Predicted Reward: 1044206.75\tCurrent Reward: -8200.0\n",
      "('key', 'press', '9')\n",
      "Current step: 205\n",
      "Best Loss: 462863892480.0\tCurrent LR: 1\tGradients Average: -75.0699691772461\n",
      "Predicted Reward: -688746.0\tCurrent Reward: -8405.0\n",
      "('key', 'press', '9')\n",
      "Current step: 210\n",
      "Best Loss: 2812792274944.0\tCurrent LR: 1\tGradients Average: 169.4700164794922\n",
      "Predicted Reward: -1685748.125\tCurrent Reward: -8610.0\n",
      "('key', 'press', '9')\n",
      "Current step: 215\n",
      "Best Loss: 1107700482048.0\tCurrent LR: 1\tGradients Average: 726.3309326171875\n",
      "Predicted Reward: -1061288.5\tCurrent Reward: -8815.0\n",
      "('key', 'press', '9')\n",
      "Current step: 220\n",
      "Best Loss: 335892316160.0\tCurrent LR: 1\tGradients Average: -6889.12646484375\n",
      "Predicted Reward: 570542.1875\tCurrent Reward: -9020.0\n",
      "('key', 'press', '9')\n",
      "Current step: 225\n",
      "Best Loss: 423937081344.0\tCurrent LR: 1\tGradients Average: -3403.516845703125\n",
      "Predicted Reward: 641879.5\tCurrent Reward: -9225.0\n",
      "('key', 'press', '9')\n",
      "Current step: 230\n",
      "Best Loss: 997391073280.0\tCurrent LR: 1\tGradients Average: -4578.572265625\n",
      "Predicted Reward: 989264.6875\tCurrent Reward: -9430.0\n",
      "('key', 'press', '9')\n",
      "Current step: 235\n",
      "Best Loss: 371872727040.0\tCurrent LR: 1\tGradients Average: -4098.5\n",
      "Predicted Reward: 600178.6875\tCurrent Reward: -9635.0\n",
      "('key', 'press', '9')\n",
      "Current step: 240\n",
      "Best Loss: 1016666390528.0\tCurrent LR: 1\tGradients Average: -56959.58203125\n",
      "Predicted Reward: 998458.75\tCurrent Reward: -9840.0\n",
      "('key', 'press', '9')\n",
      "Current step: 245\n",
      "Best Loss: 199226851328.0\tCurrent LR: 1\tGradients Average: -938.4904174804688\n",
      "Predicted Reward: -456393.34375\tCurrent Reward: -10045.0\n",
      "('key', 'press', 769)\n",
      "Current step: 250\n",
      "Best Loss: 177408278528.0\tCurrent LR: 1\tGradients Average: 1140.7569580078125\n",
      "Predicted Reward: 410948.625\tCurrent Reward: -10250.0\n",
      "('key', 'press', '9')\n",
      "Current step: 255\n",
      "Best Loss: 54016585728.0\tCurrent LR: 1\tGradients Average: 2118.00146484375\n",
      "Predicted Reward: 221959.6875\tCurrent Reward: -10455.0\n",
      "('key', 'press', '9')\n",
      "Current step: 260\n",
      "Best Loss: 18428377088.0\tCurrent LR: 1\tGradients Average: 1763.5506591796875\n",
      "Predicted Reward: -146411.15625\tCurrent Reward: -10660.0\n",
      "('key', 'press', 906)\n",
      "Current step: 265\n",
      "Best Loss: 1557954822144.0\tCurrent LR: 1\tGradients Average: -8236.26171875\n",
      "Predicted Reward: -1259045.625\tCurrent Reward: -10865.0\n",
      "('key', 'press', 552)\n",
      "Current step: 270\n",
      "Best Loss: 182399991808.0\tCurrent LR: 1\tGradients Average: -2515.801513671875\n",
      "Predicted Reward: 416013.125\tCurrent Reward: -11070.0\n",
      "('key', 'press', 851)\n",
      "Current step: 275\n",
      "Best Loss: 58048061440.0\tCurrent LR: 1\tGradients Average: 1342.33056640625\n",
      "Predicted Reward: 229656.65625\tCurrent Reward: -11275.0\n",
      "('key', 'press', 826)\n",
      "Current step: 280\n",
      "Best Loss: 50261331968.0\tCurrent LR: 1\tGradients Average: 827.5081787109375\n",
      "Predicted Reward: -235670.390625\tCurrent Reward: -11480.0\n",
      "('key', 'press', 670)\n",
      "Current step: 285\n",
      "Best Loss: 304589045760.0\tCurrent LR: 1\tGradients Average: -3846.035400390625\n",
      "Predicted Reward: 540210.875\tCurrent Reward: -11685.0\n",
      "('key', 'press', 790)\n",
      "Current step: 290\n",
      "Best Loss: 51453976576.0\tCurrent LR: 1\tGradients Average: -3054.791748046875\n",
      "Predicted Reward: -238724.6875\tCurrent Reward: -11890.0\n",
      "('key', 'press', 635)\n",
      "Current step: 295\n",
      "Best Loss: 17127418880.0\tCurrent LR: 1\tGradients Average: -13112.3759765625\n",
      "Predicted Reward: 118776.765625\tCurrent Reward: -12095.0\n",
      "('key', 'press', 704)\n",
      "Current step: 300\n",
      "Best Loss: 363656347648.0\tCurrent LR: 1\tGradients Average: -3083.0576171875\n",
      "Predicted Reward: -615339.25\tCurrent Reward: -12300.0\n",
      "('key', 'press', 631)\n",
      "Current step: 305\n",
      "Best Loss: 1042585223168.0\tCurrent LR: 1\tGradients Average: 4328.12451171875\n",
      "Predicted Reward: -1033575.625\tCurrent Reward: -12505.0\n",
      "('key', 'press', 583)\n",
      "Current step: 310\n",
      "Best Loss: 306792464384.0\tCurrent LR: 1\tGradients Average: 3282.41552734375\n",
      "Predicted Reward: -566598.5\tCurrent Reward: -12710.0\n",
      "('key', 'press', 613)\n",
      "Current step: 315\n",
      "Best Loss: 334388690944.0\tCurrent LR: 1\tGradients Average: -5107.9765625\n",
      "Predicted Reward: 565348.5\tCurrent Reward: -12915.0\n",
      "('key', 'press', 721)\n",
      "Current step: 320\n",
      "Best Loss: 635488763904.0\tCurrent LR: 1\tGradients Average: -4960.19189453125\n",
      "Predicted Reward: 784055.5\tCurrent Reward: -13120.0\n",
      "('key', 'press', 732)\n",
      "Current step: 325\n",
      "Best Loss: 174421442560.0\tCurrent LR: 1\tGradients Average: -3677.181884765625\n",
      "Predicted Reward: 404312.9375\tCurrent Reward: -13325.0\n",
      "('key', 'press', 702)\n",
      "Current step: 330\n",
      "Best Loss: 8215098368.0\tCurrent LR: 1\tGradients Average: 2265.1708984375\n",
      "Predicted Reward: -104167.1796875\tCurrent Reward: -13530.0\n",
      "('key', 'press', 669)\n",
      "Current step: 335\n",
      "Best Loss: 357424988160.0\tCurrent LR: 1\tGradients Average: 5032.6845703125\n",
      "Predicted Reward: -611585.3125\tCurrent Reward: -13735.0\n",
      "('key', 'press', 666)\n",
      "Current step: 340\n",
      "Best Loss: 185077956608.0\tCurrent LR: 1\tGradients Average: 2605.559326171875\n",
      "Predicted Reward: -444146.875\tCurrent Reward: -13940.0\n",
      "('key', 'press', 689)\n",
      "Current step: 345\n",
      "Best Loss: 2909304064.0\tCurrent LR: 1\tGradients Average: 1590.1705322265625\n",
      "Predicted Reward: 39792.96484375\tCurrent Reward: -14145.0\n",
      "('key', 'press', 727)\n",
      "Current step: 350\n",
      "Best Loss: 321802993664.0\tCurrent LR: 1\tGradients Average: -817.8452758789062\n",
      "Predicted Reward: 552926.8125\tCurrent Reward: -14350.0\n",
      "('key', 'press', 771)\n",
      "Current step: 355\n",
      "Best Loss: 83795230720.0\tCurrent LR: 1\tGradients Average: -2313.21728515625\n",
      "Predicted Reward: -304029.0625\tCurrent Reward: -14555.0\n",
      "('key', 'press', 577)\n",
      "Current step: 360\n",
      "Best Loss: 1016972902400.0\tCurrent LR: 1\tGradients Average: -4262.83056640625\n",
      "Predicted Reward: 993690.75\tCurrent Reward: -14760.0\n",
      "('key', 'press', 806)\n",
      "Current step: 365\n",
      "Best Loss: 311309959168.0\tCurrent LR: 1\tGradients Average: 921.1334228515625\n",
      "Predicted Reward: -572916.5625\tCurrent Reward: -14965.0\n",
      "('key', 'press', 568)\n",
      "Current step: 370\n",
      "Best Loss: 77623656448.0\tCurrent LR: 1\tGradients Average: 1844.8031005859375\n",
      "Predicted Reward: 263440.21875\tCurrent Reward: -15170.0\n",
      "('key', 'press', 796)\n",
      "Current step: 375\n",
      "Best Loss: 358073729024.0\tCurrent LR: 1\tGradients Average: 4767.1474609375\n",
      "Predicted Reward: -613817.625\tCurrent Reward: -15425.0\n",
      "('key', 'press', 614)\n",
      "Current step: 380\n",
      "Best Loss: 34366881792.0\tCurrent LR: 1\tGradients Average: 1318.26318359375\n",
      "Predicted Reward: 169753.0625\tCurrent Reward: -15630.0\n",
      "('key', 'press', 790)\n",
      "Current step: 385\n",
      "Best Loss: 50340405248.0\tCurrent LR: 1\tGradients Average: -1151.61328125\n",
      "Predicted Reward: -240201.671875\tCurrent Reward: -15835.0\n",
      "('key', 'press', 674)\n",
      "Current step: 390\n",
      "Best Loss: 473570050048.0\tCurrent LR: 1\tGradients Average: -8841.7001953125\n",
      "Predicted Reward: 672124.25\tCurrent Reward: -16040.0\n",
      "('key', 'press', 777)\n",
      "Current step: 395\n",
      "Best Loss: 6041020416.0\tCurrent LR: 1\tGradients Average: -2389.903564453125\n",
      "Predicted Reward: -93969.0\tCurrent Reward: -16245.0\n",
      "('key', 'press', 732)\n",
      "Current step: 400\n",
      "Best Loss: 8048344064.0\tCurrent LR: 1\tGradients Average: 1806.6119384765625\n",
      "Predicted Reward: -106162.5625\tCurrent Reward: -16450.0\n",
      "('key', 'press', 761)\n",
      "Current step: 405\n",
      "Best Loss: 134616645632.0\tCurrent LR: 1\tGradients Average: 3210.789794921875\n",
      "Predicted Reward: -383556.40625\tCurrent Reward: -16655.0\n",
      "('key', 'press', 756)\n",
      "Current step: 410\n",
      "Best Loss: 81420910592.0\tCurrent LR: 1\tGradients Average: 2125.037841796875\n",
      "Predicted Reward: -302203.5\tCurrent Reward: -16860.0\n",
      "('key', 'press', 760)\n",
      "Current step: 415\n",
      "Best Loss: 34581282816.0\tCurrent LR: 1\tGradients Average: -2028.2139892578125\n",
      "Predicted Reward: 168895.4375\tCurrent Reward: -17065.0\n",
      "('key', 'press', 811)\n",
      "Current step: 420\n",
      "Best Loss: 113246658560.0\tCurrent LR: 1\tGradients Average: -2704.235107421875\n",
      "Predicted Reward: 319251.40625\tCurrent Reward: -17270.0\n",
      "('key', 'press', 834)\n",
      "Current step: 425\n",
      "Best Loss: 52696264704.0\tCurrent LR: 1\tGradients Average: -2073.041748046875\n",
      "Predicted Reward: 212081.671875\tCurrent Reward: -17475.0\n",
      "('key', 'press', 831)\n",
      "Current step: 430\n",
      "Best Loss: 72187330560.0\tCurrent LR: 1\tGradients Average: 2318.975341796875\n",
      "Predicted Reward: -286357.0\tCurrent Reward: -17680.0\n",
      "('key', 'press', 778)\n",
      "Current step: 435\n",
      "Best Loss: 119237722112.0\tCurrent LR: 1\tGradients Average: 2091.244873046875\n",
      "Predicted Reward: -363193.15625\tCurrent Reward: -17885.0\n",
      "('key', 'press', 782)\n",
      "Current step: 440\n",
      "Best Loss: 30104614912.0\tCurrent LR: 1\tGradients Average: 1588.4866943359375\n",
      "Predicted Reward: -191596.8125\tCurrent Reward: -18090.0\n",
      "('key', 'press', 795)\n",
      "Current step: 445\n",
      "Best Loss: 41887322112.0\tCurrent LR: 1\tGradients Average: -1858.193115234375\n",
      "Predicted Reward: 186368.921875\tCurrent Reward: -18295.0\n",
      "('key', 'press', 837)\n",
      "Current step: 450\n",
      "Best Loss: 73158516736.0\tCurrent LR: 1\tGradients Average: -2119.846923828125\n",
      "Predicted Reward: 251978.3125\tCurrent Reward: -18500.0\n",
      "('key', 'press', 844)\n",
      "Current step: 455\n",
      "Best Loss: 7956861440.0\tCurrent LR: 1\tGradients Average: -977.7720336914062\n",
      "Predicted Reward: 70496.2421875\tCurrent Reward: -18705.0\n",
      "('key', 'press', 810)\n",
      "Current step: 460\n",
      "Best Loss: 5436578304.0\tCurrent LR: 1\tGradients Average: 1246.9371337890625\n",
      "Predicted Reward: -92643.15625\tCurrent Reward: -18910.0\n",
      "('key', 'press', 811)\n",
      "Current step: 465\n",
      "Best Loss: 15180604416.0\tCurrent LR: 1\tGradients Average: 1475.9949951171875\n",
      "Predicted Reward: -142324.59375\tCurrent Reward: -19115.0\n",
      "('key', 'press', 828)\n",
      "Current step: 470\n",
      "Best Loss: 10118094848.0\tCurrent LR: 1\tGradients Average: 1140.6773681640625\n",
      "Predicted Reward: -119908.7421875\tCurrent Reward: -19320.0\n",
      "('key', 'press', 832)\n",
      "Current step: 475\n",
      "Best Loss: 2988784128.0\tCurrent LR: 1\tGradients Average: 1794.94482421875\n",
      "Predicted Reward: 35144.7734375\tCurrent Reward: -19525.0\n",
      "('key', 'press', 867)\n",
      "Current step: 480\n",
      "Best Loss: 9225627648.0\tCurrent LR: 1\tGradients Average: -168.49041748046875\n",
      "Predicted Reward: -115780.1328125\tCurrent Reward: -19730.0\n",
      "('key', 'press', 799)\n",
      "Current step: 485\n",
      "Best Loss: 20755732480.0\tCurrent LR: 1\tGradients Average: -1223.2137451171875\n",
      "Predicted Reward: 124133.4921875\tCurrent Reward: -19935.0\n",
      "('key', 'press', 824)\n"
     ]
    }
   ],
   "source": [
    "# ETERNAL RETURN\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_learnweaponskill_Rapier_2_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "hakisa.mode = 'Play'\n",
    "\n",
    "reward = 0. # Cumulative reward\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = 'Hakisa'\n",
    "steps = 0\n",
    "save_point = 5 # Also optimization point\n",
    "uncertainty_factor = 0.9 # Also known as gamma or discount factor\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, gamma=0.1)\n",
    "\n",
    "action_quality_loss = torch.nn.NLLLoss()\n",
    "reward_loss = torch.nn.MSELoss()\n",
    "\n",
    "while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame)\n",
    "\n",
    "    else:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame, previous_action=(cmds[0].detach(), cmds[1].detach(), cmds[2].detach()), previous_reward=predicted_reward.detach())\n",
    "\n",
    "    del frame\n",
    "\n",
    "    command = dataset.get_command(cmds[0].detach().cpu().numpy(), cmds[1].detach().cpu().numpy(), cmds[2].detach().cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    hp = dataset.get_consequences(1036, 845, 908-845, 1050-1036, togray=True, threshold=True, thresh_gauss=7, thresh_C=-13, tesseract_config='--psm 6')\n",
    "\n",
    "    hp = preprocess_ER(hp)\n",
    "    hp = hp.split('/')\n",
    "\n",
    "    try:\n",
    "        current_hp, max_hp = hp[0], hp[1]\n",
    "    \n",
    "    except IndexError:\n",
    "\n",
    "        current_hp, max_hp = hp[0], hp[0]\n",
    "\n",
    "    del hp\n",
    "\n",
    "    try:\n",
    "        current_hp, max_hp = float(current_hp), float(max_hp)\n",
    "\n",
    "    except ValueError:\n",
    "        current_hp, max_hp = 0.0, 1000.0\n",
    "\n",
    "    # No Need for thresholding/grayscale here:\n",
    "\n",
    "    team_kills = dataset.get_consequences(3, 1611, 1661-1611, 35-3, tesseract_config='--psm 6')\n",
    "\n",
    "    team_kills = preprocess_ER(team_kills).replace(\"TK\",'')\n",
    "\n",
    "    try:\n",
    "        team_kills = float(team_kills)\n",
    "\n",
    "    except ValueError:\n",
    "        team_kills = 0\n",
    "\n",
    "    # Usando apenas team kills, pois no modo solo --> solo kill = team kill. No Cobalt, kills pessoais so menos relevantes q kills do time.\n",
    "\n",
    "    assists = dataset.get_consequences(3, 1711, 1761-1711, 35-3, tesseract_config='--psm 6')\n",
    "\n",
    "    assists = preprocess_ER(assists).replace(\"A\", '')\n",
    "\n",
    "    try:\n",
    "        assists = float(assists)\n",
    "\n",
    "    except ValueError:\n",
    "        assists = 0\n",
    "\n",
    "    farm = dataset.get_consequences(3, 1761, 1821-1761, 35-3, tesseract_config='--psm 6')\n",
    "\n",
    "    farm = preprocess_ER(farm).replace(\"H\", '')\n",
    "\n",
    "    try:\n",
    "        farm = float(farm)\n",
    "\n",
    "    except ValueError:\n",
    "        farm = 0\n",
    "    \n",
    "    farm = farm/4.\n",
    "\n",
    "    if current_hp > 0:\n",
    "\n",
    "        reward += (farm+team_kills+assists)*(current_hp/max_hp)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        countdown = dataset.get_consequences(56, 961, 990-961, 79-56, tesseract_config='--psm 6')\n",
    "        countdown = preprocess_ER(countdown)\n",
    "        try:\n",
    "            countdown = float(countdown)\n",
    "        \n",
    "        except ValueError:\n",
    "            countdown = 0\n",
    "\n",
    "        reward += (-1*(90-countdown)) - (1/(team_kills+assists+farm+1))\n",
    "\n",
    "        del countdown\n",
    "\n",
    "    del current_hp, max_hp, team_kills, assists, farm\n",
    "\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        previous_command_quality = command_quality.detach()\n",
    "\n",
    "    action_quality_cost = action_quality_loss(previous_command_quality, command_quality.argmax(1).detach()) # Input = (1, Classes), Target = (1)\n",
    "\n",
    "    predicted_reward = predicted_reward * uncertainty_factor\n",
    "\n",
    "    reward_cost = reward_loss(predicted_reward, reward)\n",
    "\n",
    "    gameplay_loss = action_quality_cost + reward_cost\n",
    "\n",
    "    gameplay_loss.backward()\n",
    "\n",
    "    previous_command_quality = command_quality.detach()\n",
    "\n",
    "    del command_quality\n",
    "\n",
    "    for n, p in hakisa.named_parameters():\n",
    "\n",
    "            if 'neuron1.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if gameplay_loss.item() < best_loss:\n",
    "\n",
    "        best_loss = gameplay_loss.item()\n",
    "        best_params = hakisa.state_dict()\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Best Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward}\")\n",
    "        print(command)\n",
    "\n",
    "        torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron_predreward1.weight\n",
      "tensor([[-2.5335e-05, -1.4189e-05, -3.6957e-05, -9.3982e-06, -9.3094e-06],\n",
      "        [ 1.2878e-04,  7.2122e-05,  1.8785e-04,  4.7772e-05,  4.7320e-05],\n",
      "        [ 1.2066e-04,  6.7577e-05,  1.7602e-04,  4.4761e-05,  4.4338e-05],\n",
      "        ...,\n",
      "        [-6.3327e-05, -3.5465e-05, -9.2376e-05, -2.3491e-05, -2.3269e-05],\n",
      "        [ 1.2434e-04,  6.9637e-05,  1.8138e-04,  4.6126e-05,  4.5690e-05],\n",
      "        [ 1.9346e-05,  1.0835e-05,  2.8221e-05,  7.1765e-06,  7.1087e-06]],\n",
      "       device='cuda:0')\n",
      "neuron_predreward2.weight\n",
      "tensor([[ 4.0096e-03,  1.4042e-03,  1.5049e-03,  1.4725e-03,  2.4419e-03,\n",
      "         -2.8749e-04, -5.2500e-04,  1.1663e-03,  3.7031e-03,  1.5295e-03,\n",
      "         -1.8508e-03, -4.8694e-04, -2.4012e-03,  4.1435e-03, -2.0085e-03,\n",
      "          2.5349e-04,  1.8503e-03, -1.4948e-03, -4.2718e-03, -7.0859e-05,\n",
      "          1.2085e-03, -2.1850e-03, -2.6786e-04, -3.4396e-03,  3.1064e-03,\n",
      "          3.9939e-03, -3.1919e-03, -2.4984e-03, -8.6820e-04,  4.6796e-04,\n",
      "         -1.4776e-03,  2.8958e-03, -1.9329e-03,  2.6890e-04, -7.0542e-04,\n",
      "         -4.3297e-05, -1.1443e-05,  3.1209e-03, -4.4129e-03,  5.0326e-03,\n",
      "          1.4617e-03,  1.0872e-03,  6.4396e-04, -3.1135e-03, -7.4372e-04,\n",
      "          5.6964e-04, -1.1685e-03,  7.2176e-05,  1.7685e-03, -1.2731e-03,\n",
      "         -3.4242e-04, -5.4949e-04, -2.5923e-03,  1.0252e-03, -2.0228e-03,\n",
      "          4.1687e-04,  3.3270e-03, -3.2142e-03, -3.4647e-03,  1.4472e-03,\n",
      "         -1.2503e-04, -3.1881e-03, -1.9701e-03, -3.2707e-03,  2.4830e-03,\n",
      "         -3.0520e-03,  2.4817e-03, -1.5821e-03, -4.1165e-03, -1.6050e-03,\n",
      "          2.4688e-03, -2.0594e-03,  1.3250e-04,  5.3225e-04,  2.6234e-03,\n",
      "         -2.3179e-03,  2.9529e-03,  1.4476e-03, -1.3935e-03,  2.9453e-03,\n",
      "          4.2294e-03,  1.9571e-03,  1.2339e-03,  4.4295e-04,  3.3958e-03,\n",
      "          2.4409e-04, -1.5280e-03,  2.7855e-04,  3.4477e-03, -2.8328e-03,\n",
      "         -5.1854e-04,  6.8318e-04,  3.0838e-03, -1.2834e-03, -2.1750e-03,\n",
      "          1.7902e-03, -8.3015e-04, -2.6485e-03, -1.9596e-03, -2.4909e-03,\n",
      "          1.3793e-04,  1.3258e-03,  4.0526e-04, -3.5184e-03,  1.2445e-03,\n",
      "         -1.2525e-03, -6.7068e-04, -4.3842e-03, -1.8989e-03,  5.1422e-03,\n",
      "          8.6924e-04, -2.6793e-03,  7.5836e-04,  6.3462e-04, -3.0191e-03,\n",
      "         -2.4545e-03, -1.1160e-03,  1.0577e-03, -2.6267e-03, -1.4691e-03,\n",
      "         -3.8127e-03,  2.3677e-03,  4.9137e-04,  1.0957e-03, -2.7044e-03,\n",
      "          1.0180e-03, -1.2258e-04,  4.2109e-03,  1.7101e-03,  3.8377e-03,\n",
      "         -3.2347e-03,  4.9939e-03, -1.7878e-04,  2.7431e-03, -3.0205e-03,\n",
      "         -2.2330e-03, -8.5189e-04,  1.2668e-03,  3.6816e-03,  2.5397e-03,\n",
      "          1.5182e-03,  4.9243e-04, -2.1151e-03, -5.2181e-04,  1.2053e-04,\n",
      "          2.8405e-03,  3.6998e-03,  1.9080e-03, -1.8650e-03,  4.0096e-03,\n",
      "         -6.9950e-04, -5.7553e-04,  1.7317e-03,  2.2493e-03, -7.0656e-04,\n",
      "         -5.6224e-04, -3.5137e-03, -1.2773e-04,  2.4233e-03, -6.7352e-04,\n",
      "         -2.1536e-03,  4.4649e-04,  1.1524e-03,  4.1888e-04,  1.6778e-03,\n",
      "          1.8813e-03,  1.4888e-03,  3.9242e-03, -1.6596e-04,  1.0023e-03,\n",
      "         -5.1198e-04, -1.6279e-03, -1.9795e-03, -4.7310e-05,  2.7854e-04,\n",
      "         -3.1233e-03, -2.4342e-03, -2.1099e-03,  1.7686e-04, -9.7093e-04,\n",
      "          4.5859e-04, -2.7235e-04,  1.4355e-03,  3.3267e-03,  4.1257e-04,\n",
      "          2.7711e-03,  7.8800e-04, -4.3029e-04,  6.4604e-04, -2.4126e-03,\n",
      "         -1.5888e-03,  2.1502e-04,  3.7424e-03, -2.5971e-03,  1.0905e-03,\n",
      "          3.0312e-03,  3.0858e-04,  2.2182e-03,  1.7890e-04,  1.4776e-04,\n",
      "          4.0195e-03, -2.1484e-03, -5.9960e-06, -2.0504e-03,  1.6121e-03,\n",
      "         -2.2464e-03, -3.1628e-04, -3.7152e-03,  2.9848e-03,  5.1126e-04,\n",
      "          2.8003e-03,  1.0067e-03,  2.4582e-04,  9.6021e-04, -1.1261e-03,\n",
      "          8.5422e-05, -1.0333e-03, -1.1406e-03,  2.5468e-04, -2.0051e-03,\n",
      "          8.5061e-04,  2.9718e-05, -7.9365e-04, -1.4120e-03, -3.0922e-03,\n",
      "          5.6882e-04, -3.8147e-03,  9.9697e-04, -1.9944e-03,  2.7173e-04,\n",
      "          4.4870e-04,  9.7577e-05,  2.3017e-03,  2.6044e-05, -1.0842e-04,\n",
      "          2.8875e-03,  3.1478e-03, -5.8972e-04,  1.4637e-03,  2.0844e-03,\n",
      "          3.4300e-03,  3.5599e-03, -2.6557e-03,  2.8192e-03,  1.5735e-04,\n",
      "          4.3531e-03, -1.1053e-04,  2.2788e-03,  2.8702e-03, -2.4567e-03,\n",
      "          3.4003e-03, -8.0638e-04, -6.7188e-04,  2.5842e-03,  2.2171e-03,\n",
      "         -4.5119e-03,  2.1040e-03, -2.8300e-04,  7.2951e-04, -3.3266e-03,\n",
      "          9.5746e-04,  4.1150e-03, -2.7136e-03,  4.2940e-03,  9.8196e-04,\n",
      "          3.0296e-04, -2.4525e-03,  9.6717e-04,  1.8772e-03,  6.7881e-04,\n",
      "          1.6155e-03, -1.4631e-03,  1.5745e-03, -3.2917e-03,  1.6344e-03,\n",
      "         -9.5485e-05,  3.5099e-04,  1.7504e-03, -4.8857e-04,  6.9714e-04,\n",
      "          1.3065e-03,  6.2556e-04,  6.3717e-05,  3.6443e-03,  3.8659e-03,\n",
      "         -3.6801e-03, -1.3164e-04,  1.3716e-03, -3.3479e-03, -2.9420e-04,\n",
      "         -4.4187e-03, -4.5550e-04,  1.2898e-03,  9.7569e-04, -5.8379e-04,\n",
      "          1.5924e-03,  8.2145e-04,  1.0776e-03,  1.3482e-03,  1.6938e-03,\n",
      "         -1.1611e-03,  1.0235e-03,  3.2649e-03, -1.1012e-03,  4.1564e-03,\n",
      "          9.6802e-04,  3.5776e-03,  1.4155e-03,  5.5867e-04,  4.0450e-03,\n",
      "          1.3052e-03,  3.0062e-03,  3.1760e-03,  2.5929e-03, -1.0129e-03,\n",
      "         -4.3451e-03,  4.7863e-03,  2.9205e-03,  1.2732e-03, -2.1592e-03,\n",
      "         -2.3385e-03,  2.8275e-03, -3.3052e-04, -8.3645e-04, -1.3257e-03,\n",
      "          1.9277e-05,  1.1624e-04, -3.0673e-03,  7.3166e-04, -3.3714e-03,\n",
      "          3.1809e-03,  2.4112e-03,  1.1428e-03, -1.3678e-03,  4.3922e-04,\n",
      "         -8.1191e-04,  4.7756e-03,  7.8930e-04, -8.7822e-04, -2.4056e-03,\n",
      "         -3.0663e-03, -1.3096e-03, -3.8812e-03, -8.4741e-05,  2.2406e-03,\n",
      "          2.8673e-03, -5.4517e-04, -1.5912e-03, -1.2537e-03, -2.0830e-03,\n",
      "          2.0411e-03,  2.3111e-03,  1.8744e-03,  1.0570e-03, -3.4618e-03,\n",
      "          4.9450e-03,  2.6674e-03, -3.9514e-03, -1.3257e-03, -2.3557e-03,\n",
      "         -1.0417e-03, -9.7147e-04,  2.7602e-04, -1.6377e-03,  1.1436e-03,\n",
      "          4.2151e-03,  1.2141e-03,  2.3897e-03,  2.4239e-03,  1.7549e-03,\n",
      "          4.9396e-03,  3.4590e-03,  4.8557e-04,  1.5881e-03, -1.9584e-04,\n",
      "          1.9381e-03, -7.3813e-04, -1.3111e-03, -3.0435e-03, -1.8820e-03,\n",
      "         -1.9240e-03,  2.5445e-03,  2.6676e-04, -4.5544e-04, -4.1506e-03,\n",
      "          2.9672e-04,  1.7765e-03, -2.7460e-03,  7.6800e-05,  2.9154e-03,\n",
      "         -1.4272e-03,  6.2381e-04,  3.3412e-04, -8.5782e-04,  5.5951e-04,\n",
      "         -6.4909e-04, -5.0219e-04,  2.8243e-03,  2.5240e-04,  2.9228e-03,\n",
      "         -9.4371e-04, -6.8230e-04,  2.3759e-04,  2.2242e-03, -9.3508e-04,\n",
      "         -3.1085e-04,  1.2990e-03, -2.0851e-03,  1.6127e-03,  1.2144e-03,\n",
      "          8.3823e-04, -1.2368e-03,  1.7996e-03,  1.4152e-04,  2.5156e-03,\n",
      "          2.2151e-03, -2.6645e-03, -7.0495e-04,  1.7489e-03,  2.1638e-03,\n",
      "          1.4780e-03,  5.9387e-04, -5.3411e-04, -3.3178e-03, -2.0229e-04,\n",
      "          2.3500e-03, -1.2307e-03,  2.2357e-03, -5.5307e-04,  1.5800e-04,\n",
      "         -6.0459e-04, -2.9207e-03,  9.5445e-04, -1.2197e-03, -1.3501e-03,\n",
      "         -2.9612e-03, -6.1465e-04,  4.3892e-04,  4.0846e-03,  3.6458e-03,\n",
      "         -5.2811e-04, -4.2181e-03,  3.5076e-03, -6.3003e-04,  2.1239e-03,\n",
      "          1.6911e-03, -1.3565e-03,  3.2909e-03,  1.8611e-03, -7.7152e-04,\n",
      "         -2.2354e-03,  6.5623e-04,  3.5164e-03,  2.6020e-03, -2.3252e-03,\n",
      "         -1.4430e-03,  6.3374e-04, -8.7236e-04,  6.9947e-04,  2.1954e-03,\n",
      "          1.2475e-03, -1.4317e-03, -4.2746e-04,  1.5638e-03, -8.4641e-04,\n",
      "          5.1023e-04, -8.4669e-04,  8.8649e-04, -3.2221e-03,  2.1912e-03,\n",
      "         -1.2121e-03, -3.4266e-03, -7.9175e-04, -3.1548e-03, -3.6050e-03,\n",
      "          2.6939e-03, -1.1013e-03, -2.4607e-04, -3.1348e-03, -2.2523e-03,\n",
      "         -1.8424e-03,  2.1681e-03,  3.1439e-04, -4.2918e-03, -9.7597e-04,\n",
      "         -4.1058e-03, -7.0158e-05,  2.0686e-04, -1.8443e-03,  2.4711e-03,\n",
      "         -4.5780e-05,  8.1415e-05, -2.1203e-03,  1.0827e-03,  1.5809e-03,\n",
      "         -4.2854e-04, -4.3431e-03,  1.9174e-03, -1.2347e-03,  1.9457e-03,\n",
      "         -1.8108e-03, -2.5422e-03,  4.4655e-04, -6.0682e-04,  4.2929e-03,\n",
      "          5.7473e-04, -1.6191e-03,  1.2112e-03, -3.1893e-03,  2.5350e-03,\n",
      "         -2.5604e-03, -1.1759e-03,  4.5641e-03, -1.7851e-04, -3.7549e-03,\n",
      "          3.6779e-03,  1.9608e-04, -2.6311e-03,  4.1061e-04,  8.0945e-04,\n",
      "         -1.7594e-03, -1.4058e-04,  4.9652e-04,  3.9458e-04, -4.2856e-04,\n",
      "         -9.6691e-05, -3.2578e-03,  2.1903e-03, -2.6858e-03,  3.7930e-03,\n",
      "         -4.4509e-03, -1.2011e-03, -1.7666e-03,  2.5916e-04,  2.5907e-03,\n",
      "         -1.8503e-03,  1.4994e-03, -2.6868e-03,  3.4874e-04,  6.7934e-05,\n",
      "         -3.7807e-03,  1.6601e-03, -3.4420e-04,  5.2233e-04, -2.3384e-03,\n",
      "         -1.4315e-03,  1.5909e-04,  2.1057e-03,  1.4439e-04,  1.4497e-03,\n",
      "          3.2388e-03, -8.3052e-06,  2.0445e-03, -1.3554e-03,  6.8810e-04,\n",
      "         -2.9265e-03,  2.5081e-04,  1.2987e-03,  5.9718e-04,  1.4769e-04,\n",
      "          1.0049e-03,  3.7365e-03, -5.1644e-03,  3.1045e-03, -2.2159e-03,\n",
      "         -1.4863e-03, -2.1422e-03,  1.7096e-04,  2.3604e-03, -4.3606e-04,\n",
      "         -3.6273e-03, -8.2394e-04, -5.5364e-04, -1.4891e-03, -9.5831e-04,\n",
      "          1.2092e-03, -2.3358e-03, -2.3149e-03, -6.7631e-04, -3.5349e-03,\n",
      "          2.1585e-03,  3.1039e-03, -1.6217e-03,  9.6133e-04, -2.3424e-03,\n",
      "         -3.0888e-04, -2.2241e-03, -9.3989e-04, -1.8052e-03,  3.8093e-03,\n",
      "         -4.4160e-03, -1.2556e-03, -1.8912e-03, -2.6226e-03, -3.2508e-04,\n",
      "         -1.5779e-03, -3.7074e-03,  2.9022e-03, -4.6624e-03, -3.7353e-03,\n",
      "          8.7744e-05,  4.9784e-03, -2.6165e-04,  2.5580e-03, -1.2095e-03,\n",
      "          3.8571e-03,  9.7304e-04, -7.7550e-04,  3.9487e-03,  7.7582e-05,\n",
      "          1.7656e-03,  7.0106e-04, -1.2102e-03, -2.4985e-03,  2.1116e-03,\n",
      "         -6.8390e-04,  3.2045e-03,  4.8118e-04, -1.1079e-03,  4.0118e-04,\n",
      "         -8.6285e-04,  2.1561e-03, -2.4233e-03,  2.0455e-03, -5.6429e-04,\n",
      "          2.4553e-03, -9.0240e-04,  4.2033e-04, -1.3389e-03,  1.9312e-03,\n",
      "          1.2510e-03, -7.4407e-04,  4.4523e-04, -3.6980e-03,  1.6364e-04,\n",
      "         -1.4400e-03, -3.1381e-03, -2.3077e-03, -2.6538e-03, -2.0233e-04,\n",
      "         -1.8214e-03,  2.4623e-04, -1.2459e-03, -2.8143e-03,  2.0397e-03,\n",
      "         -2.5852e-03, -1.8521e-03, -1.3382e-03, -1.5947e-03, -8.9743e-04,\n",
      "          3.1876e-04,  3.8793e-03,  1.0945e-03,  2.4452e-05, -3.9705e-03,\n",
      "          1.2676e-03,  1.2603e-03,  8.5658e-04, -3.0479e-03, -3.7612e-04,\n",
      "          1.0969e-03,  2.0269e-03,  2.4978e-04,  9.9493e-04,  4.2527e-03,\n",
      "         -3.1663e-03,  1.6398e-03,  2.5027e-03,  2.1554e-03, -1.4890e-03,\n",
      "          2.9066e-03,  4.7440e-04, -6.9945e-04, -2.5589e-03,  1.8752e-03,\n",
      "         -4.7124e-03, -6.6838e-04, -1.2003e-03,  3.4305e-03,  2.8324e-03,\n",
      "          3.2376e-04,  2.8905e-04, -1.8873e-03,  2.1939e-03,  5.4227e-04,\n",
      "         -6.4966e-04, -1.7395e-03,  1.3754e-03, -1.3951e-03,  2.1791e-03,\n",
      "          2.4481e-05,  1.8772e-03,  4.0263e-03, -4.1707e-03,  2.5909e-03,\n",
      "         -4.3557e-03,  1.2045e-03,  3.0081e-03, -2.4686e-03,  7.2870e-04,\n",
      "         -3.5894e-03, -1.4155e-03, -2.3002e-03, -3.0322e-03, -3.7901e-03,\n",
      "         -8.7495e-04,  6.1309e-04,  5.0060e-04, -3.3636e-03, -2.5884e-04,\n",
      "         -4.2399e-03,  1.4923e-03, -6.4475e-04,  4.1021e-03,  6.5932e-04,\n",
      "          3.1588e-04,  4.1222e-03,  8.7638e-04, -4.0748e-03,  6.3799e-04,\n",
      "          3.2292e-03, -1.5991e-03, -3.7798e-03,  1.6208e-03,  4.6712e-03,\n",
      "         -2.0235e-03, -4.2122e-03,  2.5557e-04, -1.1471e-03,  1.1863e-03,\n",
      "         -3.0428e-03,  2.7763e-03,  1.2368e-03, -6.2898e-05,  2.5060e-03,\n",
      "         -3.7254e-03, -9.1823e-04, -3.0414e-03, -9.6226e-04,  2.3635e-03,\n",
      "         -1.4348e-04,  2.0577e-03, -4.7384e-04, -8.5636e-04, -1.7844e-03,\n",
      "          2.1855e-03,  2.0216e-03,  4.8330e-04, -1.2218e-03,  1.9635e-03,\n",
      "          2.8828e-03, -7.7407e-05,  3.1916e-03, -1.3864e-04, -2.6515e-03,\n",
      "         -2.9697e-04,  5.0114e-03,  5.3226e-04, -2.0602e-03,  8.6077e-05,\n",
      "          9.3685e-04,  3.6195e-03, -2.4660e-04, -2.7712e-03,  1.5266e-03,\n",
      "         -1.2918e-03, -2.4167e-04,  7.6935e-04,  3.1991e-03, -7.9195e-04,\n",
      "         -5.8024e-04, -3.1274e-03,  2.6286e-04, -1.2649e-03, -2.0357e-03,\n",
      "         -2.5894e-04,  1.9431e-03,  2.7519e-03,  1.2607e-03,  2.5951e-04,\n",
      "         -2.4103e-03,  1.9181e-03, -1.6907e-03, -1.1380e-03, -1.9450e-03,\n",
      "          2.6736e-04,  2.4409e-03, -4.9364e-03, -2.3913e-03, -9.0773e-04,\n",
      "          2.7919e-04,  2.4827e-03,  2.3939e-03,  6.5595e-04, -1.0788e-04,\n",
      "          4.0174e-03,  1.4748e-03, -2.5458e-03,  2.5880e-03,  3.5461e-03,\n",
      "          3.9356e-03,  8.6460e-04,  3.9046e-03,  1.4848e-03, -9.7791e-04,\n",
      "          1.8355e-03,  4.2506e-03, -1.5160e-04,  1.4561e-03,  1.0277e-03,\n",
      "          1.0511e-03,  3.2998e-03,  1.0823e-03,  5.2611e-04,  3.8570e-04,\n",
      "         -1.1451e-03, -8.8399e-04,  2.3472e-03,  1.6249e-03,  1.8660e-03,\n",
      "          3.0618e-04,  1.5681e-04, -7.8731e-04, -1.4338e-03,  8.7902e-04,\n",
      "          2.1863e-05, -1.4710e-03, -2.1337e-03,  4.5362e-03, -1.3102e-03,\n",
      "          2.7875e-03, -2.0715e-03,  1.3509e-03,  4.0436e-04, -1.7707e-03,\n",
      "         -1.9401e-03, -8.1311e-04,  3.0430e-03, -1.2260e-04, -3.1877e-03,\n",
      "         -3.9278e-03, -3.8218e-04,  2.9701e-03,  2.5300e-03, -2.7970e-03,\n",
      "          2.8209e-03, -5.4286e-04, -8.5197e-05,  3.8884e-03, -3.4330e-03,\n",
      "         -1.0860e-05, -1.6923e-03, -6.3236e-04,  1.2216e-03, -1.5101e-04,\n",
      "          3.5732e-03, -3.3092e-03, -1.2673e-03, -1.0451e-03,  1.3749e-03,\n",
      "         -4.6414e-04, -1.1518e-03, -2.7091e-03, -1.1568e-03, -2.9461e-03,\n",
      "          2.3811e-03,  3.5216e-03, -1.8558e-03,  2.6936e-03,  2.1124e-03,\n",
      "         -2.8431e-03, -3.3584e-04, -8.1455e-04, -1.7023e-03,  1.2880e-03,\n",
      "          9.8540e-04, -2.8212e-03,  2.9285e-03,  3.8626e-04, -2.1382e-03,\n",
      "          1.4662e-03,  4.9890e-04,  2.5920e-03, -7.9326e-04,  1.4447e-03,\n",
      "         -3.0566e-03, -2.9683e-03, -2.0181e-03, -1.5809e-03,  4.0873e-03,\n",
      "          7.5738e-05,  4.0942e-04, -6.5545e-04, -1.4592e-03,  4.1579e-04,\n",
      "          1.3127e-03,  1.7400e-03,  2.2054e-03, -1.8929e-03,  3.6669e-03,\n",
      "          1.3324e-03, -1.2617e-03,  1.1941e-04,  1.8804e-03,  1.0172e-03,\n",
      "         -4.2060e-03, -1.7704e-03,  2.3899e-05,  1.8125e-03, -1.0648e-03,\n",
      "          9.6695e-04,  5.3322e-04, -1.7830e-03,  3.8745e-03,  8.2485e-04,\n",
      "         -9.0639e-05,  1.6009e-03,  3.7387e-03,  3.4719e-03,  1.0174e-05,\n",
      "         -7.1497e-04, -1.8560e-03,  3.1239e-03, -4.8432e-03,  9.6633e-05,\n",
      "         -2.0197e-03,  3.3803e-04, -1.3188e-03, -3.3024e-03,  5.5338e-06,\n",
      "          2.9675e-03,  3.2588e-03,  1.2059e-03, -2.6730e-03, -5.6752e-04,\n",
      "         -2.2214e-03, -9.4062e-04, -1.4580e-03, -2.9788e-03, -2.7998e-03,\n",
      "          9.3979e-04,  1.2149e-03, -1.9872e-03,  2.5020e-03, -7.0775e-04,\n",
      "          1.0174e-03,  2.3418e-03,  6.3561e-04,  4.4293e-03, -5.9672e-04,\n",
      "         -2.6475e-03, -3.1655e-05,  1.6317e-03, -3.3449e-04, -1.0911e-03,\n",
      "         -6.0221e-04, -1.5155e-03,  1.6817e-03, -3.0748e-04,  7.2670e-04,\n",
      "         -1.8610e-03,  2.5322e-03,  1.7822e-03,  4.0006e-03, -1.4439e-04,\n",
      "         -3.4804e-03, -4.4263e-03,  2.6928e-03,  3.9972e-03, -2.8335e-03,\n",
      "          1.6183e-04, -5.3010e-04,  1.4651e-03,  2.9860e-03, -4.3296e-03,\n",
      "         -2.8300e-03,  1.9309e-04, -8.8088e-04,  2.7066e-03,  1.9631e-03,\n",
      "          4.1758e-03,  1.3696e-03, -2.7341e-03,  7.8929e-04, -3.7876e-03,\n",
      "         -5.9238e-03, -3.3899e-03, -1.3256e-03,  3.0585e-03, -8.2657e-04,\n",
      "         -1.4897e-03, -2.6651e-03, -1.7205e-03,  3.0966e-03,  7.0612e-04,\n",
      "          1.6960e-03, -9.9325e-04, -2.3940e-03, -3.7408e-04,  1.0431e-03]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for n, p in hakisa.named_parameters():\n",
    "    if p.grad != None:\n",
    "        print(n)\n",
    "        print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2321e-05, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(reward_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 10\n",
      "Best Loss: 0.0\tCurrent LR: 1e-30\tGradients Average: 505079744.0\n",
      "('key', 'b')\n",
      "tensor([[1.5152]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 20\n",
      "Best Loss: 0.0\tCurrent LR: 1e-30\tGradients Average: 513861376.0\n",
      "('key', 'b')\n",
      "tensor([[3.2373]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 30\n",
      "Best Loss: 0.0\tCurrent LR: 1e-30\tGradients Average: 509129824.0\n",
      "('key', 'b')\n",
      "tensor([[3.3658]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 40\n",
      "Best Loss: 0.0\tCurrent LR: 1e-30\tGradients Average: 499914720.0\n",
      "('key', 'b')\n",
      "tensor([[1.0232]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 50\n",
      "Best Loss: 0.0\tCurrent LR: 1e-31\tGradients Average: 501219744.0\n",
      "('click', '(1623,752)')\n",
      "tensor([[0.6883]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 60\n",
      "Best Loss: 0.0\tCurrent LR: 1e-31\tGradients Average: 497129664.0\n",
      "('key', 'b')\n",
      "tensor([[1.0857]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 70\n",
      "Best Loss: 0.0\tCurrent LR: 1e-31\tGradients Average: 508000192.0\n",
      "('key', 'b')\n",
      "tensor([[1.7781]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 80\n",
      "Best Loss: 0.0\tCurrent LR: 1e-31\tGradients Average: 510023776.0\n",
      "('key', 'b')\n",
      "tensor([[1.6807]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 90\n",
      "Best Loss: 0.0\tCurrent LR: 1e-31\tGradients Average: 501954592.0\n",
      "('key', 'b')\n",
      "tensor([[1.4396]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 100\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000002e-32\tGradients Average: 494140736.0\n",
      "('click', '(1885,486)')\n",
      "tensor([[0.9783]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 110\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000002e-32\tGradients Average: 500555712.0\n",
      "('key', 'b')\n",
      "tensor([[1.4843]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 120\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000002e-32\tGradients Average: 506573984.0\n",
      "('key', 'b')\n",
      "tensor([[1.5973]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 130\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000002e-32\tGradients Average: 505925536.0\n",
      "('click', '(1262,333)')\n",
      "tensor([[0.2878]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 140\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000002e-32\tGradients Average: 501327328.0\n",
      "('rightClick', '(1262,410)')\n",
      "tensor([[0.2879]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 150\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000002e-33\tGradients Average: 487350336.0\n",
      "('key', 'b')\n",
      "tensor([[1.0013]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 160\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000002e-33\tGradients Average: 507904768.0\n",
      "('click', '(1802,386)')\n",
      "tensor([[0.8862]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 170\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000002e-33\tGradients Average: 530171872.0\n",
      "('key', 'b')\n",
      "tensor([[5.5514]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 180\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000002e-33\tGradients Average: 502365632.0\n",
      "('key', 'b')\n",
      "tensor([[1.0287]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 190\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000002e-33\tGradients Average: 497106176.0\n",
      "('key', 'b')\n",
      "tensor([[1.3325]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 200\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-34\tGradients Average: 500178080.0\n",
      "('key', 'b')\n",
      "tensor([[1.3323]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 210\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-34\tGradients Average: 488883552.0\n",
      "('click', '(1859,666)')\n",
      "tensor([[0.9497]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 220\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-34\tGradients Average: 506025664.0\n",
      "('key', 'b')\n",
      "tensor([[1.7432]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 230\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-34\tGradients Average: 505049568.0\n",
      "('rightClick', '(1589,968)')\n",
      "tensor([[0.6509]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 240\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-34\tGradients Average: 494399264.0\n",
      "('key', 'b')\n",
      "tensor([[1.5445]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 250\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-35\tGradients Average: 497692160.0\n",
      "('rightClick', '(1341,925)')\n",
      "tensor([[0.3760]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 260\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-35\tGradients Average: 499845888.0\n",
      "('key', 'b')\n",
      "tensor([[1.3373]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 270\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-35\tGradients Average: 494649568.0\n",
      "('key', 'b')\n",
      "tensor([[1.6900]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 280\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-35\tGradients Average: -507978432.0\n",
      "('rightClick', '(695,316)')\n",
      "tensor([[-0.3405]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 290\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-35\tGradients Average: 508075072.0\n",
      "('key', 'b')\n",
      "tensor([[4.2965]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 300\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-36\tGradients Average: 504000256.0\n",
      "('key', 'b')\n",
      "tensor([[1.2080]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 310\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-36\tGradients Average: 505379424.0\n",
      "('click', '(1090,368)')\n",
      "tensor([[0.0973]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 320\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-36\tGradients Average: 499393760.0\n",
      "('rightClick', '(1781,257)')\n",
      "tensor([[0.8628]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 330\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-36\tGradients Average: 501380064.0\n",
      "('key', 'b')\n",
      "tensor([[1.2529]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 340\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000004e-36\tGradients Average: 493063808.0\n",
      "('key', 'b')\n",
      "tensor([[1.8448]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 350\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-37\tGradients Average: 496639776.0\n",
      "('click', '(1446,998)')\n",
      "tensor([[0.4925]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 360\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-37\tGradients Average: -486733344.0\n",
      "('click', '(848,426)')\n",
      "tensor([[-0.1708]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 370\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-37\tGradients Average: 491929152.0\n",
      "('rightClick', '(1461,419)')\n",
      "tensor([[0.5084]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 380\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-37\tGradients Average: 491561504.0\n",
      "('key', 'b')\n",
      "tensor([[1.1445]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 390\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-37\tGradients Average: 504089120.0\n",
      "('click', '(1599,221)')\n",
      "tensor([[0.6611]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 400\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-38\tGradients Average: 493158688.0\n",
      "('key', 'b')\n",
      "tensor([[1.4702]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 410\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-38\tGradients Average: 514576288.0\n",
      "('key', 'b')\n",
      "tensor([[4.6470]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 420\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-38\tGradients Average: 500019168.0\n",
      "('key', 'b')\n",
      "tensor([[1.9664]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 430\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-38\tGradients Average: -506493024.0\n",
      "('click', '(918,823)')\n",
      "tensor([[-0.0928]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 440\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-38\tGradients Average: 503333024.0\n",
      "('key', 'b')\n",
      "tensor([[1.3168]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 450\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000006e-39\tGradients Average: 495893280.0\n",
      "('key', 'b')\n",
      "tensor([[1.1728]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 460\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000006e-39\tGradients Average: 489289152.0\n",
      "('key', 'b')\n",
      "tensor([[1.5228]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 470\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000006e-39\tGradients Average: 503820032.0\n",
      "('key', 'b')\n",
      "tensor([[1.8540]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 480\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000006e-39\tGradients Average: 0.78630131483078\n",
      "('key', 'b')\n",
      "tensor([[1.4485]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 490\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000006e-39\tGradients Average: 3.1851093769073486\n",
      "('key', 'b')\n",
      "tensor([[1.0644]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 500\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-40\tGradients Average: -0.8247790932655334\n",
      "('rightClick', '(959,884)')\n",
      "tensor([[-0.0473]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 510\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-40\tGradients Average: 3.202941417694092\n",
      "('key', 'b')\n",
      "tensor([[1.5557]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 520\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-40\tGradients Average: 3.265420913696289\n",
      "('key', 'b')\n",
      "tensor([[2.2021]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 530\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-40\tGradients Average: 0.7924113273620605\n",
      "('key', 'b')\n",
      "tensor([[1.6533]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 540\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000005e-40\tGradients Average: 0.7975094318389893\n",
      "('key', 'b')\n",
      "tensor([[1.1628]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 550\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000006e-41\tGradients Average: 0.7863312363624573\n",
      "('key', 'b')\n",
      "tensor([[1.2360]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 560\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000006e-41\tGradients Average: 3.2093470096588135\n",
      "('key', 'b')\n",
      "tensor([[2.4259]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 570\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000006e-41\tGradients Average: 3.205077648162842\n",
      "('key', 'b')\n",
      "tensor([[1.8326]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 580\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000006e-41\tGradients Average: 0.4002138674259186\n",
      "('key', 'b')\n",
      "tensor([[1.4363]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 590\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000006e-41\tGradients Average: 0.39770644903182983\n",
      "('click', '(1798,1017)')\n",
      "tensor([[0.8825]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 600\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000007e-42\tGradients Average: 0.4042212963104248\n",
      "('key', 'b')\n",
      "tensor([[1.9818]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 610\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000007e-42\tGradients Average: 0.40205585956573486\n",
      "('rightClick', '(1670,1036)')\n",
      "tensor([[0.7407]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 620\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000007e-42\tGradients Average: 0.3949540853500366\n",
      "('key', 'b')\n",
      "tensor([[1.5060]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 630\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000007e-42\tGradients Average: 0.3936532139778137\n",
      "('rightClick', '(1551,714)')\n",
      "tensor([[0.6085]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "Current step: 640\n",
      "Best Loss: 0.0\tCurrent LR: 1.0000000000000007e-42\tGradients Average: 1.6003981828689575\n",
      "('key', 'b')\n",
      "tensor([[1.2413]], device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# LEAGUE OF LEGENDS\n",
    "\n",
    "import keyboard\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_learnweaponskill_Rapier_2_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "hakisa.mode = 'Play'\n",
    "\n",
    "reward = 0. # Cumulative reward\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = 'Hakisa'\n",
    "steps = 0\n",
    "save_point = 5 # Also optimization point\n",
    "uncertainty_factor = 0.9 # Also known as gamma or discount factor\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.1)\n",
    "\n",
    "action_quality_loss = torch.nn.NLLLoss()\n",
    "reward_loss = torch.nn.MSELoss()\n",
    "\n",
    "while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame)\n",
    "\n",
    "    else:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame, previous_action=(cmds[0].detach(), cmds[1].detach(), cmds[2].detach()), previous_reward=predicted_reward.detach())\n",
    "\n",
    "    del frame\n",
    "    \n",
    "    command = dataset.get_command(cmds[0].detach().cpu().numpy(), cmds[1].detach().cpu().numpy(), cmds[2].detach().cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    # No Need for thresholding/grayscale here:\n",
    "\n",
    "    kda = dataset.get_consequences(1, 1632, 1762-1632, 30-1, tesseract_config='--psm 6')\n",
    "\n",
    "    kills, deaths, assists = preprocess_LoL_KDA(kda)\n",
    "\n",
    "    farm = dataset.get_consequences(1, 1775, 1823-1775, 30-1, tesseract_config='--psm 6')\n",
    "\n",
    "    farm = preprocess_LoL_farm(farm)\n",
    "    \n",
    "    farm = farm/8.\n",
    "\n",
    "    reward += (farm+kills+assists)/(2**deaths)\n",
    "\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        previous_command_quality = command_quality.detach()\n",
    "\n",
    "    action_quality_cost = action_quality_loss(previous_command_quality, command_quality.argmax(1).detach()) # Input = (1, Classes), Target = (1)\n",
    "\n",
    "    predicted_reward = predicted_reward * uncertainty_factor\n",
    "\n",
    "    reward_cost = reward_loss(predicted_reward, reward)\n",
    "\n",
    "    gameplay_loss = action_quality_cost + reward_cost\n",
    "\n",
    "    gameplay_loss.backward()\n",
    "\n",
    "    previous_command_quality = command_quality.detach()\n",
    "\n",
    "    del command_quality\n",
    "\n",
    "    for n, p in hakisa.named_parameters():\n",
    "\n",
    "        if 'neuron1.weight' in n:\n",
    "            grads.append(torch.mean(p.grad))\n",
    "\n",
    "        if grad_clip is not None:\n",
    "            p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if gameplay_loss.item() < best_loss:\n",
    "\n",
    "        best_loss = gameplay_loss.item()\n",
    "        best_params = hakisa.state_dict()\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Best Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward}\")\n",
    "        print(command)\n",
    "\n",
    "        torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ -8.0818,  -9.6056,  -9.6255],\n",
       "          [ -8.4776,  -8.9226,  -9.1585],\n",
       "          [ -8.5030,  -8.9643, -11.7933]],\n",
       "\n",
       "         [[ -7.1003,  -6.7701,  -5.5618],\n",
       "          [ -4.7610,  -4.7574,  -4.5979],\n",
       "          [ -5.1824,  -4.3092,  -2.8102]],\n",
       "\n",
       "         [[-12.6108, -11.3430,  -8.1823],\n",
       "          [ -6.2670,  -6.6425,  -4.7512],\n",
       "          [  1.6042,   3.0756,   1.7228]]],\n",
       "\n",
       "\n",
       "        [[[ -1.3663,  -3.8110,  -7.6285],\n",
       "          [  2.2791,  -3.0224,   1.4213],\n",
       "          [-11.3926, -11.2452,  -9.1470]],\n",
       "\n",
       "         [[ -5.2159,  -7.7696,  -5.4252],\n",
       "          [ -5.6851,  -2.1385,  -3.5461],\n",
       "          [  9.0411,   8.0570,   7.7868]],\n",
       "\n",
       "         [[ -1.5291,  -1.3853,  -0.0645],\n",
       "          [ -0.6045,   0.1015,   0.3212],\n",
       "          [  0.8363,   2.0732,   4.9115]]],\n",
       "\n",
       "\n",
       "        [[[  3.1513,   5.7473,   3.8477],\n",
       "          [  4.8267,   2.4411,   2.9113],\n",
       "          [  5.4710,   5.5335,   5.0723]],\n",
       "\n",
       "         [[  3.4653,   0.5714,  -1.3872],\n",
       "          [  0.2524,   1.2432,   0.0732],\n",
       "          [  0.0239,  -0.1330,  -1.1263]],\n",
       "\n",
       "         [[ -7.7413,  -9.5132,  -9.3880],\n",
       "          [ -7.4738,  -8.2082,  -7.7225],\n",
       "          [ -6.4299,  -6.0296,  -6.0074]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ -7.3277, -10.6375,  -9.8812],\n",
       "          [-10.2517, -10.4368,  -7.1165],\n",
       "          [ -6.9233,  -5.5827,  -5.1867]],\n",
       "\n",
       "         [[ -4.6211,  -0.0845,  -2.8919],\n",
       "          [ -8.6613,  -7.8585,  -8.2362],\n",
       "          [ -6.2861,  -5.9789,  -5.4731]],\n",
       "\n",
       "         [[  2.5726,   2.1222,   2.2598],\n",
       "          [  7.5897,   4.6523,   7.9806],\n",
       "          [ -3.9690,  -2.4524,  -3.0732]]],\n",
       "\n",
       "\n",
       "        [[[ -5.6461,  -3.1288,  -5.2587],\n",
       "          [ -6.0614,  -4.1232,  -4.7496],\n",
       "          [ -5.2090,  -5.3112,  -7.7795]],\n",
       "\n",
       "         [[  7.9084,   8.4113,   6.7967],\n",
       "          [  6.7856,   6.6462,   6.7960],\n",
       "          [  7.4288,   6.0519,   7.7404]],\n",
       "\n",
       "         [[  8.5996,   7.1970,   8.8703],\n",
       "          [  4.1634,   7.0995,   5.7949],\n",
       "          [  6.2574,   8.0155,   6.1232]]],\n",
       "\n",
       "\n",
       "        [[[  8.6829,  10.2711,   9.8875],\n",
       "          [ 13.2816,  12.4669,   9.7087],\n",
       "          [ 11.5474,  12.4357,  11.5215]],\n",
       "\n",
       "         [[ -2.7887,  -2.6767,  -0.1095],\n",
       "          [ -7.8629,  -7.4499,  -7.6417],\n",
       "          [  0.6131,   0.6756,   0.6296]],\n",
       "\n",
       "         [[-10.0827, -10.2477,  -9.4097],\n",
       "          [  5.8847,   5.5037,   5.2307],\n",
       "          [  0.1682,   0.6190,   1.6260]]]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hakisa.conv1.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "import pyautogui\n",
    "import pytesseract\n",
    "from time import sleep\n",
    "import winsound\n",
    "from re import sub\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory efficient loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory efficient loop\n",
    "\n",
    "with mss() as sct:\n",
    "    #filename = sct.shot()\n",
    "    monitor = {\"top\": 40, \"left\": 0, \"width\": 800, \"height\": 640} # Memory efficiency.\n",
    "    data = sct.grab(monitor)\n",
    "    data = Image.frombytes(\"RGB\", data.size, data.bgra, 'raw', 'BGRX')\n",
    "    data = np.array(data)\n",
    "    data = torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 800, 4)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape) # (Height, Width, Channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have our frame grabber. We just need to make sure it'll run in real time\n",
    "# After that, we'll have to create a dataset which will serve as our memory. It can be used for Hakisa to know more or less how she should handle each situation\n",
    "# The inputs in the dataset(X), will be the images. The outputs(labels), will be the keys that Hakisa must use, the commands(PyAutoGUI commands)\n",
    "# Those commands can be float-encoded, just like we do in NLP. In order to decode them, we can use K-Nearest Neighbors.\n",
    "\n",
    "# Image(game state) ------> Hakisa ------> Output(ex: 1.543) -------> Decoder(KNN + input dictionary) ------> Game Input(Keyboard/Mouse command)\n",
    "\n",
    "# Exploration Mode: Each image will compose a standardized dataset(Is this really necessary?). Each output will compose a memory dataset(labels dataset)\n",
    "\n",
    "# Studying Mode: After generating a certain amount of input images and output commands, Hakisa will iterate through them and learn what she should do in each situation.\n",
    "# She'll be trying to get patterns from each situation and associate them with the output with lower loss(which will be determined by the game scores)\n",
    "\n",
    "# Training Mode: After learning some patterns, Hakisa will play on her own. There'll be no datasets here, it's just her and the game.\n",
    "\n",
    "# Eval mode: I don't really think she'll ever reach a peak of skill... If this happens, it's because her architecture must be upgraded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_mapping=None, explore_train_steps=1000, memory_size=100, top=0, left=0, width=1920, height=1080, resize=None):\n",
    "\n",
    "        # Window resolutions for the screen grabber\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.resize = resize # For reducing the images. Must be a tuple (Height, Width)\n",
    "\n",
    "        # Hakisa's Mode - Explore, Study, Train.\n",
    "\n",
    "        #self.mode = mode # Useless for now... I don't know exactly how to mount the functions. Hakisa is quite dependent from the Dataset class.\n",
    "\n",
    "        self.steps = explore_train_steps\n",
    "\n",
    "        self.data = None # This will be created during training. However, we could make it possible for us to load a ready-made data for studying.\n",
    "        self.input_mapping = self._create_commands_dictionary(input_maps=input_mapping)\n",
    "\n",
    "        self.knn = None # Creating variable so we don't have to fit KNN at every step.\n",
    "        self._fit_knn(self.input_mapping)\n",
    "\n",
    "        self.keys_list = list(self.input_mapping.keys()) # Also for efficiency in each step\n",
    "\n",
    "        self.labels = None # Used for studying\n",
    "\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = []\n",
    "\n",
    "\n",
    "    # Pytorch's Dataset functions will only be used in Studying mode\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        inputs = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def _grab_frame(self):\n",
    "        # Unfortunately, this whole operation takes about 0.6 seconds, so we'll probably have to deal with a single frame each 1~3 seconds.\n",
    "        with mss() as sct:\n",
    "            frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "            frame = Image.frombytes(\"RGB\", frame.size, frame.bgra)\n",
    "\n",
    "            if self.resize:\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame = np.array(frame, dtype=np.float32)\n",
    "\n",
    "            frame = torch.from_numpy(frame)\n",
    "        \n",
    "        frame = frame.view(1, frame.size(2), frame.size(0), frame.size(1)).to(device) # (Batch, Channels, Height, Width)\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def _create_commands_dictionary(self, input_maps):\n",
    "        idx2key = []\n",
    "        key2idx = {}\n",
    "\n",
    "        for key in input_maps:\n",
    "            if key not in key2idx:\n",
    "                idx2key.append(key)\n",
    "                key2idx[key] = len(idx2key) - 1\n",
    "        \n",
    "        del idx2key\n",
    "\n",
    "        maximum = max(key2idx.values())\n",
    "\n",
    "        for key, value in key2idx.items():\n",
    "\n",
    "            scaled_value = (value-0)*2.0 / (maximum - 0)-1.0\n",
    "\n",
    "            key2idx[key] = scaled_value\n",
    "\n",
    "        return key2idx\n",
    "\n",
    "    def get_mouse_coordinates(self, mouse_commands, x0, xf, y0, yf):\n",
    "        '''\n",
    "        Generates a list of mouse commands and its coordinates, ready to generate an input dictionary\n",
    "        Example:\n",
    "            mouse_commands = ['click', 'move']\n",
    "            x0 = 0, xf=10, y0=0, yf=10\n",
    "\n",
    "            dataset.get_mouse_coordinates(mouse_commands, 0, 10, 0, 10)\n",
    "\n",
    "            returns: commands = ['click_(0,0)', 'move_(0,0)', 'click_(1,0)', 'move_(1,0)'...]\n",
    "        '''\n",
    "        # We need to get the window coordinates so we can use mouse commands\n",
    "\n",
    "        Xvalues = [i for i in range(x0, xf)]\n",
    "        Yvalues = [i for i in range(y0, yf)]\n",
    "\n",
    "        commands = []\n",
    "\n",
    "        for x in Xvalues:\n",
    "            for y in Yvalues:\n",
    "                for cmd in mouse_commands:\n",
    "                    cmd = cmd + \"_\" + \"(\" + str(x) + \",\" + str(y) + \")\"\n",
    "                    commands.append(cmd)\n",
    "        \n",
    "        del Xvalues, Yvalues, cmd\n",
    "        \n",
    "        return commands\n",
    "\n",
    "    def _fit_knn(self, dictionary):\n",
    "        \n",
    "        values = list(self.input_mapping.values())\n",
    "\n",
    "        values = np.array(values).reshape(-1,1)\n",
    "\n",
    "        self.knn = KNN(n_neighbors=1, algorithm='kd_tree').fit(values)\n",
    "\n",
    "        print(\"KNN fitted and ready to go!\")\n",
    "\n",
    "        del values\n",
    "        \n",
    "\n",
    "    def get_command(self, action_value):\n",
    "        '''\n",
    "        Each dictionary key = command_action ---> (keyDown_z) or (click_(100,60))\n",
    "        Each dictionary value = value for that action ---> within range [-1, 1]\n",
    "\n",
    "        Must return a tuple (command, action) --> ('keyDown', 'z') or ('click', '(100,60)')\n",
    "        '''\n",
    "\n",
    "        _, index = self.knn.kneighbors(action_value)\n",
    "\n",
    "        for subarray in index:\n",
    "            for i in subarray:\n",
    "                command = self.keys_list[i]\n",
    "        \n",
    "        del index\n",
    "\n",
    "        command = tuple(command.split('_')) # Command: keyDown_z -----> (keyDown, z) ; keyDown_shift_keyDown_z ----> (keyDown, shift, keyDown, z)\n",
    "\n",
    "        return command\n",
    "\n",
    "    def get_consequences(self, top, left, width, height, tesseract_config='--psm 8'):\n",
    "        '''\n",
    "        Used after Hakisa performed an input, in order to get its consequences(ex: score change, bombs, kills, deaths...).\n",
    "        Returns a string according to Tesseract's OCR\n",
    "        '''\n",
    "\n",
    "        with mss() as sct:\n",
    "            consequence = sct.grab(monitor={\"top\": top, \"left\": left, \"width\": width, \"height\": height})\n",
    "            consequence = Image.frombytes(\"RGB\", consequence.size, consequence.bgra)\n",
    "        \n",
    "        consequence = pytesseract.image_to_string(consequence, config=tesseract_config) \n",
    "\n",
    "        # OCR adds some strange characters(even with the whitelist function). Let's remove them.\n",
    "\n",
    "        consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence) # Attention: 0, 1 and 8 can be seen as O, l and B.\n",
    "\n",
    "        return consequence\n",
    "\n",
    "    def create_memory(self, beta, frame, key, reward):\n",
    "        '''\n",
    "        Saves data in the memory list.\n",
    "        Memory is saved in the format (frame, key, key_value, reward).\n",
    "\n",
    "        Frame will be used as input during studying. key_value, as label.\n",
    "        Key is saved for visualization, and reward works as weights(helps discarding bad decisions and saving good ones)\n",
    "\n",
    "        Memory will only be changed once it reaches its full size.\n",
    "        '''\n",
    "\n",
    "        reward = reward * beta # Beta can be a constant value, like 1e-5\n",
    "\n",
    "        key = '_'.join(key) # (command, action) ----> (command_action), like it's in the dictionary\n",
    "\n",
    "        memory = (frame, key, self.input_mapping.get(key), reward) # A tuple makes each item in the list iterable.\n",
    "\n",
    "\n",
    "        if len(self.memory) < self.memory_size:\n",
    "\n",
    "            self.memory.append(memory)\n",
    "        \n",
    "        else:\n",
    "            self.memory = sorted(self.memory, key=lambda x: x[3]) # Sorting list according to rewards values.\n",
    "            self.memory.pop(0) # Removing the item with lowest reward value\n",
    "            \n",
    "            self.memory.append(memory)\n",
    "\n",
    "    def create_data_for_study(self):\n",
    "\n",
    "        # Creating dataset for studying\n",
    "\n",
    "        inputs = [i[0].cpu() for i in self.memory]\n",
    "        labels = [i[2] for i in self.memory]\n",
    "\n",
    "        inputs = torch.cat(inputs, 0)\n",
    "        labels = torch.cat(labels, 0)\n",
    "\n",
    "        #inputs = np.stack(inputs, 0)\n",
    "        #labels = np.stack(labels, 0)\n",
    "\n",
    "        #inputs = torch.from_numpy(inputs).float()\n",
    "        #labels = torch.from_numpy(labels).float()\n",
    "\n",
    "        self.data = inputs.to(device)\n",
    "        self.labels = labels.to(device)\n",
    "\n",
    "        del inputs, labels\n",
    "\n",
    "    def use_readymade_data(self, data, labels):\n",
    "        # We aren't using data in time_steps mode, like we do for gifs, time series and forecasting in general.\n",
    "        # I thought it might be a good idea to also train Hakisa with that.\n",
    "        # This might also be the best way to train her in frames forecasting, as the process is probably too slow to be made while playing.\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_commands = ['click', 'move']\n",
    "Xvalues = [i for i in range(0,10)]\n",
    "Yvalues = [i for i in range(0, 10)]\n",
    "\n",
    "commands = []\n",
    "\n",
    "for x in Xvalues:\n",
    "    for y in Yvalues:\n",
    "        for cmd in mouse_commands:\n",
    "            cmd = cmd + \"_\" + \"(\" + str(x) + \",\" + str(y) + \")\"\n",
    "            commands.append(cmd)\n",
    "\n",
    "print(commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2out(input, kernel, stride, padding):\n",
    "    x = 2*padding\n",
    "    y = 1*(kernel-1)\n",
    "    z = (input + x - y - 1)/stride\n",
    "\n",
    "    output = z + 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv2out(15, 2, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv2out(160, 2, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hakisa(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, mode='Default'):\n",
    "\n",
    "        super(Hakisa, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        # This structure must be changed with the input size...unless you'd like to use adaptive pooling\n",
    "\n",
    "        # Let's begin supposing that we're gonna use 1080x1920 RGB images ---> (3, 1080, 1920)\n",
    "\n",
    "        # Output: always (Batch, 1)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 10, kernel_size=(3, 3), stride=(3,3), bias=False) # 360x640\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(10)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 25, kernel_size=(3, 4), stride=(3,4), bias=False) # 120x160\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(25)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=(2, 2), stride=(2,2)) # 60x80\n",
    "        self.conv3 = torch.nn.Conv2d(25, 50, kernel_size=(2, 2), stride=(2,2), bias=False) # 30x40\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(50)\n",
    "        self.conv4 = torch.nn.Conv2d(50, 75, kernel_size=(2, 2), stride=(2,2), bias=False) # 15x20\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(75)\n",
    "        self.pool2 = torch.nn.MaxPool2d(kernel_size=(4, 5), stride=(1,1)) # 12x16\n",
    "        self.conv5 = torch.nn.Conv2d(75, 100, kernel_size=(3, 3), stride=(1,1), bias=False) # 10x14\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(100)\n",
    "        self.conv6 = torch.nn.Conv2d(100, 75, kernel_size=(3, 3), stride=(1,1), bias=False) # 8x12\n",
    "        self.batchnorm6 = torch.nn.BatchNorm2d(75)\n",
    "        self.conv7 = torch.nn.Conv2d(75, 50, kernel_size=(3, 3), stride=(1,1), bias=False) # 6x10\n",
    "        self.batchnorm7 = torch.nn.BatchNorm2d(50)\n",
    "        self.conv8 = torch.nn.Conv2d(50, 25, kernel_size=(3, 3), stride=(1,1), bias=False) # 4x8\n",
    "        self.neuron = torch.nn.Linear(25*4*8, 1, bias=False)\n",
    "\n",
    "        self.PRelu = torch.nn.PReLU(1)\n",
    "        #self.tanh = torch.nn.Tanh() # Reinforcement Learning algorithms tend to use softmax. But that would result in a quite big output size.\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        if self.mode == \"Forecast\": # input: (Batch, frames_sequence, channels, height, width)\n",
    "\n",
    "            batch_size = input.size(0)\n",
    "\n",
    "            for frame_step in range(input.size(1)):\n",
    "\n",
    "                if frame_step > 0:\n",
    "\n",
    "                    previous_frame = input[batch_size//2:, frame_step-1]\n",
    "                    \n",
    "                    input_frame = input[:batch_size//2, frame_step]\n",
    "\n",
    "                    input_frame = torch.cat((previous_frame, input_frame), 0)\n",
    "\n",
    "                else:\n",
    "                    input_frame = input[:, frame_step]\n",
    "\n",
    "                x = self.conv1(input_frame)\n",
    "                x = self.batchnorm1(x)\n",
    "                x = self.PRelu(x)\n",
    "                x = self.conv2(x)\n",
    "                x = self.batchnorm2(x)\n",
    "                x = self.PRelu(x)\n",
    "\n",
    "                x = self.pool1(x)\n",
    "\n",
    "                x = self.conv3(x)\n",
    "                x = self.batchnorm3(x)\n",
    "                x = self.PRelu(x)\n",
    "                x = self.conv4(x)\n",
    "                x = self.batchnorm4(x)\n",
    "                x = self.PRelu(x)\n",
    "\n",
    "                x = self.pool2(x)\n",
    "\n",
    "                x = self.conv5(x)\n",
    "                x = self.batchnorm5(x)\n",
    "                x = self.PRelu(x)\n",
    "                x = self.conv6(x)\n",
    "                x = self.batchnorm6(x)\n",
    "                x = self.PRelu(x)\n",
    "\n",
    "                x = self.conv7(x)\n",
    "                x = self.batchnorm7(x)\n",
    "                x = self.PRelu(x)\n",
    "\n",
    "                x = self.conv8(x)\n",
    "                \n",
    "                x = self.PRelu(x)\n",
    "\n",
    "                x = x.view(x.size(0), -1)\n",
    "\n",
    "                x = self.neuron(x)\n",
    "\n",
    "\n",
    "                output = x\n",
    "\n",
    "                del x\n",
    "\n",
    "                return output\n",
    "\n",
    "        else: # input: (Batch, channels, height, width)\n",
    "\n",
    "            x = self.conv1(input)\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool1(x)\n",
    "\n",
    "            x = self.conv3(x)\n",
    "            x = self.batchnorm3(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv4(x)\n",
    "            x = self.batchnorm4(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.pool2(x)\n",
    "\n",
    "            x = self.conv5(x)\n",
    "            x = self.batchnorm5(x)\n",
    "            x = self.PRelu(x)\n",
    "            x = self.conv6(x)\n",
    "            x = self.batchnorm6(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.conv7(x)\n",
    "            x = self.batchnorm7(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = self.conv8(x)\n",
    "            \n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "            x = self.neuron(x)\n",
    "\n",
    "\n",
    "            output = x\n",
    "\n",
    "            del x\n",
    "\n",
    "            return output\n",
    "\n",
    "\n",
    "    def execute_command(self, command):\n",
    "        '''\n",
    "        Command must be a tuple (keyboard command|mouse command , key|coordinate)\n",
    "\n",
    "        We could, however, make things more interesting by creating the possibility of having 2 different commands:\n",
    "        one to select which command it will be(key down, key up, move mouse) and another to decide which key(Z, A, Q...)\n",
    "\n",
    "        Make sure that all keys are lowered\n",
    "        '''\n",
    "\n",
    "        #print(command)\n",
    "\n",
    "        #print(len(command))\n",
    "\n",
    "        for i in range(len(command)//2): # This might do\n",
    "            if \"key\" in command[2*i]:\n",
    "                if \"Up\" in command[2*i]:\n",
    "                    pyautogui.keyUp(command[(2*i)+1])\n",
    "            \n",
    "                elif \"Down\" in command[2*i]:\n",
    "                    pyautogui.keyDown(command[(2*i)+1])\n",
    "\n",
    "                else:\n",
    "                    pyautogui.press(command[(2*i)+1])\n",
    "\n",
    "            else:\n",
    "                coordinates = command[(2*i)+1].replace('(', '').replace(')', '').split(',')\n",
    "                \n",
    "                if \"move\" in command[2*i]:\n",
    "                    pyautogui.move(int(coordinates[0]), int(coordinates[1]))\n",
    "\n",
    "                elif \"rightClick\" in command[2*i]:\n",
    "                    pyautogui.rightClick(int(coordinates[0]), int(coordinates[1]))\n",
    "                \n",
    "                else:\n",
    "                    pyautogui.click(int(coordinates[0]), int(coordinates[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', 'accept', 'add', 'alt', 'altleft', 'altright', 'apps', 'backspace', 'browserback', 'browserfavorites', 'browserforward', 'browserhome', 'browserrefresh', 'browsersearch', 'browserstop', 'capslock', 'clear', 'convert', 'ctrl', 'ctrlleft', 'ctrlright', 'decimal', 'del', 'delete', 'divide', 'down', 'end', 'enter', 'esc', 'escape', 'execute', 'f1', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f2', 'f20', 'f21', 'f22', 'f23', 'f24', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'final', 'fn', 'hanguel', 'hangul', 'hanja', 'help', 'home', 'insert', 'junja', 'kana', 'kanji', 'launchapp1', 'launchapp2', 'launchmail', 'launchmediaselect', 'left', 'modechange', 'multiply', 'nexttrack', 'nonconvert', 'num0', 'num1', 'num2', 'num3', 'num4', 'num5', 'num6', 'num7', 'num8', 'num9', 'numlock', 'pagedown', 'pageup', 'pause', 'pgdn', 'pgup', 'playpause', 'prevtrack', 'print', 'printscreen', 'prntscrn', 'prtsc', 'prtscr', 'return', 'right', 'scrolllock', 'select', 'separator', 'shift', 'shiftleft', 'shiftright', 'sleep', 'space', 'stop', 'subtract', 'tab', 'up', 'volumedown', 'volumemute', 'volumeup', 'win', 'winleft', 'winright', 'yen', 'command', 'option', 'optionleft', 'optionright']\n"
     ]
    }
   ],
   "source": [
    "print(pyautogui.KEY_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_mapping = [\n",
    "    'keyDown_up', 'keyUp_up',\n",
    "    'keyDown_down', 'keyUp_down',\n",
    "    'keyDown_left', 'keyUp_left',\n",
    "    'keyDown_right', 'keyUp_right',\n",
    "    'keyDown_z', 'keyUp_z',\n",
    "    'keyDown_shift', 'keyUp_shift',\n",
    "    'key_x'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN fitted and ready to go!\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(input_mapping, explore_train_steps=100, memory_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keyDown_up': -1.0, 'keyUp_up': -0.8333333333333334, 'keyDown_down': -0.6666666666666667, 'keyUp_down': -0.5, 'keyDown_left': -0.33333333333333337, 'keyUp_left': -0.16666666666666663, 'keyDown_right': 0.0, 'keyUp_right': 0.16666666666666674, 'keyDown_z': 0.33333333333333326, 'keyUp_z': 0.5, 'keyDown_shift': 0.6666666666666667, 'keyUp_shift': 0.8333333333333333, 'key_x': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.input_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "hakisa = Hakisa().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 10, 360, 640]             270\n",
      "       BatchNorm2d-2         [-1, 10, 360, 640]              20\n",
      "         LeakyReLU-3         [-1, 10, 360, 640]               0\n",
      "            Conv2d-4         [-1, 25, 120, 160]           3,000\n",
      "       BatchNorm2d-5         [-1, 25, 120, 160]              50\n",
      "         LeakyReLU-6         [-1, 25, 120, 160]               0\n",
      "         MaxPool2d-7           [-1, 25, 60, 80]               0\n",
      "            Conv2d-8           [-1, 50, 30, 40]           5,000\n",
      "       BatchNorm2d-9           [-1, 50, 30, 40]             100\n",
      "        LeakyReLU-10           [-1, 50, 30, 40]               0\n",
      "           Conv2d-11           [-1, 75, 15, 20]          15,000\n",
      "      BatchNorm2d-12           [-1, 75, 15, 20]             150\n",
      "        LeakyReLU-13           [-1, 75, 15, 20]               0\n",
      "        MaxPool2d-14           [-1, 75, 12, 16]               0\n",
      "           Conv2d-15          [-1, 100, 10, 14]          67,500\n",
      "      BatchNorm2d-16          [-1, 100, 10, 14]             200\n",
      "        LeakyReLU-17          [-1, 100, 10, 14]               0\n",
      "           Conv2d-18            [-1, 75, 8, 12]          67,500\n",
      "      BatchNorm2d-19            [-1, 75, 8, 12]             150\n",
      "        LeakyReLU-20            [-1, 75, 8, 12]               0\n",
      "           Conv2d-21            [-1, 50, 6, 10]          33,750\n",
      "      BatchNorm2d-22            [-1, 50, 6, 10]             100\n",
      "        LeakyReLU-23            [-1, 50, 6, 10]               0\n",
      "           Conv2d-24             [-1, 25, 4, 8]          11,250\n",
      "        LeakyReLU-25             [-1, 25, 4, 8]               0\n",
      "           Linear-26                    [-1, 1]             800\n",
      "             Tanh-27                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 204,840\n",
      "Trainable params: 204,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 23.73\n",
      "Forward/backward pass size (MB): 67.20\n",
      "Params size (MB): 0.78\n",
      "Estimated Total Size (MB): 91.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(hakisa, (3, 1080, 1920))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Jigoku(score):\n",
    "    # For the game Jigoku Kisetsukan: Sense of the Seasons\n",
    "\n",
    "    score = score.replace('S', '5').replace('s', '8').replace('e', '2').replace('O', '0').replace('B', '8').replace('o', '4').replace('b', '4')\n",
    "    score = score.replace('I', '1').replace('l', '1').replace('.', '')\n",
    "\n",
    "    try:\n",
    "        score = float(score)\n",
    "\n",
    "    except ValueError:\n",
    "            score = 1.0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop complete!\n",
      "Time spent: 105.62340354919434 seconds\n"
     ]
    }
   ],
   "source": [
    "# Exploration loop\n",
    "\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "for step in range(dataset.steps):\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "\n",
    "    command = hakisa(frame)\n",
    "\n",
    "    command = dataset.get_command(command.detach().cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    # If Hakisa acts too fast, the consequence for her action might not appear right now. Not that I wouldn't want her to be fast and efficient...\n",
    "\n",
    "    score = dataset.get_consequences(1008, 1429, 1723-1429, 1046-1008, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_Jigoku(score)\n",
    "\n",
    "    mult_score = dataset.get_consequences(933, 1536, 1723-1536, 978-933, tesseract_config='--psm 8')\n",
    "\n",
    "    mult_score = preprocess_Jigoku(mult_score)\n",
    "\n",
    "    life = dataset.get_consequences(849, 400, 498-400, 904-849, tesseract_config='--psm 8')\n",
    "\n",
    "    life = preprocess_Jigoku(life)\n",
    "\n",
    "    power = dataset.get_consequences(923, 405, 503-405, 978-923, tesseract_config='--psm 8')\n",
    "\n",
    "    power = preprocess_Jigoku(power)\n",
    "\n",
    "    aura = dataset.get_consequences(1001, 400, 1045-1001, 503-400, tesseract_config='--psm 8')\n",
    "\n",
    "    aura = preprocess_Jigoku(aura)\n",
    "    aura = aura/100\n",
    "\n",
    "    if life == 0:\n",
    "\n",
    "        try:\n",
    "                reward = -(100/(score * mult_score))\n",
    "        \n",
    "        except ZeroDivisionError:\n",
    "                reward = -10\n",
    "\n",
    "    else:\n",
    "\n",
    "        reward = (score * mult_score) + (power * aura)\n",
    "\n",
    "\n",
    "    #consequence = 0.0 # for testing\n",
    "\n",
    "    dataset.create_memory(1e-3, frame, command, reward)\n",
    "\n",
    "    if step == (dataset.steps - 1):\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en.wav', winsound.SND_FILENAME) # Because yes\n",
    "\n",
    "end = time()\n",
    "print(f\"Loop complete!\\nTime spent: {end-start} seconds\")\n",
    "\n",
    "del frame, command, score, mult_score, life, power, aura, reward, step, start, end\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_rest_1_en.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, this is where we got the CUDA RuntimeError. Try using resize in Dataset creator and adjust Hakisa accordingly.\n",
    "\n",
    "dataset.create_data_for_study()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "costs = []\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = None\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to continue the studying phase.\n",
    "\n",
    "params = torch.load(f'{save_path}/Hakisa_checkpoint.tar')\n",
    "start_epoch = params['Epoch'] + 1\n",
    "hakisa.load_state_dict(params['Hakisa_params'])\n",
    "lr = params['Hakisa_LR']\n",
    "\n",
    "del params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studying loop - Classic supervised learning. Will help Hakisa try to create certain patterns for situations and her reactions.\n",
    "\n",
    "import os\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.1)\n",
    "\n",
    "study_loss = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(dataset.steps):\n",
    "\n",
    "    for i, (input_frame, labels) in enumerate(dataloader):\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        output_command = hakisa(input_frame)\n",
    "\n",
    "        loss = study_loss(output_command, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        for n, p in hakisa.named_parameters():\n",
    "\n",
    "                if 'neuron.weight' in n:\n",
    "                    grads.append(torch.mean(p.grad))\n",
    "\n",
    "                if grad_clip is not None:\n",
    "                    p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        if loss.item() < best_loss:\n",
    "\n",
    "            best_loss = loss.item()\n",
    "            best_params = hakisa.state_dict()\n",
    "\n",
    "        #if i % checkpoint == 0:\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f\"{epoch}/{dataset.steps}\")\n",
    "        print(f\"Best Discriminator Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "\n",
    "        if save_path is None:\n",
    "            try:\n",
    "                os.mkdir(\"Hakisa\")\n",
    "                save_path = \"Hakisa\"\n",
    "            except:\n",
    "                save_path = \"Hakisa\"\n",
    "                \n",
    "        torch.save({\n",
    "            'Epoch': epoch,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "        print(\"Model saved!\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameplayLoss(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gameplay Loss function. Aims to allow backpropagation through scores and, thus, allow\n",
    "    optimization aiming to achieve best gameplay performance.\n",
    "\n",
    "    Have yet to be tested. The idea is to be used during normal playthrough, after study mode.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        model_output: the output generated by the model. Necessary for backpropagation.\n",
    "        reward: the reward obtained. Simply as that. Must have the same size as model_output (try .unsqueeze(-1))\n",
    "\n",
    "    In most games, the lowest possible score one can achieve is 0, while the best score possible is infinite.\n",
    "    So we could simply define a function whose range is [0, inf[ and go on with that.\n",
    "\n",
    "    In order to avoid great numbers and possible likelihood of absurd exploding gradients, we'll use a simple log function and its derivative.\n",
    "    This can make the function bad for optimization when the performance is too good, but might give it some help when it's too low.\n",
    "    \n",
    "    Afterall, it's easier to get better when you're bad than when you're a pro.\n",
    "\n",
    "    Of course, there are many possible ways to achieve certain score.\n",
    "    In Jigoku Kisetsukan, to achieve a score of 10,000, you can kill enemies, or simply scrape through bullets without killing anyone.\n",
    "\n",
    "    However, I'm still interested in giving it a chance.\n",
    "\n",
    "    PS: Remember that, when reward < 1, log(reward) will be negative.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, model_output, reward):\n",
    "\n",
    "        if reward == 0:\n",
    "            reward = 1e-10\n",
    "\n",
    "        ctx.save_for_backward(reward)\n",
    "        \n",
    "        reward = torch.log(reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        reward, = ctx.saved_tensors\n",
    "\n",
    "        reward = 1/reward\n",
    "\n",
    "        reward = torch.tensor(reward, device=device)\n",
    "\n",
    "        return grad_output * reward, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing loop - She learns as she plays...if our GameplayLoss function actually works.\n",
    "\n",
    "import keyboard\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_learnweaponskill_Rapier_2_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "rewards = []\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = None\n",
    "steps = 0\n",
    "save_point = 0\n",
    "\n",
    "gameplay_loss = GameplayLoss()\n",
    "\n",
    "while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    #hakisa.eval()\n",
    "    hakisa.zero_grad()\n",
    "\n",
    "    frame = dataset._grab_frame()\n",
    "\n",
    "    command_value = hakisa(frame)\n",
    "\n",
    "    command = dataset.get_command(command_value)\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    del command\n",
    "\n",
    "    score = dataset.get_consequences(1008, 1429, 1723-1429, 1046-1008, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_Jigoku(score)\n",
    "\n",
    "    mult_score = dataset.get_consequences(933, 1536, 1723-1536, 978-933, tesseract_config='--psm 8')\n",
    "\n",
    "    mult_score = preprocess_Jigoku(mult_score)\n",
    "\n",
    "    life = dataset.get_consequences(849, 400, 498-400, 904-849, tesseract_config='--psm 8')\n",
    "\n",
    "    life = preprocess_Jigoku(life)\n",
    "\n",
    "    power = dataset.get_consequences(923, 405, 503-405, 978-923, tesseract_config='--psm 8')\n",
    "\n",
    "    power = preprocess_Jigoku(power)\n",
    "\n",
    "    aura = dataset.get_consequences(1001, 400, 1045-1001, 503-400, tesseract_config='--psm 8')\n",
    "\n",
    "    aura = preprocess_Jigoku(aura)\n",
    "    aura = aura/100\n",
    "\n",
    "    if life == 0:\n",
    "\n",
    "        try:\n",
    "            reward = -(100/(score * mult_score))\n",
    "        \n",
    "        except ZeroDivisionError:\n",
    "            reward = -10\n",
    "\n",
    "    else:\n",
    "\n",
    "        reward = (score * mult_score) + (power * aura)\n",
    "\n",
    "    del score, mult_score, power, aura, life\n",
    "\n",
    "    reward = reward.unsqueeze(-1) # Make sure reward.size() == command_value.size(). Otherwise, you'll get an error.\n",
    "\n",
    "    loss = gameplay_loss.apply(command_value, reward)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    for n, p in hakisa.named_parameters():\n",
    "\n",
    "            if 'neuron.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "    best_loss = 0.0\n",
    "\n",
    "    if loss.item() > best_loss:\n",
    "\n",
    "        best_loss = loss.item()\n",
    "        best_params = hakisa.state_dict()\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

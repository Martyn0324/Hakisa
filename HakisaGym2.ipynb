{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting sources:\n",
    "- https://github.com/liuruoze/HierNet-SC2/blob/396646056dbe5f8f20e43e0ef35e59db09e907c0/algo/ppo.py#L180 - Ruo Ze Liu's PPO2 implementation (in Tensorflow)\n",
    "- https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/ppo2/ppo2.py#L185 - Original code from Stable Balines\n",
    "- https://medium.com/analytics-vidhya/coding-ppo-from-scratch-with-pytorch-part-1-4-613dfc1b14c8 - Wonderful tutorial by Eric Yang Yu (Pytorch), this implementation is for **CONTINUOUS** action spaces, which are most of Gym's Environments (Robots joints)\n",
    "- https://spinningup.openai.com/en/latest/spinningup/spinningup.html - Spinning Up, by OpenAI. Interesting hyperlinks\n",
    "- https://www.alexirpan.com/2018/02/14/rl-hard.html - Deep Reinforcement Learning Doesn't Work Yet. Problem is... I can't refuse the challenge of GANs or RLs...\n",
    "- https://lilianweng.github.io/posts/2018-02-19-rl-overview/ - Lilian Weng, OpeanAI Research Leader, with her glorious blogs which are basically equivalent to a review paper.\n",
    "\n",
    "**POSSIBLE ISSUES:** Usually, PPO is applied with 2 different networks, policy and value function. Considering Deep-RL instability, it's possible that the idea of uniting both in the same network, though appealing, may be troublesome.\n",
    "\n",
    "\"Also, what we know about good CNN design from supervised learning land doesn’t seem to apply to reinforcement learning land, because you’re mostly bottlenecked by credit assignment / supervision bitrate, not by a lack of a powerful representation. Your ResNets, batchnorms, or very deep networks have no power here.\" - Hacker News comment from Andrej Karpathy, back when he was at OpenAI (extracted from Alex Irpan's blog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectActor(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Simple Neural Network for testing\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mode='exploration'):\n",
    "\n",
    "        super(SubjectActor, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        # Input = 3x200x256\n",
    "\n",
    "        self.neuron1 = nn.Linear(3*200*256, 128)\n",
    "        self.neuron2 = nn.Linear(128, 128)\n",
    "        self.neuron3 = nn.Linear(128, 12)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Our environment is MultiBinary, so we'll use Sigmoid.\n",
    "        # MultiBinary Environment: For each button, determine if pressed or not.\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, obs):\n",
    "\n",
    "        x = obs.contiguous().view(obs.size(0), -1)\n",
    "\n",
    "        x = self.neuron1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.neuron2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        if self.mode == 'exploration': # Just collecting data\n",
    "            # Random actions preferred for diversity of states\n",
    "            # Alternative (and fancy) method: torch.distributions.Bernoulli()\n",
    "\n",
    "            fake_input = torch.randn_like(x, device=device)\n",
    "            actions = self.neuron3(fake_input)\n",
    "\n",
    "            del fake_input\n",
    "\n",
    "        else:\n",
    "\n",
    "            actions = self.neuron3(x)\n",
    "\n",
    "        actions = self.sigmoid(actions)\n",
    "\n",
    "        del x\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectCritic(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Simple Neural Network for testing\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(SubjectCritic, self).__init__()\n",
    "\n",
    "        # Input = 3x200x256\n",
    "\n",
    "        self.neuron1 = nn.Linear(3*200*256, 128)\n",
    "        self.neuron2 = nn.Linear(128, 128)\n",
    "        self.neuron3 = nn.Linear(128, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, obs):\n",
    "\n",
    "        x = obs.contiguous().view(obs.size(0), -1)\n",
    "\n",
    "        x = self.neuron1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.neuron2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        expected_reward = self.neuron3(x)\n",
    "\n",
    "        del x\n",
    "\n",
    "        return expected_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we'll use Value Function V(s) --> Depends only on state (s)\n",
    "# By default, both models are separate...and they're unstable enough.\n",
    "\n",
    "policy = SubjectActor().to(device)\n",
    "value_function = SubjectCritic().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Unfortunately, RL algorithms tend to be too sentimental,\n",
    "so we'll spend quite some time adjusting hyperparameters\n",
    "\n",
    "Some parameters used in Gym's environments, which are too simple:\n",
    "https://github.com/araffin/rl-baselines-zoo/blob/master/hyperparams/ppo2.yml\n",
    "\n",
    "For complex games, like we want for Hakisa, we should stick to something\n",
    "close to the parameters used by Ruo Ze Liu for HierNet, in StarCraft 2:\n",
    "\n",
    "https://github.com/liuruoze/HierNet-SC2/blob/396646056dbe5f8f20e43e0ef35e59db09e907c0/param.py\n",
    "\n",
    "\"Even Ignoring Generalization Issues, The Final Results Can be Unstable and Hard to Reproduce\" - Alex Irpan\n",
    "\n",
    "\"[Supervised learning] wants to work. Even if you screw something up you'll usually get something non-random back.\n",
    "RL must be forced to work.\n",
    "If you screw something up or don't tune something well enough you're exceedingly likely to get a policy that is even worse than random.\n",
    "And even if it's all well tuned you'll get a bad policy 30% of the time, just because.\" - Andrej Karpathy\n",
    "'''\n",
    "\n",
    "gamma = 0.99 # Gamma for the Discount Rewards.\n",
    "lamb = 0.95 # Lambda for Generalized Advantage Estimation. Together with gamma, basically a \"weight\" for Exponential Moving Average\n",
    "BATCH_SIZE = 16 # In reality, all tensors are batch 1, so we'll use gradient accumulation to simulate multiple batches.\n",
    "EPOCHS = 10\n",
    "lr = 2e-4\n",
    "value_weight = 1.0\n",
    "entropy_weight = 1e-2\n",
    "target_KLD = 0.05 # Early-Stopping parameter to avoid suboptimal policy in PPO2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In reality, the AI will only play the game to acquire data\n",
    "# The magic really happens during her training, which is done offline.\n",
    "\n",
    "'''\n",
    "\"We can call our data collected in each rollout a batch\"\n",
    "- Yu, Eric. Coding PPO From Scratch With PyTorch (2/4)\n",
    "https://medium.com/@eyyu/coding-ppo-from-scratch-with-pytorch-part-2-4-f9d8b8aa938a\n",
    "\n",
    "\"A trajectory is a sequence of states and actions in the world. [...]\n",
    "Trajectories are also frequently called episodes or rollouts.\"\n",
    "- OpenAI's Spinning Up: https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n",
    "'''\n",
    "\n",
    "# Creating lists to store data.\n",
    "# Using a separate cell in case of storing multiple episodes (playthroughs)\n",
    "# TO CONSIDER: Using pickle or torch to save and load playthroughs\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "log_probs = []\n",
    "values = []\n",
    "rewards = []\n",
    "rewards_to_go = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration Phase - Let her learn how the environment is.\n",
    "\n",
    "env = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\", state=\"ChunLiVsBlanka.1star\")\n",
    "obs = env.reset()\n",
    "obs = torch.from_numpy(obs)\n",
    "obs = obs/255\n",
    "obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "steps = 0\n",
    "\n",
    "while steps < 1000:\n",
    "    env.render()\n",
    "\n",
    "    # Collecting State --> Must be done at the beginning\n",
    "\n",
    "    states.append(obs.cpu())\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        log_prob = policy(obs)\n",
    "        value = value_function(obs)\n",
    "\n",
    "    action = log_prob.squeeze(0)\n",
    "\n",
    "    # MultiBinary Environment --> Only 0.0 or 1.0 accepted\n",
    "    bin = []\n",
    "    for x in action:\n",
    "        if x > 0.5:\n",
    "            bin.append(1.)\n",
    "        else:\n",
    "            bin.append(0.)\n",
    "    \n",
    "    action = bin # This is the actual action\n",
    "    log_prob = torch.log(torch.clamp(log_prob, 1e-10, 1.0)) # The probability distribution. Clipping to avoid NaN\n",
    "\n",
    "    del bin\n",
    "\n",
    "    obs, reward, end, info = env.step(action)\n",
    "    obs = torch.from_numpy(obs)\n",
    "    obs = obs/255\n",
    "    obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "    #reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    reward = (info['health']**(1+info['matches_won'])) - (info['enemy_health']**(1+info['enemy_matches_won']))\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "    reward = -(10.0/(torch.exp(reward) + 1.0)) + 5.0 # Normalizing to -5 to +5 (sigmoid function)\n",
    "\n",
    "    # Collecting variables for the previous (collected) state\n",
    "    \n",
    "    actions.append(action)\n",
    "    log_probs.append(log_prob.cpu())\n",
    "    values.append(value.cpu())\n",
    "    rewards.append(reward.cpu())\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing colected data\n",
    "\n",
    "states = torch.cat(states, 0)\n",
    "log_probs = torch.cat(log_probs, 0)\n",
    "\n",
    "discounted_reward = 0 # The final rewards provide greater impact on the algorithm\n",
    "for r_t in reversed(rewards):\n",
    "    \n",
    "    discounted_reward = r_t + discounted_reward * gamma\n",
    "    rewards_to_go.insert(0, discounted_reward)\n",
    "\n",
    "rewards_to_go = torch.tensor(rewards_to_go)\n",
    "\n",
    "\n",
    "# Calculating Generalized Advantage Estimation\n",
    "\n",
    "'''\n",
    "\"One kind of return is the finite-horizon undiscounted return, which is just the sum of rewards obtained in a fixed window of steps [...]\n",
    "Another kind of return is the infinite-horizon discounted return, which is the sum of all rewards ever obtained by the agent,\n",
    "but discounted by how far off in the future they're obtained.\n",
    "[...]\n",
    "Why would we ever want a discount factor, though? Don't we just want to get all rewards?\n",
    "We do, but the discount factor is both intuitively appealing and mathematically convenient.\n",
    "On an intuitive level: cash now is better than cash later.\"\n",
    "- OpenAI's Spinning Up: https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n",
    "'''\n",
    "\n",
    "deltas = []\n",
    "final_delta = rewards[-1] - values[-1]\n",
    "deltas.append(final_delta)\n",
    "\n",
    "for t in reversed(range(len(rewards)-1)):\n",
    "    \n",
    "    delta = rewards[t] + gamma * values[t+1] - values[t] # Future returns multiplied by gamma -> Uncertainty\n",
    "    deltas.insert(0, delta)\n",
    "\n",
    "gae = deepcopy(deltas)\n",
    "\n",
    "for t in reversed(range(len(gae)-1)):\n",
    "    gae[t] = gae[t] + gamma * lamb * gae[t+1]\n",
    "\n",
    "gae = torch.tensor(gae, dtype=torch.float)\n",
    "\n",
    "# Normalizing GAE\n",
    "# https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/ppo/ppo.py - Line 221\n",
    "\n",
    "gae = (gae - gae.mean())/(gae.std() + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10\n",
      "Current step: 100\n",
      "Current Loss: 17.62795066833496\n",
      "Surrogate Loss: 1.0507831573486328\tValue Loss: 16.58133888244629\tEntropy: 4.170949935913086\n",
      "Predicted Reward: 0.11062917113304138\tCurrent Reward: 4.182648181915283\n",
      "KL DIVERGENCE: 4.569689622258011e-07\n",
      "Policy Gradients: 0.00018324678239878267\n",
      "Reward Gradients: -0.009541415609419346\n",
      "\n",
      "\n",
      "0/10\n",
      "Current step: 200\n",
      "Current Loss: 128.87820434570312\n",
      "Surrogate Loss: 1.0656965970993042\tValue Loss: 127.81668090820312\tEntropy: 4.172884941101074\n",
      "Predicted Reward: 0.12138014286756516\tCurrent Reward: 11.426983833312988\n",
      "KL DIVERGENCE: 1.1076530199716217e-06\n",
      "Policy Gradients: -8.278109453385696e-05\n",
      "Reward Gradients: -0.14590764045715332\n",
      "\n",
      "\n",
      "0/10\n",
      "Current step: 300\n",
      "Current Loss: 25965.0234375\n",
      "Surrogate Loss: 2.528183937072754\tValue Loss: 25962.5\tEntropy: 4.175563812255859\n",
      "Predicted Reward: 0.11860451847314835\tCurrent Reward: 161.2474365234375\n",
      "KL DIVERGENCE: 3.159046173095703e-06\n",
      "Policy Gradients: 0.0001701192813925445\n",
      "Reward Gradients: -1.3525042533874512\n",
      "\n",
      "\n",
      "0/10\n",
      "Current step: 400\n",
      "Current Loss: 247773.65625\n",
      "Surrogate Loss: -0.7106889486312866\tValue Loss: 247774.359375\tEntropy: 4.167096138000488\n",
      "Predicted Reward: 0.1419488489627838\tCurrent Reward: 497.91131591796875\n",
      "KL DIVERGENCE: 3.904104232788086e-06\n",
      "Policy Gradients: 0.00015979385352693498\n",
      "Reward Gradients: -10.131048202514648\n",
      "\n",
      "\n",
      "0/10\n",
      "Current step: 500\n",
      "Current Loss: 245767.78125\n",
      "Surrogate Loss: -0.7102834582328796\tValue Loss: 245768.484375\tEntropy: 4.174962997436523\n",
      "Predicted Reward: 0.12293348461389542\tCurrent Reward: 495.87335205078125\n",
      "KL DIVERGENCE: 6.337960712698987e-06\n",
      "Policy Gradients: 2.4654087610542774e-05\n",
      "Reward Gradients: -2.6521053314208984\n",
      "\n",
      "\n",
      "0/10\n",
      "Current step: 600\n",
      "Current Loss: 240288.375\n",
      "Surrogate Loss: -0.7107805013656616\tValue Loss: 240289.078125\tEntropy: 4.169388771057129\n",
      "Predicted Reward: 0.11258644610643387\tCurrent Reward: 490.30548095703125\n",
      "KL DIVERGENCE: 6.397565357474377e-06\n",
      "Policy Gradients: -0.00012050192162860185\n",
      "Reward Gradients: -4.148527145385742\n",
      "\n",
      "\n",
      "0/10\n",
      "Current step: 700\n",
      "Current Loss: 225559.5\n",
      "Surrogate Loss: -0.7105817794799805\tValue Loss: 225560.203125\tEntropy: 4.1673784255981445\n",
      "Predicted Reward: 0.16242653131484985\tCurrent Reward: 475.0942077636719\n",
      "KL DIVERGENCE: 3.973643288190942e-06\n",
      "Policy Gradients: -9.351257176604122e-05\n",
      "Reward Gradients: -8.845019340515137\n",
      "\n",
      "\n",
      "0/10\n",
      "Current step: 800\n",
      "Current Loss: 187815.625\n",
      "Surrogate Loss: -0.7123014330863953\tValue Loss: 187816.34375\tEntropy: 4.169256687164307\n",
      "Predicted Reward: 0.15911129117012024\tCurrent Reward: 433.5369567871094\n",
      "KL DIVERGENCE: 2.389152996329358e-06\n",
      "Policy Gradients: 0.00019473917200230062\n",
      "Reward Gradients: -10.986156463623047\n",
      "\n",
      "\n",
      "0/10\n",
      "Current step: 900\n",
      "Current Loss: 101522.359375\n",
      "Surrogate Loss: -0.7111129760742188\tValue Loss: 101523.078125\tEntropy: 4.166967868804932\n",
      "Predicted Reward: 0.18722720444202423\tCurrent Reward: 318.8140869140625\n",
      "KL DIVERGENCE: 1.0728836059570312e-06\n",
      "Policy Gradients: -8.214029367081821e-05\n",
      "Reward Gradients: -2.226196527481079\n",
      "\n",
      "\n",
      "1/10\n",
      "Current step: 100\n",
      "Current Loss: 16.984315872192383\n",
      "Surrogate Loss: 1.0502779483795166\tValue Loss: 15.93820858001709\tEntropy: 4.171802043914795\n",
      "Predicted Reward: 0.1903795450925827\tCurrent Reward: 4.182648181915283\n",
      "KL DIVERGENCE: 1.4851491414447082e-06\n",
      "Policy Gradients: 0.00024390897306147963\n",
      "Reward Gradients: -0.021411139518022537\n",
      "\n",
      "\n",
      "1/10\n",
      "Current step: 200\n",
      "Current Loss: 127.380859375\n",
      "Surrogate Loss: 1.0645694732666016\tValue Loss: 126.32046508789062\tEntropy: 4.174677848815918\n",
      "Predicted Reward: 0.18774576485157013\tCurrent Reward: 11.426983833312988\n",
      "KL DIVERGENCE: 4.187226295471191e-06\n",
      "Policy Gradients: -0.0001806432119337842\n",
      "Reward Gradients: -0.1803695112466812\n",
      "\n",
      "\n",
      "1/10\n",
      "Current step: 300\n",
      "Current Loss: 25943.771484375\n",
      "Surrogate Loss: 2.528824806213379\tValue Loss: 25941.24609375\tEntropy: 4.175055503845215\n",
      "Predicted Reward: 0.1845695674419403\tCurrent Reward: 161.2474365234375\n",
      "KL DIVERGENCE: 4.251798145560315e-06\n",
      "Policy Gradients: -0.0002971813955809921\n",
      "Reward Gradients: -2.615400791168213\n",
      "\n",
      "\n",
      "1/10\n",
      "Current step: 400\n",
      "Current Loss: 247710.140625\n",
      "Surrogate Loss: -0.711368978023529\tValue Loss: 247710.859375\tEntropy: 4.165433883666992\n",
      "Predicted Reward: 0.2057102769613266\tCurrent Reward: 497.91131591796875\n",
      "KL DIVERGENCE: 2.7666490041156067e-06\n",
      "Policy Gradients: 0.00022135624021757394\n",
      "Reward Gradients: -17.806041717529297\n",
      "\n",
      "\n",
      "1/10\n",
      "Current step: 500\n",
      "Current Loss: 245713.8125\n",
      "Surrogate Loss: -0.7108789682388306\tValue Loss: 245714.515625\tEntropy: 4.173393249511719\n",
      "Predicted Reward: 0.17737668752670288\tCurrent Reward: 495.87335205078125\n",
      "KL DIVERGENCE: 4.06801700592041e-06\n",
      "Policy Gradients: 1.6472809875267558e-05\n",
      "Reward Gradients: -3.713508367538452\n",
      "\n",
      "\n",
      "1/10\n",
      "Current step: 600\n",
      "Current Loss: 240236.671875\n",
      "Surrogate Loss: -0.7112902998924255\tValue Loss: 240237.390625\tEntropy: 4.168203830718994\n",
      "Predicted Reward: 0.16530373692512512\tCurrent Reward: 490.30548095703125\n",
      "KL DIVERGENCE: 5.806486115034204e-06\n",
      "Policy Gradients: -9.613962174626067e-05\n",
      "Reward Gradients: -7.485535621643066\n",
      "\n",
      "\n",
      "1/10\n",
      "Current step: 700\n",
      "Current Loss: 225503.84375\n",
      "Surrogate Loss: -0.7111005783081055\tValue Loss: 225504.5625\tEntropy: 4.166072845458984\n",
      "Predicted Reward: 0.2210177779197693\tCurrent Reward: 475.0942077636719\n",
      "KL DIVERGENCE: 3.98854444938479e-06\n",
      "Policy Gradients: -9.725312702357769e-05\n",
      "Reward Gradients: -13.046503067016602\n",
      "\n",
      "\n",
      "1/10\n",
      "Current step: 800\n",
      "Current Loss: 187759.921875\n",
      "Surrogate Loss: -0.7130471467971802\tValue Loss: 187760.640625\tEntropy: 4.167542457580566\n",
      "Predicted Reward: 0.22339403629302979\tCurrent Reward: 433.5369567871094\n",
      "KL DIVERGENCE: 1.8527110796640045e-06\n",
      "Policy Gradients: 0.00019201263785362244\n",
      "Reward Gradients: -16.89885711669922\n",
      "\n",
      "\n",
      "1/10\n",
      "Current step: 900\n",
      "Current Loss: 101483.3671875\n",
      "Surrogate Loss: -0.7118377089500427\tValue Loss: 101484.0859375\tEntropy: 4.165200710296631\n",
      "Predicted Reward: 0.2484062761068344\tCurrent Reward: 318.8140869140625\n",
      "KL DIVERGENCE: 1.1722247563739074e-06\n",
      "Policy Gradients: -8.721595804672688e-05\n",
      "Reward Gradients: -2.9012930393218994\n",
      "\n",
      "\n",
      "2/10\n",
      "Current step: 100\n",
      "Current Loss: 16.54214859008789\n",
      "Surrogate Loss: 1.050052523612976\tValue Loss: 15.496267318725586\tEntropy: 4.172181129455566\n",
      "Predicted Reward: 0.2461182177066803\tCurrent Reward: 4.182648181915283\n",
      "KL DIVERGENCE: 2.299746029166272e-06\n",
      "Policy Gradients: 0.00024304176622536033\n",
      "Reward Gradients: -0.02812572568655014\n",
      "\n",
      "\n",
      "2/10\n",
      "Current step: 200\n",
      "Current Loss: 125.95175170898438\n",
      "Surrogate Loss: 1.063124656677246\tValue Loss: 124.8927993774414\tEntropy: 4.176935195922852\n",
      "Predicted Reward: 0.25143882632255554\tCurrent Reward: 11.426983833312988\n",
      "KL DIVERGENCE: 1.1081497177656274e-05\n",
      "Policy Gradients: -0.00025671455659903586\n",
      "Reward Gradients: -0.2099699229001999\n",
      "\n",
      "\n",
      "2/10\n",
      "Current step: 300\n",
      "Current Loss: 25923.9375\n",
      "Surrogate Loss: 2.528919219970703\tValue Loss: 25921.412109375\tEntropy: 4.174911975860596\n",
      "Predicted Reward: 0.24615585803985596\tCurrent Reward: 161.2474365234375\n",
      "KL DIVERGENCE: 6.601214408874512e-06\n",
      "Policy Gradients: -0.000596361409407109\n",
      "Reward Gradients: -3.742095708847046\n",
      "\n",
      "\n",
      "2/10\n",
      "Current step: 400\n",
      "Current Loss: 247648.359375\n",
      "Surrogate Loss: -0.711887776851654\tValue Loss: 247649.078125\tEntropy: 4.164146423339844\n",
      "Predicted Reward: 0.26777711510658264\tCurrent Reward: 497.91131591796875\n",
      "KL DIVERGENCE: 3.07460641124635e-06\n",
      "Policy Gradients: 0.00018593664572108537\n",
      "Reward Gradients: -22.47333526611328\n",
      "\n",
      "\n",
      "2/10\n",
      "Current step: 500\n",
      "Current Loss: 245651.53125\n",
      "Surrogate Loss: -0.7112369537353516\tValue Loss: 245652.25\tEntropy: 4.17243766784668\n",
      "Predicted Reward: 0.2401883900165558\tCurrent Reward: 495.87335205078125\n",
      "KL DIVERGENCE: 4.0978193283081055e-06\n",
      "Policy Gradients: -3.4164571843575686e-05\n",
      "Reward Gradients: -3.540964365005493\n",
      "\n",
      "\n",
      "2/10\n",
      "Current step: 600\n",
      "Current Loss: 240175.875\n",
      "Surrogate Loss: -0.7116235494613647\tValue Loss: 240176.59375\tEntropy: 4.1674089431762695\n",
      "Predicted Reward: 0.2273257076740265\tCurrent Reward: 490.30548095703125\n",
      "KL DIVERGENCE: 6.467103958129883e-06\n",
      "Policy Gradients: -0.00020457510254345834\n",
      "Reward Gradients: -10.090230941772461\n",
      "\n",
      "\n",
      "2/10\n",
      "Current step: 700\n",
      "Current Loss: 225447.609375\n",
      "Surrogate Loss: -0.7114734649658203\tValue Loss: 225448.328125\tEntropy: 4.165159702301025\n",
      "Predicted Reward: 0.2802109122276306\tCurrent Reward: 475.0942077636719\n",
      "KL DIVERGENCE: 4.8776469157019164e-06\n",
      "Policy Gradients: -0.0001567568106111139\n",
      "Reward Gradients: -16.061784744262695\n",
      "\n",
      "\n",
      "2/10\n",
      "Current step: 800\n",
      "Current Loss: 187707.0625\n",
      "Surrogate Loss: -0.7137138843536377\tValue Loss: 187707.78125\tEntropy: 4.165976524353027\n",
      "Predicted Reward: 0.2843955159187317\tCurrent Reward: 433.5369567871094\n",
      "KL DIVERGENCE: 3.4471354410925414e-06\n",
      "Policy Gradients: 0.00010508280684007332\n",
      "Reward Gradients: -20.616008758544922\n",
      "\n",
      "\n",
      "2/10\n",
      "Current step: 900\n",
      "Current Loss: 101446.375\n",
      "Surrogate Loss: -0.7125071287155151\tValue Loss: 101447.09375\tEntropy: 4.163510799407959\n",
      "Predicted Reward: 0.3064820170402527\tCurrent Reward: 318.8140869140625\n",
      "KL DIVERGENCE: 3.0249357223510742e-06\n",
      "Policy Gradients: -7.674646622035652e-05\n",
      "Reward Gradients: -3.5460762977600098\n",
      "\n",
      "\n",
      "3/10\n",
      "Current step: 100\n",
      "Current Loss: 16.103818893432617\n",
      "Surrogate Loss: 1.0496944189071655\tValue Loss: 15.058297157287598\tEntropy: 4.1727447509765625\n",
      "Predicted Reward: 0.30214595794677734\tCurrent Reward: 4.182648181915283\n",
      "KL DIVERGENCE: 3.6011140309710754e-06\n",
      "Policy Gradients: 0.0001701528817648068\n",
      "Reward Gradients: -0.03095712698996067\n",
      "\n",
      "\n",
      "3/10\n",
      "Current step: 200\n",
      "Current Loss: 124.60574340820312\n",
      "Surrogate Loss: 1.061589002609253\tValue Loss: 123.54833221435547\tEntropy: 4.179365634918213\n",
      "Predicted Reward: 0.3117542862892151\tCurrent Reward: 11.426983833312988\n",
      "KL DIVERGENCE: 2.2619962692260742e-05\n",
      "Policy Gradients: -0.00040473570697940886\n",
      "Reward Gradients: -0.21293075382709503\n",
      "\n",
      "\n",
      "3/10\n",
      "Current step: 300\n",
      "Current Loss: 25905.005859375\n",
      "Surrogate Loss: 2.5285115242004395\tValue Loss: 25902.48046875\tEntropy: 4.175105571746826\n",
      "Predicted Reward: 0.304963082075119\tCurrent Reward: 161.2474365234375\n",
      "KL DIVERGENCE: 9.382764801557641e-06\n",
      "Policy Gradients: -0.0009714692132547498\n",
      "Reward Gradients: -4.1244001388549805\n",
      "\n",
      "\n",
      "3/10\n",
      "Current step: 400\n",
      "Current Loss: 247592.078125\n",
      "Surrogate Loss: -0.7123184204101562\tValue Loss: 247592.796875\tEntropy: 4.163080215454102\n",
      "Predicted Reward: 0.32434555888175964\tCurrent Reward: 497.91131591796875\n",
      "KL DIVERGENCE: 4.207094661978772e-06\n",
      "Policy Gradients: 9.144016803475097e-05\n",
      "Reward Gradients: -26.558443069458008\n",
      "\n",
      "\n",
      "3/10\n",
      "Current step: 500\n",
      "Current Loss: 245589.90625\n",
      "Surrogate Loss: -0.7115890979766846\tValue Loss: 245590.625\tEntropy: 4.171551704406738\n",
      "Predicted Reward: 0.30235204100608826\tCurrent Reward: 495.87335205078125\n",
      "KL DIVERGENCE: 4.907449238089612e-06\n",
      "Policy Gradients: -3.281506724306382e-05\n",
      "Reward Gradients: -3.9035539627075195\n",
      "\n",
      "\n",
      "3/10\n",
      "Current step: 600\n",
      "Current Loss: 240117.5\n",
      "Surrogate Loss: -0.7119345664978027\tValue Loss: 240118.21875\tEntropy: 4.1666483879089355\n",
      "Predicted Reward: 0.286899209022522\tCurrent Reward: 490.30548095703125\n",
      "KL DIVERGENCE: 7.162491783674341e-06\n",
      "Policy Gradients: -0.0002359332429477945\n",
      "Reward Gradients: -12.24471664428711\n",
      "\n",
      "\n",
      "3/10\n",
      "Current step: 700\n",
      "Current Loss: 225395.484375\n",
      "Surrogate Loss: -0.7117627859115601\tValue Loss: 225396.203125\tEntropy: 4.164501190185547\n",
      "Predicted Reward: 0.33509933948516846\tCurrent Reward: 475.0942077636719\n",
      "KL DIVERGENCE: 5.240241989667993e-06\n",
      "Policy Gradients: -0.00012290388986002654\n",
      "Reward Gradients: -18.92391586303711\n",
      "\n",
      "\n",
      "3/10\n",
      "Current step: 800\n",
      "Current Loss: 187659.65625\n",
      "Surrogate Loss: -0.7142881751060486\tValue Loss: 187660.375\tEntropy: 4.164579391479492\n",
      "Predicted Reward: 0.3391008675098419\tCurrent Reward: 433.5369567871094\n",
      "KL DIVERGENCE: 6.640951141889673e-06\n",
      "Policy Gradients: 7.581526006106287e-05\n",
      "Reward Gradients: -24.43804359436035\n",
      "\n",
      "\n",
      "3/10\n",
      "Current step: 900\n",
      "Current Loss: 101411.1328125\n",
      "Surrogate Loss: -0.7131292223930359\tValue Loss: 101411.8515625\tEntropy: 4.161929130554199\n",
      "Predicted Reward: 0.36180195212364197\tCurrent Reward: 318.8140869140625\n",
      "KL DIVERGENCE: 6.6061816141882446e-06\n",
      "Policy Gradients: -5.8056994021171704e-05\n",
      "Reward Gradients: -3.823709726333618\n",
      "\n",
      "\n",
      "4/10\n",
      "Current step: 100\n",
      "Current Loss: 15.664581298828125\n",
      "Surrogate Loss: 1.0493719577789307\tValue Loss: 14.619382858276367\tEntropy: 4.173218727111816\n",
      "Predicted Reward: 0.35911810398101807\tCurrent Reward: 4.182648181915283\n",
      "KL DIVERGENCE: 5.533298008231213e-06\n",
      "Policy Gradients: 0.00014150456991046667\n",
      "Reward Gradients: -0.0443294458091259\n",
      "\n",
      "\n",
      "4/10\n",
      "Current step: 200\n",
      "Current Loss: 123.27717590332031\n",
      "Surrogate Loss: 1.0601210594177246\tValue Loss: 122.22123718261719\tEntropy: 4.181757926940918\n",
      "Predicted Reward: 0.37161290645599365\tCurrent Reward: 11.426983833312988\n",
      "KL DIVERGENCE: 3.834068775177002e-05\n",
      "Policy Gradients: -0.0004288169729989022\n",
      "Reward Gradients: -0.2163364142179489\n",
      "\n",
      "\n",
      "4/10\n",
      "Current step: 300\n",
      "Current Loss: 25887.171875\n",
      "Surrogate Loss: 2.5278120040893555\tValue Loss: 25884.6484375\tEntropy: 4.175543785095215\n",
      "Predicted Reward: 0.36036911606788635\tCurrent Reward: 161.2474365234375\n",
      "KL DIVERGENCE: 1.212457846122561e-05\n",
      "Policy Gradients: -0.0012720394879579544\n",
      "Reward Gradients: -4.633233070373535\n",
      "\n",
      "\n",
      "4/10\n",
      "Current step: 400\n",
      "Current Loss: 247535.40625\n",
      "Surrogate Loss: -0.712714672088623\tValue Loss: 247536.125\tEntropy: 4.162105083465576\n",
      "Predicted Reward: 0.381271630525589\tCurrent Reward: 497.91131591796875\n",
      "KL DIVERGENCE: 6.164113983686548e-06\n",
      "Policy Gradients: 7.3047976911766455e-06\n",
      "Reward Gradients: -28.610244750976562\n",
      "\n",
      "\n",
      "4/10\n",
      "Current step: 500\n",
      "Current Loss: 245531.046875\n",
      "Surrogate Loss: -0.7118425369262695\tValue Loss: 245531.765625\tEntropy: 4.170940399169922\n",
      "Predicted Reward: 0.3617292046546936\tCurrent Reward: 495.87335205078125\n",
      "KL DIVERGENCE: 6.84956739860354e-06\n",
      "Policy Gradients: 2.7186331863049418e-05\n",
      "Reward Gradients: -4.911856174468994\n",
      "\n",
      "\n",
      "4/10\n",
      "Current step: 600\n",
      "Current Loss: 240057.234375\n",
      "Surrogate Loss: -0.7122200727462769\tValue Loss: 240057.953125\tEntropy: 4.165943622589111\n",
      "Predicted Reward: 0.3483790159225464\tCurrent Reward: 490.30548095703125\n",
      "KL DIVERGENCE: 8.101264938886743e-06\n",
      "Policy Gradients: -0.00018432042270433158\n",
      "Reward Gradients: -13.024227142333984\n",
      "\n",
      "\n",
      "4/10\n",
      "Current step: 700\n",
      "Current Loss: 225341.875\n",
      "Surrogate Loss: -0.7120170593261719\tValue Loss: 225342.59375\tEntropy: 4.163912773132324\n",
      "Predicted Reward: 0.39155593514442444\tCurrent Reward: 475.0942077636719\n",
      "KL DIVERGENCE: 5.602836608886719e-06\n",
      "Policy Gradients: -0.00010386099165771157\n",
      "Reward Gradients: -20.52629280090332\n",
      "\n",
      "\n",
      "4/10\n",
      "Current step: 800\n",
      "Current Loss: 187610.90625\n",
      "Surrogate Loss: -0.7147642970085144\tValue Loss: 187611.625\tEntropy: 4.163409233093262\n",
      "Predicted Reward: 0.39539390802383423\tCurrent Reward: 433.5369567871094\n",
      "KL DIVERGENCE: 1.0634462341840845e-05\n",
      "Policy Gradients: 8.484157297061756e-05\n",
      "Reward Gradients: -26.640169143676758\n",
      "\n",
      "\n",
      "4/10\n",
      "Current step: 900\n",
      "Current Loss: 101375.25\n",
      "Surrogate Loss: -0.7136337161064148\tValue Loss: 101375.96875\tEntropy: 4.160640239715576\n",
      "Predicted Reward: 0.4181582033634186\tCurrent Reward: 318.8140869140625\n",
      "KL DIVERGENCE: 1.093745231628418e-05\n",
      "Policy Gradients: -2.187959944421891e-05\n",
      "Reward Gradients: -4.02708101272583\n",
      "\n",
      "\n",
      "5/10\n",
      "Current step: 100\n",
      "Current Loss: 15.23612117767334\n",
      "Surrogate Loss: 1.0490273237228394\tValue Loss: 14.191267967224121\tEntropy: 4.17378044128418\n",
      "Predicted Reward: 0.41551825404167175\tCurrent Reward: 4.182648181915283\n",
      "KL DIVERGENCE: 7.54992197471438e-06\n",
      "Policy Gradients: 7.001187623245642e-05\n",
      "Reward Gradients: -0.043716318905353546\n",
      "\n",
      "\n",
      "5/10\n",
      "Current step: 200\n",
      "Current Loss: 121.9589614868164\n",
      "Surrogate Loss: 1.058669090270996\tValue Loss: 120.90447235107422\tEntropy: 4.184150695800781\n",
      "Predicted Reward: 0.43132704496383667\tCurrent Reward: 11.426983833312988\n",
      "KL DIVERGENCE: 5.7617824495537207e-05\n",
      "Policy Gradients: -0.0004951017908751965\n",
      "Reward Gradients: -0.23246850073337555\n",
      "\n",
      "\n",
      "5/10\n",
      "Current step: 300\n",
      "Current Loss: 25869.357421875\n",
      "Surrogate Loss: 2.527268886566162\tValue Loss: 25866.833984375\tEntropy: 4.175889015197754\n",
      "Predicted Reward: 0.41573643684387207\tCurrent Reward: 161.2474365234375\n",
      "KL DIVERGENCE: 1.4821688637312036e-05\n",
      "Policy Gradients: -0.0017822741065174341\n",
      "Reward Gradients: -4.771978378295898\n",
      "\n",
      "\n",
      "5/10\n",
      "Current step: 400\n",
      "Current Loss: 247482.359375\n",
      "Surrogate Loss: -0.7131426334381104\tValue Loss: 247483.078125\tEntropy: 4.1610798835754395\n",
      "Predicted Reward: 0.43460512161254883\tCurrent Reward: 497.91131591796875\n",
      "KL DIVERGENCE: 8.945664376369677e-06\n",
      "Policy Gradients: 1.7542473869980313e-05\n",
      "Reward Gradients: -30.45610809326172\n",
      "\n",
      "\n",
      "5/10\n",
      "Current step: 500\n",
      "Current Loss: 245470.8125\n",
      "Surrogate Loss: -0.7120722532272339\tValue Loss: 245471.53125\tEntropy: 4.1703782081604\n",
      "Predicted Reward: 0.4225007891654968\tCurrent Reward: 495.87335205078125\n",
      "KL DIVERGENCE: 9.08970832824707e-06\n",
      "Policy Gradients: -5.421907189884223e-06\n",
      "Reward Gradients: -5.028033256530762\n",
      "\n",
      "\n",
      "5/10\n",
      "Current step: 600\n",
      "Current Loss: 239995.8125\n",
      "Surrogate Loss: -0.7125198245048523\tValue Loss: 239996.53125\tEntropy: 4.165205001831055\n",
      "Predicted Reward: 0.41105717420578003\tCurrent Reward: 490.30548095703125\n",
      "KL DIVERGENCE: 8.945664376369677e-06\n",
      "Policy Gradients: -0.00014573206135537475\n",
      "Reward Gradients: -13.333263397216797\n",
      "\n",
      "\n",
      "5/10\n",
      "Current step: 700\n",
      "Current Loss: 225284.65625\n",
      "Surrogate Loss: -0.7123557925224304\tValue Loss: 225285.375\tEntropy: 4.163128852844238\n",
      "Predicted Reward: 0.4518349766731262\tCurrent Reward: 475.0942077636719\n",
      "KL DIVERGENCE: 6.660819053649902e-06\n",
      "Policy Gradients: -0.00012391500058583915\n",
      "Reward Gradients: -21.921871185302734\n",
      "\n",
      "\n",
      "5/10\n",
      "Current step: 800\n",
      "Current Loss: 187559.71875\n",
      "Surrogate Loss: -0.7152304649353027\tValue Loss: 187560.4375\tEntropy: 4.162271499633789\n",
      "Predicted Reward: 0.45445945858955383\tCurrent Reward: 433.5369567871094\n",
      "KL DIVERGENCE: 1.519918441772461e-05\n",
      "Policy Gradients: 0.00010829757957253605\n",
      "Reward Gradients: -29.23736000061035\n",
      "\n",
      "\n",
      "5/10\n",
      "Current step: 900\n",
      "Current Loss: 101336.8984375\n",
      "Surrogate Loss: -0.7141086459159851\tValue Loss: 101337.6171875\tEntropy: 4.1594367027282715\n",
      "Predicted Reward: 0.47839733958244324\tCurrent Reward: 318.8140869140625\n",
      "KL DIVERGENCE: 1.5988945960998535e-05\n",
      "Policy Gradients: -1.3036255040788092e-05\n",
      "Reward Gradients: -4.2676591873168945\n",
      "\n",
      "\n",
      "6/10\n",
      "Current step: 100\n",
      "Current Loss: 14.826186180114746\n",
      "Surrogate Loss: 1.0486387014389038\tValue Loss: 13.781722068786621\tEntropy: 4.174465656280518\n",
      "Predicted Reward: 0.47027403116226196\tCurrent Reward: 4.182648181915283\n",
      "KL DIVERGENCE: 9.28342342376709e-06\n",
      "Policy Gradients: 7.393374835373834e-05\n",
      "Reward Gradients: -0.04504040628671646\n",
      "\n",
      "\n",
      "6/10\n",
      "Current step: 200\n",
      "Current Loss: 120.58348846435547\n",
      "Surrogate Loss: 1.0573818683624268\tValue Loss: 119.5302963256836\tEntropy: 4.186320781707764\n",
      "Predicted Reward: 0.4939931035041809\tCurrent Reward: 11.426983833312988\n",
      "KL DIVERGENCE: 7.7376767876558e-05\n",
      "Policy Gradients: -0.0005773534066975117\n",
      "Reward Gradients: -0.22487910091876984\n",
      "\n",
      "\n",
      "6/10\n",
      "Current step: 300\n",
      "Current Loss: 25851.3125\n",
      "Surrogate Loss: 2.5267813205718994\tValue Loss: 25848.7890625\tEntropy: 4.176224708557129\n",
      "Predicted Reward: 0.47184842824935913\tCurrent Reward: 161.2474365234375\n",
      "KL DIVERGENCE: 1.7156204194179736e-05\n",
      "Policy Gradients: -0.00212110485881567\n",
      "Reward Gradients: -4.881255626678467\n",
      "\n",
      "\n",
      "6/10\n",
      "Current step: 400\n",
      "Current Loss: 247424.015625\n",
      "Surrogate Loss: -0.7136403918266296\tValue Loss: 247424.734375\tEntropy: 4.159877300262451\n",
      "Predicted Reward: 0.49326837062835693\tCurrent Reward: 497.91131591796875\n",
      "KL DIVERGENCE: 1.2755393981933594e-05\n",
      "Policy Gradients: 8.574424282414839e-05\n",
      "Reward Gradients: -33.613468170166016\n",
      "\n",
      "\n",
      "6/10\n",
      "Current step: 500\n",
      "Current Loss: 245410.8125\n",
      "Surrogate Loss: -0.7124150991439819\tValue Loss: 245411.53125\tEntropy: 4.169517517089844\n",
      "Predicted Reward: 0.48307520151138306\tCurrent Reward: 495.87335205078125\n",
      "KL DIVERGENCE: 1.0515253052290063e-05\n",
      "Policy Gradients: 2.8966032914468087e-05\n",
      "Reward Gradients: -5.367432594299316\n",
      "\n",
      "\n",
      "6/10\n",
      "Current step: 600\n",
      "Current Loss: 239935.0625\n",
      "Surrogate Loss: -0.712806224822998\tValue Loss: 239935.78125\tEntropy: 4.164453506469727\n",
      "Predicted Reward: 0.4730849862098694\tCurrent Reward: 490.30548095703125\n",
      "KL DIVERGENCE: 9.998679161071777e-06\n",
      "Policy Gradients: -6.065910929464735e-05\n",
      "Reward Gradients: -13.898476600646973\n",
      "\n",
      "\n",
      "6/10\n",
      "Current step: 700\n",
      "Current Loss: 225228.453125\n",
      "Surrogate Loss: -0.7126628160476685\tValue Loss: 225229.171875\tEntropy: 4.162410736083984\n",
      "Predicted Reward: 0.5110601782798767\tCurrent Reward: 475.0942077636719\n",
      "KL DIVERGENCE: 7.91748425399419e-06\n",
      "Policy Gradients: -5.775510726380162e-05\n",
      "Reward Gradients: -23.90334129333496\n",
      "\n",
      "\n",
      "6/10\n",
      "Current step: 800\n",
      "Current Loss: 187507.203125\n",
      "Surrogate Loss: -0.7155143618583679\tValue Loss: 187507.921875\tEntropy: 4.161645889282227\n",
      "Predicted Reward: 0.5151192545890808\tCurrent Reward: 433.5369567871094\n",
      "KL DIVERGENCE: 1.900891584227793e-05\n",
      "Policy Gradients: 0.00027604433125816286\n",
      "Reward Gradients: -30.607816696166992\n",
      "\n",
      "\n",
      "6/10\n",
      "Current step: 900\n",
      "Current Loss: 101297.5703125\n",
      "Surrogate Loss: -0.7145428657531738\tValue Loss: 101298.2890625\tEntropy: 4.1583356857299805\n",
      "Predicted Reward: 0.5401756167411804\tCurrent Reward: 318.8140869140625\n",
      "KL DIVERGENCE: 2.1497409761650488e-05\n",
      "Policy Gradients: -1.7489652236690745e-05\n",
      "Reward Gradients: -4.50455904006958\n",
      "\n",
      "\n",
      "7/10\n",
      "Current step: 100\n",
      "Current Loss: 14.41549015045166\n",
      "Surrogate Loss: 1.0482895374298096\tValue Loss: 13.371376037597656\tEntropy: 4.175055027008057\n",
      "Predicted Reward: 0.5259590148925781\tCurrent Reward: 4.182648181915283\n",
      "KL DIVERGENCE: 1.0619561180646997e-05\n",
      "Policy Gradients: -3.672872480819933e-05\n",
      "Reward Gradients: -0.044948264956474304\n",
      "\n",
      "\n",
      "7/10\n",
      "Current step: 200\n",
      "Current Loss: 119.19963836669922\n",
      "Surrogate Loss: 1.0562593936920166\tValue Loss: 118.14756774902344\tEntropy: 4.188164710998535\n",
      "Predicted Reward: 0.5574135184288025\tCurrent Reward: 11.426983833312988\n",
      "KL DIVERGENCE: 9.644528472563252e-05\n",
      "Policy Gradients: -0.0005737834144383669\n",
      "Reward Gradients: -0.2521015703678131\n",
      "\n",
      "\n",
      "7/10\n",
      "Current step: 300\n",
      "Current Loss: 25832.537109375\n",
      "Surrogate Loss: 2.526094913482666\tValue Loss: 25830.015625\tEntropy: 4.176755428314209\n",
      "Predicted Reward: 0.5302442908287048\tCurrent Reward: 161.2474365234375\n",
      "KL DIVERGENCE: 1.8825134247890674e-05\n",
      "Policy Gradients: -0.0024145825300365686\n",
      "Reward Gradients: -5.2542243003845215\n",
      "\n",
      "\n",
      "7/10\n",
      "Current step: 400\n",
      "Current Loss: 247366.109375\n",
      "Surrogate Loss: -0.7139986753463745\tValue Loss: 247366.828125\tEntropy: 4.159086227416992\n",
      "Predicted Reward: 0.5514649748802185\tCurrent Reward: 497.91131591796875\n",
      "KL DIVERGENCE: 1.58846378326416e-05\n",
      "Policy Gradients: 0.000172813146491535\n",
      "Reward Gradients: -35.42074203491211\n",
      "\n",
      "\n",
      "7/10\n",
      "Current step: 500\n",
      "Current Loss: 245352.109375\n",
      "Surrogate Loss: -0.7128677368164062\tValue Loss: 245352.828125\tEntropy: 4.168359279632568\n",
      "Predicted Reward: 0.5423392057418823\tCurrent Reward: 495.87335205078125\n",
      "KL DIVERGENCE: 1.1642774552456103e-05\n",
      "Policy Gradients: 9.311378380516544e-05\n",
      "Reward Gradients: -5.559447765350342\n",
      "\n",
      "\n",
      "7/10\n",
      "Current step: 600\n",
      "Current Loss: 239877.0625\n",
      "Surrogate Loss: -0.7131306529045105\tValue Loss: 239877.78125\tEntropy: 4.163633346557617\n",
      "Predicted Reward: 0.5322819352149963\tCurrent Reward: 490.30548095703125\n",
      "KL DIVERGENCE: 1.0386109352111816e-05\n",
      "Policy Gradients: 9.775667422218248e-05\n",
      "Reward Gradients: -15.21203327178955\n",
      "\n",
      "\n",
      "7/10\n",
      "Current step: 700\n",
      "Current Loss: 225175.453125\n",
      "Surrogate Loss: -0.7128710150718689\tValue Loss: 225176.171875\tEntropy: 4.161988735198975\n",
      "Predicted Reward: 0.5668995976448059\tCurrent Reward: 475.0942077636719\n",
      "KL DIVERGENCE: 8.781751603237353e-06\n",
      "Policy Gradients: -6.995959847699851e-05\n",
      "Reward Gradients: -24.32442855834961\n",
      "\n",
      "\n",
      "7/10\n",
      "Current step: 800\n",
      "Current Loss: 187456.59375\n",
      "Surrogate Loss: -0.7157413363456726\tValue Loss: 187457.3125\tEntropy: 4.161128044128418\n",
      "Predicted Reward: 0.5735465288162231\tCurrent Reward: 433.5369567871094\n",
      "KL DIVERGENCE: 2.1790465325466357e-05\n",
      "Policy Gradients: 0.0002485770673956722\n",
      "Reward Gradients: -31.418733596801758\n",
      "\n",
      "\n",
      "7/10\n",
      "Current step: 900\n",
      "Current Loss: 101260.8125\n",
      "Surrogate Loss: -0.7150020599365234\tValue Loss: 101261.5390625\tEntropy: 4.157184600830078\n",
      "Predicted Reward: 0.5978974103927612\tCurrent Reward: 318.8140869140625\n",
      "KL DIVERGENCE: 2.6966135919792578e-05\n",
      "Policy Gradients: -1.8868257029680535e-05\n",
      "Reward Gradients: -4.453031539916992\n",
      "\n",
      "\n",
      "8/10\n",
      "Current step: 100\n",
      "Current Loss: 14.017327308654785\n",
      "Surrogate Loss: 1.0479257106781006\tValue Loss: 12.973577499389648\tEntropy: 4.17566442489624\n",
      "Predicted Reward: 0.5807629823684692\tCurrent Reward: 4.182648181915283\n",
      "KL DIVERGENCE: 1.119077205657959e-05\n",
      "Policy Gradients: -5.638942093355581e-05\n",
      "Reward Gradients: -0.04814133793115616\n",
      "\n",
      "\n",
      "8/10\n",
      "Current step: 200\n",
      "Current Loss: 117.99424743652344\n",
      "Surrogate Loss: 1.0553232431411743\tValue Loss: 116.943115234375\tEntropy: 4.189643383026123\n",
      "Predicted Reward: 0.6129602193832397\tCurrent Reward: 11.426983833312988\n",
      "KL DIVERGENCE: 0.00011093418288510293\n",
      "Policy Gradients: -0.0007196388323791325\n",
      "Reward Gradients: -0.2947940230369568\n",
      "\n",
      "\n",
      "8/10\n",
      "Current step: 300\n",
      "Current Loss: 25814.025390625\n",
      "Surrogate Loss: 2.525489330291748\tValue Loss: 25811.50390625\tEntropy: 4.177217483520508\n",
      "Predicted Reward: 0.5878481268882751\tCurrent Reward: 161.2474365234375\n",
      "KL DIVERGENCE: 1.990298551390879e-05\n",
      "Policy Gradients: -0.002626331988722086\n",
      "Reward Gradients: -5.596007823944092\n",
      "\n",
      "\n",
      "8/10\n",
      "Current step: 400\n",
      "Current Loss: 247307.890625\n",
      "Surrogate Loss: -0.7144119143486023\tValue Loss: 247308.609375\tEntropy: 4.158143520355225\n",
      "Predicted Reward: 0.6099911332130432\tCurrent Reward: 497.91131591796875\n",
      "KL DIVERGENCE: 1.7831724107963964e-05\n",
      "Policy Gradients: 0.00015957787400111556\n",
      "Reward Gradients: -35.86185073852539\n",
      "\n",
      "\n",
      "8/10\n",
      "Current step: 500\n",
      "Current Loss: 245295.15625\n",
      "Surrogate Loss: -0.7133347988128662\tValue Loss: 245295.875\tEntropy: 4.167150497436523\n",
      "Predicted Reward: 0.5998123288154602\tCurrent Reward: 495.87335205078125\n",
      "KL DIVERGENCE: 1.2626251191250049e-05\n",
      "Policy Gradients: 0.00011446354619693011\n",
      "Reward Gradients: -5.429219722747803\n",
      "\n",
      "\n",
      "8/10\n",
      "Current step: 600\n",
      "Current Loss: 239820.859375\n",
      "Surrogate Loss: -0.7133578062057495\tValue Loss: 239821.578125\tEntropy: 4.163078308105469\n",
      "Predicted Reward: 0.5896722078323364\tCurrent Reward: 490.30548095703125\n",
      "KL DIVERGENCE: 9.829799637373071e-06\n",
      "Policy Gradients: 0.00017373860464431345\n",
      "Reward Gradients: -16.7037410736084\n",
      "\n",
      "\n",
      "8/10\n",
      "Current step: 700\n",
      "Current Loss: 225124.59375\n",
      "Surrogate Loss: -0.7131479978561401\tValue Loss: 225125.3125\tEntropy: 4.161342620849609\n",
      "Predicted Reward: 0.6204936504364014\tCurrent Reward: 475.0942077636719\n",
      "KL DIVERGENCE: 9.45727060752688e-06\n",
      "Policy Gradients: -8.581935981055722e-05\n",
      "Reward Gradients: -24.533599853515625\n",
      "\n",
      "\n",
      "8/10\n",
      "Current step: 800\n",
      "Current Loss: 187408.3125\n",
      "Surrogate Loss: -0.7160804271697998\tValue Loss: 187409.03125\tEntropy: 4.160269737243652\n",
      "Predicted Reward: 0.6293109655380249\tCurrent Reward: 433.5369567871094\n",
      "KL DIVERGENCE: 2.4775665224296972e-05\n",
      "Policy Gradients: 0.0002128822379745543\n",
      "Reward Gradients: -31.532299041748047\n",
      "\n",
      "\n",
      "8/10\n",
      "Current step: 900\n",
      "Current Loss: 101225.4921875\n",
      "Surrogate Loss: -0.715507984161377\tValue Loss: 101226.21875\tEntropy: 4.155899524688721\n",
      "Predicted Reward: 0.6534247398376465\tCurrent Reward: 318.8140869140625\n",
      "KL DIVERGENCE: 3.1828880310058594e-05\n",
      "Policy Gradients: 1.4014281987329014e-05\n",
      "Reward Gradients: -4.440290451049805\n",
      "\n",
      "\n",
      "9/10\n",
      "Current step: 100\n",
      "Current Loss: 13.630019187927246\n",
      "Surrogate Loss: 1.0475437641143799\tValue Loss: 12.586651802062988\tEntropy: 4.1763224601745605\n",
      "Predicted Reward: 0.6348809599876404\tCurrent Reward: 4.182648181915283\n",
      "KL DIVERGENCE: 1.2040138244628906e-05\n",
      "Policy Gradients: -8.954869554145262e-05\n",
      "Reward Gradients: -0.046374477446079254\n",
      "\n",
      "\n",
      "9/10\n",
      "Current step: 200\n",
      "Current Loss: 116.85726165771484\n",
      "Surrogate Loss: 1.054388403892517\tValue Loss: 115.80706024169922\tEntropy: 4.191108703613281\n",
      "Predicted Reward: 0.6656148433685303\tCurrent Reward: 11.426983833312988\n",
      "KL DIVERGENCE: 0.00012344619608484209\n",
      "Policy Gradients: -0.0006658040801994503\n",
      "Reward Gradients: -0.31415796279907227\n",
      "\n",
      "\n",
      "9/10\n",
      "Current step: 300\n",
      "Current Loss: 25797.142578125\n",
      "Surrogate Loss: 2.525348424911499\tValue Loss: 25794.62109375\tEntropy: 4.177310466766357\n",
      "Predicted Reward: 0.6403972506523132\tCurrent Reward: 161.2474365234375\n",
      "KL DIVERGENCE: 1.9436081856838427e-05\n",
      "Policy Gradients: -0.0026665080804377794\n",
      "Reward Gradients: -5.939541816711426\n",
      "\n",
      "\n",
      "9/10\n",
      "Current step: 400\n",
      "Current Loss: 247252.65625\n",
      "Surrogate Loss: -0.7148770093917847\tValue Loss: 247253.375\tEntropy: 4.157033443450928\n",
      "Predicted Reward: 0.6655252575874329\tCurrent Reward: 497.91131591796875\n",
      "KL DIVERGENCE: 1.9997358322143555e-05\n",
      "Policy Gradients: 8.306011295644566e-05\n",
      "Reward Gradients: -36.34944152832031\n",
      "\n",
      "\n",
      "9/10\n",
      "Current step: 500\n",
      "Current Loss: 245241.28125\n",
      "Surrogate Loss: -0.713890552520752\tValue Loss: 245242.0\tEntropy: 4.165702819824219\n",
      "Predicted Reward: 0.6542060375213623\tCurrent Reward: 495.87335205078125\n",
      "KL DIVERGENCE: 1.4637907952419482e-05\n",
      "Policy Gradients: 7.74065701989457e-05\n",
      "Reward Gradients: -5.612948417663574\n",
      "\n",
      "\n",
      "9/10\n",
      "Current step: 600\n",
      "Current Loss: 239767.3125\n",
      "Surrogate Loss: -0.7136920690536499\tValue Loss: 239768.03125\tEntropy: 4.1622514724731445\n",
      "Predicted Reward: 0.6443482637405396\tCurrent Reward: 490.30548095703125\n",
      "KL DIVERGENCE: 9.24368760024663e-06\n",
      "Policy Gradients: 0.00021823217684868723\n",
      "Reward Gradients: -16.788196563720703\n",
      "\n",
      "\n",
      "9/10\n",
      "Current step: 700\n",
      "Current Loss: 225075.3125\n",
      "Surrogate Loss: -0.7134698629379272\tValue Loss: 225076.03125\tEntropy: 4.160567283630371\n",
      "Predicted Reward: 0.6724154949188232\tCurrent Reward: 475.0942077636719\n",
      "KL DIVERGENCE: 1.0038415894086938e-05\n",
      "Policy Gradients: -0.0001000024494715035\n",
      "Reward Gradients: -24.655963897705078\n",
      "\n",
      "\n",
      "9/10\n",
      "Current step: 800\n",
      "Current Loss: 187361.375\n",
      "Surrogate Loss: -0.7163297533988953\tValue Loss: 187362.09375\tEntropy: 4.159615993499756\n",
      "Predicted Reward: 0.6835447549819946\tCurrent Reward: 433.5369567871094\n",
      "KL DIVERGENCE: 2.6221077860100195e-05\n",
      "Policy Gradients: 0.00013861650950275362\n",
      "Reward Gradients: -31.631134033203125\n",
      "\n",
      "\n",
      "9/10\n",
      "Current step: 900\n",
      "Current Loss: 101192.109375\n",
      "Surrogate Loss: -0.7160037755966187\tValue Loss: 101192.8359375\tEntropy: 4.154634475708008\n",
      "Predicted Reward: 0.70587557554245\tCurrent Reward: 318.8140869140625\n",
      "KL DIVERGENCE: 3.654261672636494e-05\n",
      "Policy Gradients: 3.382776412763633e-05\n",
      "Reward Gradients: -4.670267105102539\n",
      "\n",
      "\n",
      "999\n",
      "3.202259540557861e-05\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "# CONSOLIDATION PHASE - https://en.wikipedia.org/wiki/Memory_consolidation\n",
    "# She remembers what she saw, and learns from it.\n",
    "\n",
    "policy.mode = 'consolidation'\n",
    "\n",
    "# In RL, an Epoch consists of an entire episode + training from that episode,\n",
    "# while a Batch consists of an episode. We won't use this terminology here as it's confusing.\n",
    "# https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\n",
    "\n",
    "\n",
    "policy_optm = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "value_optm = torch.optim.Adam(value_function.parameters(), lr=lr)\n",
    "value_criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    steps = 0\n",
    "    batches = torch.randperm(len(states))\n",
    "\n",
    "    for batch in range(0, len(batches)-1):\n",
    "\n",
    "        obs = states[batch].to(device).unsqueeze(0)\n",
    "        previous_log_prob = log_probs[batch].to(device).unsqueeze(0) # For Surrogate Loss\n",
    "        reward = rewards[batch].to(device).unsqueeze(-1).unsqueeze(-1)\n",
    "        advantage = gae[batch].to(device)\n",
    "        reward_to_go = rewards_to_go[batch].to(device).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        current_log_prob = policy(obs) # Not in log yet\n",
    "        # Calculating Entropy - Used to avoid deterministic behavior and it's a possible replace/complement to epsilon-greedy.\n",
    "        entropy = (current_log_prob * torch.log(torch.clamp(current_log_prob, 1e-10, 1.0))).sum()\n",
    "        entropy = -entropy.mean()\n",
    "        current_log_prob = torch.log(torch.clamp(current_log_prob, 1e-10, 1.0))\n",
    "\n",
    "        value = value_function(obs)\n",
    "\n",
    "        # Calculating Surrogate Loss\n",
    "\n",
    "        ratio = torch.exp(current_log_prob - previous_log_prob)\n",
    "        clipped_ratio = torch.clamp(ratio, min=0.8, max=1.2)\n",
    "\n",
    "        surrogate_loss = -torch.minimum((ratio * advantage), (clipped_ratio * advantage))\n",
    "        surrogate_loss = surrogate_loss.mean()\n",
    "\n",
    "        # Calculating KL Divergence between policies -> Used for early stopping\n",
    "\n",
    "        kld = (ratio - 1) - (current_log_prob - previous_log_prob)\n",
    "        kld = kld.mean()\n",
    "\n",
    "        # Calculating Value Loss\n",
    "\n",
    "        value_loss = value_criterion(value, reward_to_go)\n",
    "\n",
    "        # Total Loss (plus entropy discount)\n",
    "\n",
    "        total_loss = surrogate_loss + (value_loss * value_weight) - (entropy * entropy_weight)\n",
    "\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Using Gradient Accumulation to avoid using batch size greater than 1 -> Lower computation cost\n",
    "\n",
    "        if steps % BATCH_SIZE == 0:\n",
    "\n",
    "            policy_optm.step()\n",
    "            value_optm.step()\n",
    "            policy.zero_grad()\n",
    "            value_function.zero_grad()\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        # If KLD between policies goes beyond threshold --> early stopping\n",
    "\n",
    "        if kld.item() > target_KLD:\n",
    "\n",
    "            break\n",
    "\n",
    "        if steps % 100 == 0:\n",
    "\n",
    "            print(f\"{epoch}/{EPOCHS}\")\n",
    "            print(f\"Current step: {steps}\")\n",
    "            print(f\"Current Loss: {total_loss.item()}\")\n",
    "            #print(f\"Surrogate Loss: {surrogate_loss.item()}\\tValue Loss: {value_loss.item()}\\tEntropy: {entropy.item()}\\nAdvantage: {advantage[-10:]}\")\n",
    "            print(f\"Surrogate Loss: {surrogate_loss.item()}\\tValue Loss: {value_loss.item()}\\tEntropy: {entropy.item()}\")\n",
    "            print(f\"Predicted Reward: {value.item()}\\tCurrent Reward: {reward_to_go.item()}\")\n",
    "            print(f\"KL DIVERGENCE: {kld.item()}\")\n",
    "            print(f\"Policy Gradients: {policy.neuron1.weight.grad.mean().item()}\")\n",
    "            print(f\"Reward Gradients: {value_function.neuron1.weight.grad.mean().item()}\\n\\n\")\n",
    "\n",
    "print(steps)\n",
    "print(kld.item())\n",
    "print(target_KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell to adjust parameters and restart training\n",
    "\n",
    "policy = SubjectActor().to(device)\n",
    "value_function = SubjectCritic().to(device)\n",
    "\n",
    "gamma = 0.99 # Gamma for the Discount Rewards.\n",
    "lamb = 0.95 # Lambda for Generalized Advantage Estimation. Together with gamma, basically a \"weight\" for Exponential Moving Average\n",
    "BATCH_SIZE = 16 # In reality, all tensors are batch 1, so we'll use gradient accumulation to simulate multiple batches.\n",
    "EPOCHS = 10\n",
    "lr = 2e-8\n",
    "value_weight = 1.0\n",
    "entropy_weight = 1e-3\n",
    "target_KLD = 0.05 # Early-Stopping parameter to avoid suboptimal policy in PPO2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2.1845510005950928\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "print(steps)\n",
    "print(kld.item())\n",
    "print(target_KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gameplay Mode\n",
    "\n",
    "env = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\", state=\"ChunLiVsBlanka.1star\")\n",
    "obs = env.reset()\n",
    "obs = torch.from_numpy(obs)\n",
    "obs = obs/255\n",
    "obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "steps = 0\n",
    "\n",
    "# If you'd like to save and train even more\n",
    "\n",
    "'''states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "deltas = []'''\n",
    "\n",
    "while steps < 1000:\n",
    "    env.render()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        log_prob = policy(obs)\n",
    "        value = value_function(obs)\n",
    "\n",
    "    action = log_prob.squeeze(0)\n",
    "\n",
    "    # MultiBinary Environment --> Only 0.0 or 1.0 accepted\n",
    "    bin = []\n",
    "    for x in action:\n",
    "        if x > 0.5:\n",
    "            bin.append(1.)\n",
    "        else:\n",
    "            bin.append(0.)\n",
    "\n",
    "    action = bin\n",
    "\n",
    "    obs, reward, end, info = env.step(action)\n",
    "    obs = torch.from_numpy(obs)\n",
    "    obs = obs/255\n",
    "    obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "    #reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    reward = (info['health']**(1+info['matches_won'])) - (info['enemy_health']**(1+info['enemy_matches_won']))\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "    reward = -(10.0/(torch.exp(reward) + 1.0)) + 5.0 # Normalizing to -5 to +5 (sigmoid function)\n",
    "\n",
    "    '''states.append(obs.cpu())\n",
    "    actions.append(action.cpu())\n",
    "    rewards.append(reward.cpu())\n",
    "    deltas.append(delta.cpu())'''\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(close=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

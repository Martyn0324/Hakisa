{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retro\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTION-VALUE FUNCTION / Q-LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sources for this notebook are almost the same as the one for PPO:\n",
    "\n",
    "- https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/deepq/dqn.py - Deep Q-Learning code from Stable Baselines\n",
    "- https://github.com/saashanair/rl-series/blob/master/dqn/dqn_agent.py - Implementation of Deep Q-Learning from scratch in Pytorch, by Saasha Nair. Codes from people who are not from OpenAI (or big techs in general) are almost always way more readable and easier to understand.\n",
    "- https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html - Pytorch tutorial on Reinforcement Learning using Deep Q-Learning. One of the few Pytorch tutorials that really teaches you something other than downloading and importing ready-made functions.\n",
    "- https://spinningup.openai.com/en/latest/spinningup/rl_intro.html - OpenAI's Spinning Up, by Josh Achiam. However, it's not really good to learn about Q-Learning, as it's still counter-intuitive and it's focused on On-Policy algorithms, like PPO's antecessors (TRPO and Vanilla Policy Gradient)\n",
    "- https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf - Deep Q-Learning original paper. Pay attention to the algorithm and the 3 equations there. Though the paper is quite clean and concise.\n",
    "- https://lilianweng.github.io/posts/2018-02-19-rl-overview/#q-learning-off-policy-td-control - Lilian Weng's blog about Reinforcement Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectActorCritic(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Simple Neural Network for testing\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mode='exploration', epsilon=0.99):\n",
    "\n",
    "        super(SubjectActorCritic, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Input = 3x200x256\n",
    "\n",
    "        self.neuron1 = nn.Linear(3*200*256, 128)\n",
    "        self.neuron2 = nn.Linear(128, 128)\n",
    "\n",
    "        # The environment is MultiBinary, with 12 elements that can be 0 or 1\n",
    "        # Since this layer will provide both actions and value, sigmoid would imply that\n",
    "        # The action is either high or low value.\n",
    "        # Parallel liner layers is the best solution I could think of\n",
    "\n",
    "        self.output_neuron = nn.ModuleList([nn.Linear(128, 2)for _ in range(12)])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, obs):\n",
    "\n",
    "        x = obs.contiguous().view(obs.size(0), -1)\n",
    "\n",
    "        x = self.neuron1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.neuron2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        actions_values = []\n",
    "\n",
    "        for layer in range(12):\n",
    "\n",
    "            actions_values.append(self.output_neuron[layer](x))\n",
    "\n",
    "        if self.mode == 'exploration':\n",
    "            # Just collecting data\n",
    "\n",
    "            for i in range(len(actions_values)):\n",
    "\n",
    "                if torch.randn((1,)) < self.epsilon:\n",
    "\n",
    "                    action = actions_values[i]\n",
    "                    action = torch.zeros_like(action)\n",
    "                    random_idx = torch.randint(0, 2, ([])).item()\n",
    "                    action[0, random_idx] = 1.0\n",
    "\n",
    "                    actions_values[i] = action\n",
    "\n",
    "                    del action, i\n",
    "\n",
    "        del x\n",
    "\n",
    "        return actions_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we'll use a single function to determine the value of each action\n",
    "# And also a target network to determine the value states for the loss\n",
    "\n",
    "action_value = SubjectActorCritic().to(device)\n",
    "target_network = deepcopy(action_value).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Unfortunately, RL algorithms tend to be too sentimental,\n",
    "so we'll spend quite some time adjusting hyperparameters\n",
    "\n",
    "Even more unfortunately, it seems that research in deep RL \n",
    "is mostly done by OpenAI exclusively, and they were in love with PPO\n",
    "\n",
    "Some parameters used in some Gym's environments, which are too simple:\n",
    "\n",
    "https://github.com/saashanair/rl-series/blob/master/dqn/main.py\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "For complex games, like we wnt for Hakisa, we unfortunately don't have any basis.\n",
    "Maybe we could stick to something close to PPO2 parameters:\n",
    "\n",
    "https://github.com/liuruoze/HierNet-SC2/blob/396646056dbe5f8f20e43e0ef35e59db09e907c0/param.py\n",
    "\n",
    "\"Even Ignoring Generalization Issues, The Final Results Can be Unstable and Hard to Reproduce\" - Alex Irpan\n",
    "\n",
    "\"[Supervised learning] wants to work. Even if you screw something up you'll usually get something non-random back.\n",
    "RL must be forced to work.\n",
    "If you screw something up or don't tune something well enough you're exceedingly likely to get a policy that is even worse than random.\n",
    "And even if it's all well tuned you'll get a bad policy 30% of the time, just because.\" - Andrej Karpathy\n",
    "'''\n",
    "\n",
    "gamma = 0.99 # Gamma for the Discount Rewards.\n",
    "BATCH_SIZE = 16 # In reality, all tensors are batch 1, so we'll use gradient accumulation to simulate multiple batches.\n",
    "EPOCHS = 10\n",
    "lr = 1e-4\n",
    "target_delay = BATCH_SIZE * 16 # Steps before applying update to the target network\n",
    "action_value.epsilon = 0.99 # Epsilon for the epsilon-greedy strategy\n",
    "# Pytorch tutorial uses a Tau weight to apply soft update to target network.\n",
    "#update_weight = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In reality, the AI will only play the game to acquire data\n",
    "# The magic really happens during her training, which is done offline.\n",
    "\n",
    "# While in PPO the AI plays the game to acquire data and then\n",
    "# trains offline, DQN usually applies training online.\n",
    "# We'll try training offline first, then going online.\n",
    "\n",
    "'''\n",
    "\"BATCH_SIZE is the number of transitions sampled from the replay buffer\" - Pytorch's DQN tutorial.\n",
    "\n",
    "\"A trajectory is a sequence of states and actions in the world. [...]\n",
    "Trajectories are also frequently called episodes or rollouts.\"\n",
    "- OpenAI's Spinning Up: https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n",
    "\n",
    "In DQN, the Replay Buffer(or Memory) is the same as the Rollout in PPO, but with\n",
    "a more intuitive name.\n",
    "'''\n",
    "\n",
    "# Creating lists to store data.\n",
    "# Using a separate cell in case of storing multiple episodes (playthroughs)\n",
    "# TO CONSIDER: Using pickle or torch to save and load playthroughs\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "log_probs = []\n",
    "values = []\n",
    "rewards = []\n",
    "rewards_to_go = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration Phase - Let her learn how the environment is.\n",
    "\n",
    "env = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\", state=\"ChunLiVsBlanka.1star\")\n",
    "obs = env.reset()\n",
    "obs = torch.from_numpy(obs)\n",
    "obs = obs/255\n",
    "obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "steps = 0\n",
    "\n",
    "while steps < 1000:\n",
    "    env.render()\n",
    "\n",
    "    # Collecting State --> Must be done at the beginning\n",
    "\n",
    "    states.append(obs.cpu())\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # The action is provided by the DQN network,\n",
    "        # while the value, by the target network\n",
    "\n",
    "        prob = action_value(obs)\n",
    "        value = target_network(obs)\n",
    "\n",
    "    # MultiBinary Environment --> Only 0.0 or 1.0 accepted\n",
    "    bin = []\n",
    "    for action in prob:\n",
    "        bin.append(action.argmax().item())\n",
    "    \n",
    "    action = bin # This is the actual action\n",
    "\n",
    "    del bin\n",
    "\n",
    "    obs, reward, end, info = env.step(action)\n",
    "    obs = torch.from_numpy(obs)\n",
    "    obs = obs/255\n",
    "    obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    reward = (info['health']**(1+info['matches_won'])) - (info['enemy_health']**(1+info['enemy_matches_won']))\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "    reward = -(10.0/(torch.exp(reward) + 1.0)) + 5.0 # Normalizing to -5 to +5 (sigmoid function)\n",
    "\n",
    "    # Collecting variables for the previous (collected) state\n",
    "    \n",
    "    actions.append(action) # List of indices\n",
    "    log_probs.append(prob) # List of tensors\n",
    "    values.append(value) # List of tensors\n",
    "    rewards.append(reward.cpu()) # Pure tensors\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing colected data\n",
    "\n",
    "states = torch.cat(states, 0)\n",
    "\n",
    "discounted_reward = 0 # The final rewards provide greater impact on the algorithm\n",
    "for r_t in reversed(rewards):\n",
    "    \n",
    "    discounted_reward = r_t + discounted_reward * gamma\n",
    "    rewards_to_go.insert(0, discounted_reward)\n",
    "\n",
    "rewards_to_go = torch.tensor(rewards_to_go)\n",
    "\n",
    "# There's no need to use advantage here --> Time-Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2\n",
      "Current step: 100\n",
      "Current Loss: 1.3783857822418213\n",
      "Model Gradients: -0.0040936581790447235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Alive\\lib\\site-packages\\ipykernel_launcher.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2\n",
      "Current step: 200\n",
      "Current Loss: 0.25955578684806824\n",
      "Model Gradients: -0.022637929767370224\n",
      "1/2\n",
      "Current step: 300\n",
      "Current Loss: 2.609018325805664\n",
      "Model Gradients: -0.0007244820590130985\n",
      "1/2\n",
      "Current step: 400\n",
      "Current Loss: 2.49064564704895\n",
      "Model Gradients: 0.040356554090976715\n",
      "1/2\n",
      "Current step: 500\n",
      "Current Loss: 9.377496719360352\n",
      "Model Gradients: -0.007127691991627216\n",
      "1/2\n",
      "Current step: 600\n",
      "Current Loss: 2.7017438411712646\n",
      "Model Gradients: 0.026180408895015717\n",
      "1/2\n",
      "Current step: 700\n",
      "Current Loss: 0.3448813259601593\n",
      "Model Gradients: 0.020731082186102867\n",
      "1/2\n",
      "Current step: 800\n",
      "Current Loss: 0.6616409420967102\n",
      "Model Gradients: -0.04040137678384781\n",
      "1/2\n",
      "Current step: 900\n",
      "Current Loss: 1.0524049997329712\n",
      "Model Gradients: -0.021784093230962753\n",
      "1/2\n",
      "Current step: 1000\n",
      "Current Loss: 4.267786502838135\n",
      "Model Gradients: -0.03505820780992508\n",
      "2/2\n",
      "Current step: 100\n",
      "Current Loss: 0.04128098115324974\n",
      "Model Gradients: 0.005391942802816629\n",
      "2/2\n",
      "Current step: 200\n",
      "Current Loss: 0.005966844968497753\n",
      "Model Gradients: 0.04450089856982231\n",
      "2/2\n",
      "Current step: 300\n",
      "Current Loss: 1.0631963014602661\n",
      "Model Gradients: -0.00724928081035614\n",
      "2/2\n",
      "Current step: 400\n",
      "Current Loss: 4.391650676727295\n",
      "Model Gradients: 0.05444521829485893\n",
      "2/2\n",
      "Current step: 500\n",
      "Current Loss: 0.9572865962982178\n",
      "Model Gradients: -0.01279542688280344\n",
      "2/2\n",
      "Current step: 600\n",
      "Current Loss: 0.016627272590994835\n",
      "Model Gradients: 0.0061037177219986916\n",
      "2/2\n",
      "Current step: 700\n",
      "Current Loss: 0.2873018980026245\n",
      "Model Gradients: 0.018351750448346138\n",
      "2/2\n",
      "Current step: 800\n",
      "Current Loss: 0.054056063294410706\n",
      "Model Gradients: -0.01110102515667677\n",
      "2/2\n",
      "Current step: 900\n",
      "Current Loss: 0.7870970964431763\n",
      "Model Gradients: -0.0024704348761588335\n",
      "2/2\n",
      "Current step: 1000\n",
      "Current Loss: 0.5315628051757812\n",
      "Model Gradients: -0.012968868017196655\n"
     ]
    }
   ],
   "source": [
    "# CONSOLIDATION PHASE - https://en.wikipedia.org/wiki/Memory_consolidation\n",
    "# She remembers what she saw, and learns from it.\n",
    "\n",
    "action_value.mode = 'consolidation'\n",
    "target_network.mode = 'target' # Remember to turn off the target network exploration mode\n",
    "\n",
    "# In RL, an Epoch consists of an entire episode + training from that episode,\n",
    "# while a Batch consists of an episode.\n",
    "# DQN also uses Batch as a number of samples extracted from memory.\n",
    "# https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\n",
    "# We'll stick to traditional terminology of ML, to avoid needless confusion.\n",
    "\n",
    "optimizer = torch.optim.Adam(action_value.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    steps = 0\n",
    "    batches = torch.randperm(len(states))\n",
    "\n",
    "    for batch in batches:\n",
    "\n",
    "        obs = states[batch].to(device).unsqueeze(0)\n",
    "\n",
    "        try:\n",
    "            next_obs = states[batch+1].to(device).unsqueeze(0)\n",
    "        \n",
    "        except: # Terminal state\n",
    "            next_obs = None\n",
    "\n",
    "        action = actions[batch]\n",
    "        reward = rewards[batch].to(device)\n",
    "\n",
    "        # Compute Q(s_t, a)\n",
    "        # The model computes Q(s_t), the value of that state\n",
    "        # Then we select the index of the action that was taken.\n",
    "\n",
    "        Q_s = action_value(obs)\n",
    "\n",
    "        Q_s_a = []\n",
    "\n",
    "        for item in range(len(Q_s)):\n",
    "\n",
    "            a = Q_s[item][0, action[item]]\n",
    "\n",
    "            Q_s_a.append(a)\n",
    "\n",
    "        # Compute target Q(s', a'), such that\n",
    "        # yi = r_t + gamma * Q(s', a'), with the maximum values\n",
    "        # for each action\n",
    "        # However, if reached terminal state, yi = r_t\n",
    "            \n",
    "        if next_obs is None:\n",
    "\n",
    "            Q_target = [0.0]*12\n",
    "\n",
    "        else:\n",
    "            \n",
    "            Q_target = target_network(next_obs)\n",
    "\n",
    "            Q_t_a = []\n",
    "\n",
    "            for item in range(len(Q_target)):\n",
    "\n",
    "                a = Q_target[item].argmax(-1)\n",
    "                \n",
    "                Q_t_a.append(a)\n",
    "\n",
    "        Q_t_a = torch.tensor(Q_t_a, device=device)\n",
    "        \n",
    "        r_t = torch.full_like(Q_t_a, reward.item(), device=device)\n",
    "\n",
    "        y_i = r_t + gamma * Q_t_a\n",
    "\n",
    "        '''# TO CONSIDER: adding an entropy factor to the loss\n",
    "        # PS: Doesn't seem much needed here, and it's too complicated in MultiBinary Environment\n",
    "\n",
    "        entropy = []\n",
    "\n",
    "        for item in range(len(Q_s)):\n",
    "\n",
    "            current_prob = Q_s[item]\n",
    "\n",
    "            e = (current_prob * torch.log(torch.clamp(current_prob, 1e-10, 1.0))).sum()\n",
    "            e = -e.mean()\n",
    "            entropy.append(e)\n",
    "            \n",
    "        entropy = torch.tensor(entropy, device=device)'''\n",
    "\n",
    "        for item in range(len(Q_s_a)):\n",
    "\n",
    "            loss = criterion(Q_s_a[item], y_i[item]) #- (entropy * 1e-3)\n",
    "            loss.backward(retain_graph=True) \n",
    "\n",
    "        # Using Gradient Accumulation to avoid using batch size greater than 1 -> Lower computation cost\n",
    "\n",
    "        if steps % BATCH_SIZE == 0:\n",
    "\n",
    "            optimizer.step()\n",
    "            action_value.zero_grad()\n",
    "\n",
    "        if steps % target_delay:\n",
    "\n",
    "            target_network.load_state_dict(action_value.state_dict())\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 100 == 0:\n",
    "\n",
    "            print(f\"{epoch+1}/{EPOCHS}\")\n",
    "            print(f\"Current step: {steps}\")\n",
    "            print(f\"Current Loss: {loss.item()}\")\n",
    "            print(f\"Model Gradients: {action_value.neuron1.weight.grad.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell to adjust parameters and restart training\n",
    "\n",
    "action_value = SubjectActorCritic().to(device)\n",
    "target_network = deepcopy(action_value).to(device).eval()\n",
    "\n",
    "gamma = 0.99 # Gamma for the Discount Rewards.\n",
    "BATCH_SIZE = 16 # In reality, all tensors are batch 1, so we'll use gradient accumulation to simulate multiple batches.\n",
    "EPOCHS = 2\n",
    "lr = 2e-4\n",
    "target_delay = BATCH_SIZE * 16 # Steps before applying update to the target network\n",
    "action_value.epsilon = 0.99 # Epsilon for the epsilon-greedy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gameplay Mode\n",
    "\n",
    "env = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\", state=\"ChunLiVsBlanka.1star\")\n",
    "obs = env.reset()\n",
    "obs = torch.from_numpy(obs)\n",
    "obs = obs/255\n",
    "obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "steps = 0\n",
    "\n",
    "# If you'd like to save and train even more\n",
    "\n",
    "'''states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "deltas = []'''\n",
    "\n",
    "while steps < 1000:\n",
    "    env.render()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        prob = action_value(obs)\n",
    "        value = target_network(obs)\n",
    "\n",
    "    # MultiBinary Environment --> Only 0.0 or 1.0 accepted\n",
    "    bin = []\n",
    "    for action in prob:\n",
    "        bin.append(action.argmax().item())\n",
    "    \n",
    "    action = bin # This is the actual action\n",
    "\n",
    "    del bin\n",
    "\n",
    "    obs, reward, end, info = env.step(action)\n",
    "    obs = torch.from_numpy(obs)\n",
    "    obs = obs/255\n",
    "    obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "    #reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    reward = (info['health']**(1+info['matches_won'])) - (info['enemy_health']**(1+info['enemy_matches_won']))\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "    reward = -(10.0/(torch.exp(reward) + 1.0)) + 5.0 # Normalizing to -5 to +5 (sigmoid function)\n",
    "\n",
    "    '''states.append(obs.cpu())\n",
    "    actions.append(action.cpu())\n",
    "    rewards.append(reward.cpu())\n",
    "    deltas.append(delta.cpu())'''\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New cell to adjust parameters and restart training\n",
    "\n",
    "action_value = SubjectActorCritic().to(device)\n",
    "target_network = deepcopy(action_value).to(device).eval()\n",
    "\n",
    "gamma = 0.99 # Gamma for the Discount Rewards.\n",
    "lamb = 0.95 # Lambda for Generalized Advantage Estimation. Together with gamma, basically a \"weight\" for Exponential Moving Average\n",
    "BATCH_SIZE = 16 # In reality, all tensors are batch 1, so we'll use gradient accumulation to simulate multiple batches.\n",
    "EPOCHS = 2\n",
    "lr = 1e-4\n",
    "target_delay = BATCH_SIZE * 16 # Steps before applying update to the target network\n",
    "action_value.epsilon = 0.99 # Epsilon for the epsilon-greedy strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 100\n",
      "Current Loss: 0.01855369098484516\n",
      "Current step: 200\n",
      "Current Loss: 0.6386929154396057\n",
      "Current step: 300\n",
      "Current Loss: 0.007749685551971197\n",
      "Current step: 400\n",
      "Current Loss: 0.05683543160557747\n",
      "Current step: 500\n",
      "Current Loss: 0.13423769176006317\n",
      "Current step: 600\n",
      "Current Loss: 0.09188412874937057\n",
      "Current step: 700\n",
      "Current Loss: 0.0023667553905397654\n",
      "Current step: 800\n",
      "Current Loss: 0.004315526224672794\n",
      "Current step: 900\n",
      "Current Loss: 9.851335525512695\n",
      "Current step: 1000\n",
      "Current Loss: 1.447236180305481\n"
     ]
    }
   ],
   "source": [
    "# Play and Learn\n",
    "# This method is more unstable than using Exploration + Consolidation.\n",
    "# It's recommended to use Exploration + Consolidation and, only then, this method.\n",
    "\n",
    "'''\n",
    "\"Use reinforcement learning just as the fine-tuning step:\n",
    "The first AlphaGo paper started with supervised learning, and then did RL fine-tuning on top of it.\n",
    "This is a nice recipe, since it lets you use a faster-but-less-powerful method to speed up initial learning.\n",
    "It's worked in other contexts - see Sequence Tutor (Jaques et al, ICML 2017).\n",
    "You can view this as starting the RL process with a reasonable prior, instead of a random one,\n",
    "where the problem of learning the prior is offloaded to some other approach.\"\n",
    "- Alex Irpan, https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "\n",
    "\"In SL training, we found a learning rate of 1e-4 and 10 training epochs achieve the\n",
    "best result. The best model achieves a 0.15 win rate against the level-1 built-in AI.\n",
    "Note that though this result is not as good as that we acquire in the HRL method, the training\n",
    "here faces 564 actions, thus is much difficult.\"\n",
    "- Liu, Ruo-Ze et al. On Efficient Reinforcement Learning for Full-length Game of StarCraft II\n",
    "\n",
    "'''\n",
    "\n",
    "env = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\", state=\"ChunLiVsBlanka.1star\")\n",
    "obs = env.reset()\n",
    "obs = torch.from_numpy(obs)\n",
    "obs = obs/255\n",
    "obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "steps = 0\n",
    "\n",
    "action_value.mode = 'consolidation'\n",
    "target_network.mode = 'target' # Remember to turn off the target network exploration mode\n",
    "\n",
    "optimizer = torch.optim.Adam(action_value.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "while steps < 1000:\n",
    "    env.render()\n",
    "\n",
    "    Q_s = action_value(obs)\n",
    "    \n",
    "    # Compute Q(s_t, a)\n",
    "    # The model computes Q(s_t), the value of that state\n",
    "    # Then we select the index of the action that was taken.\n",
    "\n",
    "    Q_s = action_value(obs)\n",
    "\n",
    "    Q_s_a = []\n",
    "\n",
    "    for item in range(len(Q_s)):\n",
    "\n",
    "        a = Q_s[item][0, action[item]]\n",
    "\n",
    "        Q_s_a.append(a)\n",
    "\n",
    "    # MultiBinary Environment --> Only 0.0 or 1.0 accepted\n",
    "    bin = []\n",
    "    for action in prob:\n",
    "        bin.append(action.argmax().item())\n",
    "    \n",
    "    action = bin # This is the actual action\n",
    "\n",
    "    del bin\n",
    "\n",
    "    # Getting next observation and its consequences\n",
    "\n",
    "    obs, reward, end, info = env.step(action)\n",
    "    obs = torch.from_numpy(obs)\n",
    "    obs = obs/255\n",
    "    obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    reward = (info['health']**(1+info['matches_won'])) - (info['enemy_health']**(1+info['enemy_matches_won']))\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "    reward = -(10.0/(torch.exp(reward) + 1.0)) + 5.0 # Normalizing to -5 to +5 (sigmoid function)\n",
    "\n",
    "    # Compute target Q(s', a'), such that\n",
    "    # yi = r_t + gamma * Q(s', a'), with the maximum values\n",
    "    # for each action\n",
    "    # However, if reached terminal state, yi = r_t\n",
    "        \n",
    "    if end:\n",
    "\n",
    "        Q_target = [0.0]*12\n",
    "\n",
    "    else:\n",
    "        \n",
    "        Q_target = target_network(obs)\n",
    "\n",
    "        Q_t_a = []\n",
    "\n",
    "        for item in range(len(Q_target)):\n",
    "\n",
    "            a = Q_target[item].argmax(-1)\n",
    "            \n",
    "            Q_t_a.append(a)\n",
    "\n",
    "    Q_t_a = torch.tensor(Q_t_a, device=device)\n",
    "    \n",
    "    r_t = torch.full_like(Q_t_a, reward.item(), device=device)\n",
    "\n",
    "    y_i = r_t + gamma * Q_t_a\n",
    "\n",
    "    '''# TO CONSIDER: adding an entropy factor to the loss\n",
    "    # Here, such factor may be interesting.\n",
    "\n",
    "    entropy = []\n",
    "\n",
    "    for item in range(len(Q_s)):\n",
    "\n",
    "        current_prob = Q_s[item]\n",
    "\n",
    "        e = (current_prob * torch.log(torch.clamp(current_prob, 1e-10, 1.0))).sum()\n",
    "        e = -e.mean()\n",
    "        entropy.append(e)\n",
    "        \n",
    "    entropy = torch.tensor(entropy)'''\n",
    "\n",
    "    for item in range(len(Q_s_a)):\n",
    "\n",
    "        loss = criterion(Q_s_a[item], y_i[item])\n",
    "        loss.backward(retain_graph=True) \n",
    "\n",
    "    # Using Gradient Accumulation to avoid using batch size greater than 1 -> Lower computation cost\n",
    "\n",
    "    if steps % BATCH_SIZE == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        action_value.zero_grad()\n",
    "\n",
    "    if steps % target_delay:\n",
    "\n",
    "        target_network.load_state_dict(action_value.state_dict())\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Current Loss: {loss.item()}\")\n",
    "\n",
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPECIAL: https://arxiv.org/pdf/2102.04518.pdf - A* applied to Deep Q-Learning, giving birth to Q*-Learning. Supposedly the RL method used for next OpenAI's AGI, capable of expressing logical sense. Meanwhile, PPO was used for GPT-3(and 4) using a categorical reward model (rewards 0 to 10, from worst to best)\n",
    "\n",
    "\"A* search selects the node with the lowest cost for expansion and computes the cost of all of its children.\n",
    "This process continues until a node associated with a goal state is selected for expansion.\n",
    "Expanding a node requires that every possible action be applied to the state associated with that node,\n",
    "thereby generating new states and, subsequently, new nodes.\"\n",
    "- This reminds a bit of how the Transformer Generation is done: Greedy or Beam Search, both trying to select\n",
    "the most likely tokens given an input token (or sequence of tokens).\n",
    "\n",
    "\"DQNs are DNNs that map a single state to the sum of the transition cost and the heuristic\n",
    "value for each of its successor states. This allows us to only generate one node per iteration as we can store tuples\n",
    "of nodes and actions in a priority queue whose priority is determined by the DQN. When removing a tuple of a node\n",
    "and action from the queue, we can then generate a new node by applying the action to the state associated with that\n",
    "node.\"\n",
    "- In this case, a Transformer *(or a Generative Transformer)* with DQN Method \n",
    "doesn't seem to be that much different from its classic counterpart, right?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# Hakisa
SerpentAI's Awesome, but, unfortunately, it's too expensive computationally. Let's see what we can do about that

The idea of SerpentAI, when using Reinforcement Learning algorithms, is basically grab screenshots in real time, pass those screenshots to an algorithm and, then, perform a command according to that algorithm's output.

However, unfortunately, Serpent also relies on many, MANY intermediaries, which makes it too slow to execute each step(takes some minutes to start a run, each step in random mode takes around 3~4 seconds and, in my case, I simply can't use training mode in my personal computer with a GTX 1650 Ti).

By removing those intermediaries(at least during training) and by having greater access and control over the algorithm we're going to use, it's possible to diminish the time between steps to around 1.3 seconds.

## General

Initially, we'll use no dataset. The only thing we're going to use is an input map. This input map is a list of commands in a specific structure `('command_action')` which will be used, in the Pytorch's Dataset class in order to generate a dictionary, where each input map(key) will be assigned to a value between -1 and 1.

We will, then, use a loop to make our neural network, Hakisa, play a game which will be our active window. With each step, a screenshot will be taken in real time and passed as input to Hakisa, which will then generate an output accordingly.

Since Neural Networks can only generate floats, the outputs generated by Hakisa will be passed to a K-Nearest Neighbors that has been fitted to that dictionary values in order to get the value that is closest to Hakisa's output. With that value, we can get the dictionary key that will, then, be used to execute a command through PyAUTOGUI module.

Hakisa will have 3 learning modes: exploration, study and play.

### Exploration Mode

Hakisa will simply play the game and ~~generate outputs according to the images she's receiving~~ she'll now generate random outputs, independently of the image she's receiving. Each step will generate a memory for Hakisa(that will actually be part of the Dataset class, not Hakisa class) composed of the input frame, the output key in the input mapping, its value and the reward obtained. This memory will, posteriorly, be our dataset for the study mode.

At each step, a tuple `(frame, key, value, reward)` will be added to the memory. If the memory gets full(as defined when initializing Dataset class), the items with lower rewards will be discarded in order to add the new items.

In this mode, it's important that Hakisa generates outputs as diverse as possible ~~, so avoid using weights initialization through normal or uniform distribution.~~ Hakisa's network won't be used at all.

### Study Mode

We'll use Hakisa's memory to generate a classic dataset for machine learning. Each frame will serve as input, and each value, a label. As criterion, we'll use MSE Loss.

This stage works as a way to make Hakisa identify patterns in each situation in the game and associate those situations with a value(controller command) that is best suited for a situation. The input is a frame where a projectile is coming towards your character? Then the best output is the one associated with the command "move left" or "move right". There's an enemy in the input frame? The best output is the one associated with the command "shoot".

Of course, not necessarily Hakisa's memory will contain exactly the best output for that situation. For this motive, it's important to use a memory size smaller than the number of exploration steps. This way, the memory will tend to have the best outputs for specifics situations.

You can also use a ready-made dataset, with frames captured by you when you were playing and labels defined by yourself(maybe there's some way to properly capture mouse/keyboard commands in real time...)

**EDIT:** Now Hakisa will also try to predict the reward she'll get in that step. This prediction will be passed to another loss having the actual reward as target.
The Study Loss will then be: study_loss = cross_entropy(command_type_output, command_type_label) + mse(action1_out, action1_label) + mse(action2_out, action2_label) + mse(predicted_reward, actual_reward).

### Play Mode

Here, Hakisa will use what she learned in the previous stages and play all by herself. At each step, a screenshot will be taken and passed to Hakisa as input, and she'll generate an output accordingly.

**EDIT:** Now, Hakisa will also receive the previous output actions and the previous reward received.

We're gonna be using a custom loss function, GameplayLoss, which will use the reward generated by Hakisa's command in order to get gradients for backpropagation. This function is log based so the gradients returned are bigger as the reward decreases, and smaller as the reward increases.

This way, Hakisa can correct some associations she made in the study mode, probably because she didn't get exposed to determined situations or because she didn't generate the best output for that situation during the exploration mode.

This way, Hakisa can play and get better and better as she plays, all by herself.

**EDIT: The GameplayLoss function is actually making Hakisa's outputs as random as when she's on Exploration mode. Possible corrections might be using cumulative rewards during exploration mode or simply remaking/replacing this function. Softmax and Cross-Entropy Loss must be avoided in order to avoid great output sizes.**

**EDITÂ²: The GameplayLoss function will indeed have to be remade or replaced, as its gradients makes Hakisa generate outputs that will only correspond on the extreme commands in the input mapping dictionary (she'll only generate the command for -1 and for 1).**

**Also good consideration for gameplay loss function: Liu, Ruo-Ze et al. Rethinking of AlphaStar: https://arxiv.org/pdf/2104.06890.pdf . - Still uses Categorical Cross Entropy, but might be a good inspiration.**


# Update and possible upgrades

While testing my NLP models(and also chatting in Python's Discord server) I've learned that softmax can't really be avoided. The motive is simple: numbers have a correlation between each other, but our input mappings, just like words and sentences, don't. The input map `press X` isn't bigger or smaller than `click (512, 600)`.

This can only be avoided by the use of categories. In a Classifier, for example, the number `0` can be the label `Dog`, while `1` can be `Cat`, and there's no mathematical relation between `Dog` or `Cat`, they're simply categories. This is learned with time by the classifier, through the use of a softmax or sigmoid function.
In NLP, each word is assigned to an integer, which works as a label. Letter `a` can be the label `0`, `b` be `1`, `c`, `2` and so on. This relation comes up as the model trains. The same happens with Reinforcement Learning models, like Rainbow DQN, or even the Hierarchical AlphaStar.

However, there's a way to avoid having to use softmax, despite this...kinda. In NLP, it's used the technique `word2vec`, which converts words to vectors, that is, a single value. `word2vec` in fact, consists on the use of algorithms to associate words to specific values and it's included in the embedding layers, commonly used in NLP models and it's used in the mentioned paper above. It makes the word `apple` has a vector closer to `fruit` than to, let's say, `car`.

For Hakisa, we could do something like that to make associations between certain input mapping and a vector. `press X` can be associated with the number `0.75` and `press Z` can be associated with `0.76`, while `click (512, 600)` can be associated with `0.10`.This technique, however, requires one-hot encoding and the use of softmax, which can be make things computationally expensive.
However, we could create a separate model, disconnected from Hakisa, that would be trained separately to convert each input map to a vector. After its training has been complete, it would be used to generate the dictionary of input mappings for Hakisa. After that, we'll continue making things as we do right now.


*I'll be testing this idea with NLP models and see if this works and if I should make some adjustments. Consider this text if you want to test Hakisa.*

### Vector Embedding

Applying vector embedding seems promising. But it may be more efficient for human recorded gameplay instead of exploration mode, as it'll provide a better idea of context for each command. Exploration may still be useful, but only if `memory_size` <<< `exploration_steps`.*

Vector Embedding can be quite efficient in NLP when you're dealing with a reasonable amount of data(which can be understood as: you're not just playing with some dozens of words).

For Hakisa, this can be quite good for games that use mouse, since we can get up to 4, 5 or even 6 million possible commands. However, this is quite an annoyance for games that only uses the keyboard, which will have few possible inputs.

We also want to avoid directly working with millions of data in our input mapping since this will make it mandatory to use cloud servers CPUs to properly fit KNN in seconds(or minutes).
There's also the problem that, in NLP, the vector applied to a word depends on the context that it appears and how many times it appears. For Hakisa, the "context" could vary according to the commands itself(if the action type is mouse) and according to the game frame. All of this makes me conclude that using a ready-made data or a recorded human gameplay can be way more efficient for Hakisa as it'll already dispose of a proper context(game frame, reward...the game state) for each command, something that must be discovered when in exploration mode.

Study Mode will probably have to be modified: instead of simple Supervised Learning, we'll also train a Vector Embedding Layer for each action command. So we'll have `command_type` being one-hot encoded and then serving as input for this vector embedding layer, which will throw an output with the same size as the input, which will then be passed into a Cross Entropy having the input as target. The same thing will be applied to `action1` and `action2`.

Those vector embedding layers will be used to create our input_mapping dictionary, which will now be used to convert Hakisa's outputs into input commands. KNN will have to be properly fitted again.
*In the exploration mode, Hakisa will simply choose a random integer which will serve as an index for the list of input mappings generated upon calling the `Dataset` class*

Categorical Cross Entropy will be used to optimize the vector embedding layers, while Mean Squared Error will be used to optimize Hakisa's output(remember that Hakisa's output will be a vector, a single number).

*Perhaps one could simply add Pytorch's Embedding layers in order to do this...but meh*

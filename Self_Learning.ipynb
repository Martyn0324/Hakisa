{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "import winsound\n",
    "import pickle\n",
    "#import pyautogui\n",
    "import keyboard\n",
    "import mouse\n",
    "import pytesseract\n",
    "from re import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Hakisa will be composed of 3 parts:\n",
    "\n",
    "1) Feature extraction -> Extracts features from the input\n",
    "2) Pseudo-labeling -> generates a pseudo-label according to the features extracted. Those labels will try to define the best command for that state(input)\n",
    "3) Action selection -> Generates a command (command_type, action1, action2).\n",
    "\n",
    "What's the difference? There's no Exploration Mode, only recorded gameplay.\n",
    "\n",
    "In Study Mode, recorded gameplay is passed as input to Hakisa. There's no human labeled data. Pseudo-labels will be generated according to the length of each action argument,\n",
    "which will be used as targets for the loss function.\n",
    "The input for the loss function is the action generated in the end.\n",
    "\n",
    "So, Study Mode will be:\n",
    "Input --------------> Feature Extraction\n",
    "\n",
    "Feature Extraction ------------> Pseudo-labels\n",
    "Feature Extraction ------------> Command\n",
    "\n",
    "Loss = CrossEntropy(Command, Pseudo-labels)\n",
    "\n",
    "To consider: instead of using classes for pseudo-labels, actually using vectors.\n",
    "\n",
    "In Play Mode, optimization will still be coordinated by predicted reward and actual (cumulative) reward (TD-Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Generates input maps and commands for Hakisa.\n",
    "\n",
    "    Remember: command_types = list of strings, actions1 and 2 = list of strings(keyboard), X coordinates or None(mouse)\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        command_types = None,\n",
    "        actions1 = None,\n",
    "        actions2 = None,\n",
    "        top=0,\n",
    "        left=0,\n",
    "        width=1920,\n",
    "        height=1080,\n",
    "        resize=None\n",
    "    ):\n",
    "\n",
    "        # Window resolutions for the screen grabber\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.resize = resize # For reducing the images. Must be a tuple (Height, Width)\n",
    "\n",
    "        self.data = None # This will be created during training. However, it's possible to load a ready-made data for training.\n",
    "\n",
    "        # Initially, we'll be using lists. After our vector embedding has been properly trained, we'll create a dictionary\n",
    "        # of input mappings with it -----> Here, we won't be doing this as we'll actually be testing the softmax and a more traditional approach.\n",
    "\n",
    "        self.command_type = command_types\n",
    "        self.actions1 = actions1\n",
    "        self.actions2 = actions2\n",
    "\n",
    "\n",
    "    # Pytorch's Dataset functions will only be used in Studying mode\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        inputs = self.data[idx]\n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def grab_frame(self):\n",
    "        # Unfortunately, this whole operation takes about 0.6 seconds, so we'll probably have to deal with a single frame each 1~3 seconds.\n",
    "        with mss() as sct:\n",
    "            frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "            frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if self.resize:\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame = np.array(frame, dtype=np.float32)\n",
    "\n",
    "            frame = torch.from_numpy(frame)\n",
    "        \n",
    "        frame = frame.view(1, frame.size(2), frame.size(0), frame.size(1)).to(device) # (Batch, Channels, Height, Width)\n",
    "\n",
    "        return frame\n",
    "\n",
    "\n",
    "    def get_command(self, cmd_type, action1, action2):\n",
    "        '''\n",
    "        Hakisa's output: (command_type, action1, action2) ----> (key, Down, z) or (click, 100, 60)\n",
    "        command_type, action1 and action2 are the argmax output from a logsoftmax function and will be used as index for their respectives lists.\n",
    "        '''\n",
    "\n",
    "        cmd_type = self.command_type[cmd_type]\n",
    "        action1 = self.actions1[action1]\n",
    "        action2 = self.actions2[action2]\n",
    "\n",
    "        command = (cmd_type, action1, action2)\n",
    "\n",
    "        del cmd_type, action1, action2\n",
    "\n",
    "        return command\n",
    "\n",
    "    def get_consequences(self, top, left, width, height, togray=False, threshold=False, thresh_gauss=171, thresh_C=13, tesseract_config='--psm 8'):\n",
    "        '''\n",
    "        Used after Hakisa performed an input, in order to get its consequences(ex: score change, bombs, kills, deaths...).\n",
    "        Returns a string according to Tesseract's OCR.\n",
    "        '''\n",
    "\n",
    "        with mss() as sct:\n",
    "            consequence = sct.grab(monitor={\"top\": top, \"left\": left, \"width\": width, \"height\": height})\n",
    "\n",
    "            consequence = Image.frombytes(\"RGB\", consequence.size, consequence.bgra, 'raw', 'BGRX')\n",
    "\n",
    "        if togray is True:\n",
    "\n",
    "            consequence = consequence.convert(\"P\") # Sometimes, simply converting to grayscale is enough\n",
    "\n",
    "            if threshold is True:\n",
    "                if \"ADAPTIVE_THRESH_GAUSSIAN_C\" and \"adaptiveThreshold\" and \"THRESH_BINARY\" not in dir():\n",
    "                    from cv2 import adaptiveThreshold, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY\n",
    "\n",
    "                consequence = adaptiveThreshold(np.array(consequence),255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,thresh_gauss,thresh_C)\n",
    "                consequence = Image.fromarray(consequence)\n",
    "        \n",
    "        consequence = pytesseract.image_to_string(consequence, config=tesseract_config) \n",
    "\n",
    "        # OCR adds some strange characters(even with the whitelist function). Let's remove them.\n",
    "\n",
    "        consequence = sub('[^A-Za-z0-9/.]', '', consequence) # Attention: 0, 1 and 8 can be seen as O, l and B.\n",
    "\n",
    "        return consequence\n",
    "\n",
    "    def create_data(self, data, commands):\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "    def record_gameplay(self, number_of_screenshots, screenshot_delay, grayscale=False, resize=False, path=None):\n",
    "\n",
    "        # Resizing and grayscaling isn't really necessary here, but can save you some time later.\n",
    "        # Both saving you from writing more code and from making your hardware having to process more and more data at once.\n",
    "\n",
    "        print(f\"Ok. Screenshot capture will begin in 5 seconds\")\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "        for i in range(number_of_screenshots):\n",
    "\n",
    "            with mss() as sct:\n",
    "\n",
    "                frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "                frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if grayscale:\n",
    "\n",
    "                frame = frame.convert('L')\n",
    "\n",
    "            if resize:\n",
    "\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame.save(f\"{path}/{i+2000}.png\")\n",
    "\n",
    "            sleep(screenshot_delay)\n",
    "        \n",
    "        print(\"Screenshot capture finished!\")\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jigoku Kisetsukan\n",
    "\n",
    "command_type = ['key']\n",
    "\n",
    "actions1 = ['Down', 'Up']\n",
    "\n",
    "actions2 = ['up', 'down', 'left', 'right', 'z', 'x', 'shift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(command_types=command_type, actions1=actions1, actions2=actions2, resize=(200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 100, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(100)\n",
    "        self.conv2 = torch.nn.Conv2d(100, 100, kernel_size=3, stride=1, padding=1, bias=False) # 200x200\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(100)\n",
    "        self.conv3 = torch.nn.Conv2d(100, 200, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(200)\n",
    "        self.conv4 = torch.nn.Conv2d(200, 200, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(200)\n",
    "        # Add pool 2x2 ---> 50x50\n",
    "        self.conv5 = torch.nn.Conv2d(200, 400, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(400)\n",
    "        self.conv6 = torch.nn.Conv2d(400, 400, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm6 = torch.nn.BatchNorm2d(400)\n",
    "        # Add pool 2x2 ---> 25x25\n",
    "        self.conv7 = torch.nn.Conv2d(400, 800, kernel_size=4, stride=1, bias=False) # 22x22\n",
    "        self.batchnorm7 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv8 = torch.nn.Conv2d(800, 1000, kernel_size=3, stride=1, bias=False) # 20x20\n",
    "        self.batchnorm8 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 10x10\n",
    "        self.conv9 = torch.nn.Conv2d(1000, 800, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm9 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv10 = torch.nn.Conv2d(800, 400, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm10 = torch.nn.BatchNorm2d(400)\n",
    "        # Add pool 2x2 ---> 5x5\n",
    "        self.neuron1 = torch.nn.Linear(400*5*5, 200*2*2, bias=False)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(200*2*2)\n",
    "\n",
    "        self.PRelu = torch.nn.PReLU()\n",
    "        self.pool2x2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = torch.nn.Dropout2d(0.25)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.conv1(input)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.PRelu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.PRelu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.PRelu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.PRelu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.batchnorm5(x)\n",
    "        x = self.PRelu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.batchnorm6(x)\n",
    "        x = self.PRelu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = self.batchnorm7(x)\n",
    "        x = self.PRelu(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.batchnorm8(x)\n",
    "        x = self.PRelu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = self.conv9(x)\n",
    "        x = self.batchnorm9(x)\n",
    "        x = self.PRelu(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.batchnorm10(x)\n",
    "        x = self.PRelu(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2x2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1) # (batch, 400*5*5)\n",
    "\n",
    "        x = self.neuron1(x) # (batch, 200*2*2)\n",
    "        x = self.layer_norm1(x)\n",
    "        output = self.PRelu(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, command_types, actions1, actions2):\n",
    "\n",
    "        super(Teacher, self).__init__()\n",
    "\n",
    "        self.command_types = len(command_types)\n",
    "        self.actions1 = len(actions1)\n",
    "        self.actions2 = len(actions2)\n",
    "\n",
    "        # Remember: The features have been properly extracted into a tensor with size (Batch, 200*2*2)\n",
    "\n",
    "        self.neuron_type = torch.nn.Linear(200*2*2, self.command_types, bias=False)\n",
    "        self.neuron_action1 = torch.nn.Linear(200*2*2, self.actions1, bias=False)\n",
    "        self.neuron_action2 = torch.nn.Linear(200*2*2, self.actions2, bias=False)\n",
    "        \n",
    "        self.pool2x2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.PRelu = torch.nn.PReLU()\n",
    "        self.softmax = torch.nn.LogSoftmax(-1)\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        command_type = self.neuron_type(features)\n",
    "        command_type = self.softmax(command_type)\n",
    "\n",
    "        action1 = self.neuron_action1(features)\n",
    "        action1 = self.softmax(action1)\n",
    "        \n",
    "        action2 = self.neuron_action2(features)\n",
    "        action2 = self.softmax(action2)\n",
    "\n",
    "        return (command_type, action1, action2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hakisa(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, command_types, actions1, action1_dims, actions2, action2_dims, mode='Study'):\n",
    "\n",
    "        super(Hakisa, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.command_types = len(command_types)\n",
    "        self.actions1 = len(actions1)\n",
    "        self.action1_dims = action1_dims # How many dimensions the embedding matrix will have\n",
    "        self.actions2 = len(actions2)\n",
    "        self.action2_dims = action2_dims\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.teacher = Teacher(command_types, actions1, actions2)\n",
    "\n",
    "        # Features extracted. \n",
    "\n",
    "        if len(self.command_types) > 1:\n",
    "\n",
    "            self.embed_command_type = torch.nn.Embedding(len(self.command_types), len(self.command_types)) # To be concatenated to features extracted from state\n",
    "\n",
    "            self.neuron_command_study = torch.nn.Linear(200*2*2, len(self.command_types), bias=False)\n",
    "            self.neuron_command_play = torch.nn.Linear(1200, len(self.command_types), bias=False)\n",
    "\n",
    "            # Considering the command_type that has been predicted, what should be the action1 and action2?\n",
    "\n",
    "            self.neuron2 = torch.nn.Linear(len(self.command_types), 100, bias=False)\n",
    "\n",
    "        else:\n",
    "        \n",
    "            self.neuron2_study = torch.nn.Linear(200*2*2, 100, bias=False)\n",
    "            self.neuron2_play = torch.nn.Linear(1000, 100, bias=False)\n",
    "\n",
    "        self.neuron_quality = torch.nn.Linear(1, 100, bias=False)\n",
    "\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(100)\n",
    "\n",
    "        self.embed_action1 = torch.nn.Embedding(self.actions1, self.action1_dims)\n",
    "        self.neuron_action1A = torch.nn.Linear(self.actions1*self.action1_dims, 100, bias=False) # To be concatenated with neuron 2\n",
    "        self.neuron_action1B = torch.nn.Linear(100*2, self.actions1, bias=False) # Apply softmax\n",
    "\n",
    "        self.embed_action2 = torch.nn.Embedding(self.actions2, self.action2_dims)\n",
    "        self.neuron_action2A = torch.nn.Linear(self.actions2*self.action2_dims, 100, bias=False) # To be concatenated with neuron 2\n",
    "        self.neuron_action2B = torch.nn.Linear(100*2, self.actions2, bias=False)\n",
    "\n",
    "        self.neuron_reward = torch.nn.Linear(1, 100, bias=False)\n",
    "        self.layer_normcat = torch.nn.LayerNorm(1000)\n",
    "        self.neuron_predquality = torch.nn.Linear(self.command_types+self.actions1+self.actions2, 3, bias=False)\n",
    "        self.neuron_predreward1 = torch.nn.Linear(self.command_types+self.actions1+self.actions2, 100, bias=False)\n",
    "        self.neuron_predreward2 = torch.nn.Linear(100, 1, bias=False)\n",
    "\n",
    "        self.PRelu = torch.nn.PReLU(1)\n",
    "\n",
    "        self.softmax = torch.nn.LogSoftmax(-1) # Since we're using softmax here, use NLLLoss during study and play mode.\n",
    "    \n",
    "\n",
    "    def forward(self, input_frame, previous_action=None, previous_reward=None):\n",
    "\n",
    "        features = self.feature_extractor(input_frame)\n",
    "\n",
    "        if self.mode == \"Study\":\n",
    "\n",
    "            labelsA = self.teacher(features)\n",
    "            labelsB = self.teacher(features) # To avoid having to iterate through everything 2 times in a single batch.\n",
    "\n",
    "            if self.command_types > 1:\n",
    "\n",
    "                vector = self.embed_command_type(torch.arange(self.command_types, device=device)) # (Batch, 1, n_commands)\n",
    "                vector = vector.view(features.size(0), -1) # (batch, n_commands)\n",
    "\n",
    "                command_type = self.neuron_command_study(features) # (Batch, n_commands)\n",
    "\n",
    "                command_type = command_type + vector\n",
    "\n",
    "                del vector\n",
    "\n",
    "                command_type = self.softmax(command_type) # (Batch, n_commands)\n",
    "\n",
    "                x = self.neuron2(command_type) # (Batch, 100)\n",
    "\n",
    "            else:\n",
    "                command_type = torch.zeros((features.size(0), 1), device=device)\n",
    "\n",
    "                x = self.neuron2_study(features) # (Batch, 100)\n",
    "\n",
    "            x = self.layer_norm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            vector = self.embed_action1(torch.arange(self.actions1, device=device)) # (Batch, 1, n_action1_dims)\n",
    "            vector = vector.view(x.size(0), -1) # (Batch, n_action1_dims)\n",
    "            vector = self.neuron_action1A(vector) # (Batch, 100)\n",
    "\n",
    "            vector = torch.cat((x, vector), -1) # (Batch, 100*2)\n",
    "\n",
    "            action1 = self.neuron_action1B(vector) # (Batch, len(actions1))\n",
    "\n",
    "            action1 = self.softmax(action1)\n",
    "\n",
    "            vector = self.embed_action2(torch.arange(self.actions2, device=device)) # (Batch, 1, n_action2_dims)\n",
    "            vector = vector.view(x.size(0), -1) # (Batch, n_action2_dims)\n",
    "            vector = self.neuron_action2A(vector) # (Batch, 100)\n",
    "\n",
    "            vector = torch.cat((x, vector), -1) # (Batch, 100*2)\n",
    "\n",
    "            action2 = self.neuron_action2B(vector) # (Batch, 1)\n",
    "\n",
    "            action2 = self.softmax(action2)\n",
    "\n",
    "            del vector, x\n",
    "\n",
    "            return (command_type, action1, action2), labelsA, labelsB\n",
    "\n",
    "        else:\n",
    "\n",
    "            if previous_action==None and previous_reward==None: # For first iteration\n",
    "\n",
    "                previous_action = (torch.zeros((1), device=device), torch.zeros(1, device=device), torch.zeros(1, device=device))\n",
    "                previous_reward = torch.zeros((features.size(0), 1), device=device)\n",
    "\n",
    "            a, b, c = previous_action\n",
    "\n",
    "            previous_action = a + b + c\n",
    "\n",
    "            del a, b, c\n",
    "\n",
    "            previous_action = previous_action.unsqueeze(0) # (Batch, actions_concatenated)\n",
    "\n",
    "            previous_action = self.neuron_quality(previous_action) # (batch, 100)\n",
    "            previous_reward = self.neuron_reward(previous_reward) # (batch, 100)\n",
    "\n",
    "            x = torch.cat((features, previous_action, previous_reward), 1) # (batch, 1000)\n",
    "\n",
    "            x = self.layer_normcat(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            if self.command_types > 1:\n",
    "\n",
    "                vector = self.embed_command_type(torch.arange(self.command_types, device=device)) # (Batch, 1, n_commands)\n",
    "                vector = vector.view(x.size(0), -1) # (batch, n_commands)\n",
    "\n",
    "                command_type = self.neuron_command_play(x)\n",
    "                command_type = command_type + vector\n",
    "\n",
    "                del vector\n",
    "\n",
    "                command_type = self.softmax(command_type) # (Batch, n_commands)\n",
    "\n",
    "                x = self.neuron2(command_type)\n",
    "\n",
    "            else:\n",
    "                command_type = torch.zeros((x.size(0), 1), device=device)\n",
    "\n",
    "                x = self.neuron2_play(x) # (Batch, 100)\n",
    "\n",
    "            x = self.layer_norm2(x)\n",
    "            x = self.PRelu(x)\n",
    "\n",
    "            vector = self.embed_action1(torch.arange(self.actions1, device=device)) # (Batch, 1, n_action1_dims)\n",
    "            vector = vector.view(x.size(0), -1) # (Batch, n_action1_dims)\n",
    "            vector = self.neuron_action1A(vector) # (Batch, 100)\n",
    "\n",
    "            vector = torch.cat((x, vector), -1) # (Batch, 100*2)\n",
    "\n",
    "            action1 = self.neuron_action1B(vector) # (Batch, len(actions1))\n",
    "\n",
    "            action1 = self.softmax(action1) # Since each action here have size (1, len(actions1)), access each action with actions1[torch.argmax(action1, -1)]\n",
    "\n",
    "            vector = self.embed_action2(torch.arange(self.actions2, device=device)) # (Batch, 1, n_action2_dims)\n",
    "            vector = vector.view(x.size(0), -1) # (Batch, n_action2_dims)\n",
    "            vector = self.neuron_action2A(vector) # (Batch, 100)\n",
    "\n",
    "            vector = torch.cat((x, vector), -1) # (Batch, 100*2)\n",
    "\n",
    "            action2 = self.neuron_action2B(vector) # (Batch, len(actions2))\n",
    "\n",
    "            action2 = self.softmax(action2)\n",
    "\n",
    "            del vector\n",
    "\n",
    "            x = torch.cat((command_type, action1, action2), 1) # (Batch, len(command_type) + len(action1) + len(action2))\n",
    "\n",
    "            # Attention: .detach() here causes all the previous layers to be excluded from backpropagation.\n",
    "            # This happens because the backpropagation is based on predicted_reward only.\n",
    "\n",
    "            command_quality = self.neuron_predquality(x) # (Batch, 3)\n",
    "            command_quality = self.softmax(command_quality)\n",
    "\n",
    "            x = self.neuron_predreward1(x)\n",
    "            predicted_reward = self.neuron_predreward2(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "            return (command_type, action1, action2), command_quality, predicted_reward\n",
    "\n",
    "\n",
    "    def execute_command(self, command):\n",
    "        '''\n",
    "        Command must be a tuple(command_type, action1, action2), where:\n",
    "\n",
    "            command_type: key(keyboard) or move, rightClick, click(mouse)\n",
    "            action1: Up, Down, press(keyboard), X coordinate(mouse) or None(no mouse movement)\n",
    "            action2: 'a', 'z', 'shift'...(keyboard), Y coordinate(mouse) or None(no mouse movement)\n",
    "\n",
    "        Make sure all key actions(action2) are lowered.\n",
    "\n",
    "        Have in mind that Hakisa might output command_type 'key' and action1 that is equivalent to a mouse action.\n",
    "        '''\n",
    "\n",
    "        if \"key\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                \n",
    "                if \"Up\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        #pyautogui.keyUp(command[2])\n",
    "                        keyboard.release(command[2])\n",
    "                \n",
    "                    except:\n",
    "                        pass # If Hakisa predicts a mouse action for a keyboard command, she won't do anything.\n",
    "\n",
    "                elif \"Down\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        #pyautogui.keyDown(command[2])\n",
    "                        keyboard.press(command[2])\n",
    "\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif \"press\" in command[1]:\n",
    "\n",
    "                    try:\n",
    "                        keyboard.send(command[2]) # Some games won't work with pyautogui.press(), so use keyboard module, since we'll import it for Play Mode.\n",
    "                    \n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            except:\n",
    "\n",
    "                pass # If Hakisa predicts a keyboard command, but outputs a mouse action, she won't do anything.\n",
    "\n",
    "        elif \"move\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                #pyautogui.moveTo(command[1], command[2], duration=0.19) # Duration = 0.19 seconds to be more realistic\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "\n",
    "            except:\n",
    "                pass # If Hakisa predict a mouse command, but outputs a keyboard action, she won't do anything.\n",
    "\n",
    "        elif \"rightclick\" in command[0]:\n",
    "            \n",
    "            try:\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "                mouse.right_click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif \"click\" in command[0]:\n",
    "\n",
    "            try:\n",
    "                #pyautogui.moveTo(command[1], command[2], duration=0.19)\n",
    "                mouse.move(command[1], command[2], duration=0.1)\n",
    "                mouse.click() # Same case as press. Use mouse module.\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError # It was probably you who made a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hakisa = Hakisa(command_types=command_type, actions1=actions1, action1_dims=1, actions2=actions2, action2_dims=1, mode='Study').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.command_type)\n",
    "print(dataset.actions1)\n",
    "print(dataset.actions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "images_by_order = []\n",
    "\n",
    "for directory, _, files in os.walk(\"D:/Python/Projects/Hakisa/Hakisa/JK_gameplay/\"):\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        file = file.split('.')\n",
    "        file = file[0] # Getting exclusively the number\n",
    "\n",
    "        images_by_order.append(file)\n",
    "\n",
    "images_by_order = sorted([int(x) for x in images_by_order])\n",
    "\n",
    "# Problem: for strings, Python considers that 1000 < 2. Maybe something related to how the string is assembled?\n",
    "\n",
    "images_data = []\n",
    "\n",
    "for i in images_by_order[0:100]:\n",
    "\n",
    "    i = directory + '/' + str(i) + '.png'\n",
    "    image = Image.open(i)\n",
    "    image = image.resize((200, 200))\n",
    "    array = np.array(image, dtype=np.float32)\n",
    "    image.close()\n",
    "    images_data.append(array)\n",
    "\n",
    "images_data = np.stack(images_data, 0)\n",
    "images_data = torch.from_numpy(images_data)\n",
    "images_data = images_data.view(images_data.size(0), images_data.size(3), images_data.size(1), images_data.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.create_data(images_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, gamma=0.1)\n",
    "loss = torch.nn.NLLLoss()\n",
    "best_loss = float('inf')\n",
    "grads = []\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studying loop - Self-Learning\n",
    "\n",
    "# Can be applied to any game\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for i, input_frame in enumerate(dataloader):\n",
    "\n",
    "        input_frame = input_frame.to(device)\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        cmds, pseudo_pred, pseudo_labels = hakisa(input_frame)\n",
    "\n",
    "        del input_frame\n",
    "\n",
    "        command_type, command_type_pred, command_type_target = cmds[0], pseudo_pred[0], pseudo_labels[0].detach().argmax()\n",
    "        action1, action1_pred, action1_target = cmds[1], pseudo_pred[1], pseudo_labels[1].detach().argmax()\n",
    "        action2, action2_pred, action2_target = cmds[2], pseudo_pred[2], pseudo_labels[2].detach().argmax()\n",
    "\n",
    "        # Used to promote consistency in the teacher network ---> Consider using an output EMA, or even Hakisa's gradients EMA.\n",
    "\n",
    "        teacher_type_loss = loss(command_type_pred, command_type_target)\n",
    "        teacher_action1_loss = loss(action1_pred, action1_target)\n",
    "        teacher_action2_loss = loss(action2_pred, action2_target)\n",
    "\n",
    "        command_type_loss = loss(command_type, command_type_target)\n",
    "        action1_loss = loss(action1, action1_target)\n",
    "        action2_loss = loss(action2, action2_loss)\n",
    "\n",
    "        command_type_cost = (teacher_type_loss * (1 - (0.99**epoch))) + command_type_loss # Teacher loss influence must grow over time. 0.99 decays faster than 0.999.\n",
    "        action1_cost = (teacher_action1_loss * (1 - (0.99**epoch))) + action1_loss\n",
    "        action2_cost = (teacher_action2_loss * (1 - (0.99**epoch))) + action2_loss\n",
    "\n",
    "        study_loss = command_type_cost + action1_cost + action2_loss\n",
    "\n",
    "        study_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        for n, p in hakisa.named_parameters():\n",
    "\n",
    "            if 'neuron1.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        if study_loss.item() < best_loss and epoch > 0:\n",
    "\n",
    "            best_loss = study_loss.item()\n",
    "            best_params = hakisa.state_dict()\n",
    "\n",
    "        if i % dataset.memory_size == 0:\n",
    "            print(f\"{epoch}/{epochs}\")\n",
    "            print(f\"Best Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "            print(f\"Teacher Type Loss: {teacher_type_loss.item()}\\tAction1 Loss: {teacher_action1_loss.item()}\\tAction2 Loss: {teacher_action2_loss.item()}\")\n",
    "            print(f\"Hakisa Type Loss: {command_type_loss.item()}\\tAction1 Loss: {action1_loss.item()}\\tAction2 Loss: {action2_loss.item()}\")\n",
    "            print(f\"Command Type Cost: {command_type_cost.item()}\\tAction1 Cost: {action1_cost.item()}\\tAction2 Cost: {action2_cost.item()}\\nStudy Loss: {study_loss.item()}\")\n",
    "\n",
    "            if save_path is None:\n",
    "                try:\n",
    "                    os.mkdir(\"Hakisa\")\n",
    "                    save_path = \"Hakisa\"\n",
    "                except:\n",
    "                    save_path = \"Hakisa\"\n",
    "                    \n",
    "            torch.save({\n",
    "                'Epoch': epoch,\n",
    "                'Hakisa_params': best_params,\n",
    "                'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "            }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "            print(\"Model saved!\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_Jigoku(score):\n",
    "    # For the game Jigoku Kisetsukan: Sense of the Seasons\n",
    "\n",
    "    score = score.replace('S', '5').replace('s', '8').replace('e', '2').replace('O', '0').replace('B', '8').replace('o', '4').replace('b', '4')\n",
    "    score = score.replace('I', '1').replace('l', '1').replace('.', '')\n",
    "\n",
    "    try:\n",
    "        score = float(score)\n",
    "\n",
    "    except ValueError:\n",
    "            score = 1.0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing loop - She learns as she plays\n",
    "# Jigoku Kisetsukan\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sleep(5)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_learnweaponskill_Rapier_2_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "hakisa.mode = 'Play'\n",
    "\n",
    "reward = 0. # Cumulative reward\n",
    "learning_rate = []\n",
    "grads = []\n",
    "grad_clip = None\n",
    "save_path = 'Hakisa'\n",
    "steps = 0\n",
    "save_point = 10 # Also optimization point\n",
    "uncertainty_factor = 0.9 # Also known as gamma or discount factor\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.1)\n",
    "\n",
    "action_quality_loss = torch.nn.NLLLoss()\n",
    "reward_loss = torch.nn.MSELoss()\n",
    "\n",
    "while keyboard.is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    frame = dataset.grab_frame()\n",
    "\n",
    "    if steps == 0: # First iteration\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame)\n",
    "\n",
    "    else:\n",
    "\n",
    "        cmds, command_quality, predicted_reward = hakisa(frame, previous_action=(cmds[0].detach(), cmds[1].detach(), cmds[2].detach()), previous_reward=predicted_reward.detach())\n",
    "\n",
    "    del frame\n",
    "\n",
    "    command = dataset.get_command(cmds[0].detach().cpu().numpy(), cmds[1].detach().cpu().numpy(), cmds[2].detach().cpu().numpy())\n",
    "\n",
    "    hakisa.execute_command(command)\n",
    "\n",
    "    score = dataset.get_consequences(1008, 1429, 1723-1429, 1046-1008, tesseract_config='--psm 8')\n",
    "\n",
    "    score = preprocess_Jigoku(score)\n",
    "\n",
    "    mult_score = dataset.get_consequences(933, 1536, 1723-1536, 978-933, tesseract_config='--psm 8')\n",
    "\n",
    "    mult_score = preprocess_Jigoku(mult_score)\n",
    "\n",
    "    life = dataset.get_consequences(849, 400, 498-400, 904-849, tesseract_config='--psm 8')\n",
    "\n",
    "    life = preprocess_Jigoku(life)\n",
    "\n",
    "    power = dataset.get_consequences(923, 405, 503-405, 978-923, tesseract_config='--psm 8')\n",
    "\n",
    "    power = preprocess_Jigoku(power)\n",
    "\n",
    "    aura = dataset.get_consequences(1001, 400, 1045-1001, 503-400, tesseract_config='--psm 8')\n",
    "\n",
    "    aura = preprocess_Jigoku(aura)\n",
    "    aura = aura/100\n",
    "\n",
    "    if life == 0:\n",
    "\n",
    "        try:\n",
    "            reward += -(100./(score * mult_score))\n",
    "        \n",
    "        except ZeroDivisionError:\n",
    "            reward += -10.\n",
    "\n",
    "    else:\n",
    "\n",
    "        reward += ((score * mult_score) + (power * aura))*1e-6\n",
    "\n",
    "    del score, mult_score, power, aura, life\n",
    "\n",
    "    reward = torch.tensor(reward, device=device)\n",
    "\n",
    "    if steps == 0:\n",
    "\n",
    "        previous_command_quality = command_quality.detach()\n",
    "\n",
    "    action_quality_cost = action_quality_loss(previous_command_quality, command_quality.argmax(1).detach()) # Input = (1, Classes), Target = (1)\n",
    "\n",
    "    predicted_reward = predicted_reward * uncertainty_factor\n",
    "\n",
    "    reward_cost = reward_loss(predicted_reward, reward)\n",
    "\n",
    "    gameplay_loss = action_quality_cost + reward_cost\n",
    "\n",
    "    gameplay_loss.backward()\n",
    "\n",
    "    previous_command_quality = command_quality.detach()\n",
    "\n",
    "    del command_quality\n",
    "\n",
    "    for n, p in hakisa.named_parameters(): # Checking how the grads and backpropagation are going\n",
    "\n",
    "            if 'neuron1.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                p.register_hook(lambda grad: torch.clamp(grad, -grad_clip, grad_clip))\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    if gameplay_loss.item() < best_loss:\n",
    "\n",
    "        best_loss = gameplay_loss.item()\n",
    "        best_params = hakisa.state_dict()\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Best Loss: {best_loss}\\tCurrent LR: {scheduler.get_last_lr()[0]}\\tGradients Average: {grads[-1]}\")\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward}\")\n",
    "\n",
    "        torch.save({\n",
    "            'Steps': steps,\n",
    "            'Hakisa_params': best_params,\n",
    "            'Hakisa_LR': scheduler.get_last_lr()[0]\n",
    "        }, f\"{save_path}/Hakisa_checkpoint.tar\")\n",
    "\n",
    "\n",
    "        winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

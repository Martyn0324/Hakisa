{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # For debugging images\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from thalamus import Thalamus\n",
    "from limbicusBH import ResidualBlock, LimbicSystemA, LimbicSystemB, LimbicSystemC\n",
    "from human_teacher import Keylogger, record, save_chunk\n",
    "from study_mode import train_batch_study, train_one_epoch_study\n",
    "from consolidation import train_batch_consolidation, train_one_epoch_consolidation\n",
    "from HakisaBH import Hakisa\n",
    "from keyboard import is_pressed # To stop certain iterations\n",
    "from time import sleep\n",
    "import winsound # To know when it's already running\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to implement Hakisa (again) using PPO2 and, this time,\n",
    "trying to make things more readable and with less code.\n",
    "\n",
    "Trying to implement Hakisa (again), but this time focusing on Deep Q-Learning.\n",
    "Maybe we'll use PPO2 later.\n",
    "\n",
    "Interesting sources:\n",
    "- https://github.com/liuruoze/HierNet-SC2/blob/396646056dbe5f8f20e43e0ef35e59db09e907c0/algo/ppo.py#L180 - Ruo Ze Liu's PPO2 implementation (in Tensorflow)\n",
    "- https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/ppo2/ppo2.py#L185 - Original code from Stable Balines\n",
    "- https://medium.com/analytics-vidhya/coding-ppo-from-scratch-with-pytorch-part-1-4-613dfc1b14c8 - Wonderful tutorial by Eric Yang Yu (Pytorch), this implementation is for **CONTINUOUS** action spaces, which are most of Gym's Environments (Robots joints)\n",
    "- https://spinningup.openai.com/en/latest/spinningup/spinningup.html - Spinning Up, by Josh Achiam from OpenAI. Interesting hyperlinks and wonderfully simple explanations\n",
    "- https://www.alexirpan.com/2018/02/14/rl-hard.html - Deep Reinforcement Learning Doesn't Work Yet. Problem is... I can't refuse the challenge of GANs or RLs...\n",
    "- https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/deepq/dqn.py - Deep Q-Learning code from Stable Baselines\n",
    "- https://github.com/saashanair/rl-series/blob/master/dqn/dqn_agent.py - Implementation of Deep Q-Learning from scratch in Pytorch, by Saasha Nair. Codes from people who are not from OpenAI (or big techs in general) are almost always way more readable and easier to understand.\n",
    "- https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html - Pytorch tutorial on Reinforcement Learning using Deep Q-Learning. One of the few Pytorch tutorials that really teaches you something other than downloading and importing ready-made functions.\n",
    "- https://spinningup.openai.com/en/latest/spinningup/rl_intro.html - OpenAI's Spinning Up, by Josh Achiam. However, it's not really good to learn about Q-Learning, as it's still counter-intuitive and it's focused on On-Policy algorithms, like PPO's antecessors (TRPO and Vanilla Policy Gradient)\n",
    "- https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf - Deep Q-Learning original paper. Pay attention to the algorithm and the 3 equations there. Though the paper is quite clean and concise.\n",
    "- https://lilianweng.github.io/posts/2018-02-19-rl-overview/#q-learning-off-policy-td-control - Lilian Weng's blog about Reinforcement Learning.\n",
    "\n",
    "**POSSIBLE ISSUES:** Usually, PPO is applied with 2 different networks, policy and value function. Considering Deep-RL instability, it's possible that the idea of uniting both in the same network, though appealing, may be troublesome.\n",
    "(This is another argument in favor of DQN)\n",
    "\n",
    "\"Also, what we know about good CNN design from supervised learning land doesn’t seem to apply to reinforcement learning land, because you’re mostly bottlenecked by credit assignment / supervision bitrate, not by a lack of a powerful representation. Your ResNets, batchnorms, or very deep networks have no power here.\" - Hacker News comment from Andrej Karpathy, back when he was at OpenAI (extracted from Alex Irpan's blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_model = LimbicSystemA().to(device).eval()\n",
    "score_model = LimbicSystemB().to(device).eval()\n",
    "diamond_model = LimbicSystemC().to(device).eval()\n",
    "\n",
    "a = torch.load(\"D:/Python/Projects/Hakisa/Hakisa/LB_model.pt\")\n",
    "b = torch.load(\"D:/Python/Projects/Hakisa/Hakisa/SCORE_model.pt\")\n",
    "c = torch.load(\"D:/Python/Projects/Hakisa/Hakisa/DIAMOND_model.pt\")\n",
    "\n",
    "lb_model.load_state_dict(a)\n",
    "score_model.load_state_dict(b)\n",
    "diamond_model.load_state_dict(c)\n",
    "\n",
    "del a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bullet Heaven\n",
    "\n",
    "command_types = ['left', 'right']\n",
    "\n",
    "actions1 = [i for i in range(1, 1919)]\n",
    "\n",
    "actions2 = [i for i in range(1, 1079)]\n",
    "\n",
    "reward_regions = [\n",
    "    (750, 1700, 1900-1700, 990-750),\n",
    "    (180, 0, 240, 60),\n",
    "    (800, 0, 250, 250)\n",
    "]\n",
    "\n",
    "# ATTENTION: Using Resize and decreasing the inputs to 200x200 makes way for\n",
    "# computation and reaction time MUCH MORE faster\n",
    "# due to lower number of convolutions (both layers and operations within layers)\n",
    "\n",
    "thalamus = Thalamus(command_types, actions1, actions2, resize=(200, 200),\n",
    "                    reward_models=[lb_model, diamond_model, score_model],\n",
    "                    reward_regions=reward_regions)\n",
    "keylogger = Keylogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "'''\n",
    "Some parameters used in Gym's environments, which are too simple:\n",
    "https://github.com/araffin/rl-baselines-zoo/blob/master/hyperparams/ppo2.yml\n",
    "\n",
    "It's recommended to stick to something close to the parameters used by Ruo Ze Liu for HierNet, in StarCraft 2:\n",
    "\n",
    "https://github.com/liuruoze/HierNet-SC2/blob/396646056dbe5f8f20e43e0ef35e59db09e907c0/param.py\n",
    "\n",
    "\"Even Ignoring Generalization Issues, The Final Results Can be Unstable and Hard to Reproduce\" - Alex Irpan\n",
    "\n",
    "\"[Supervised learning] wants to work. Even if you screw something up you'll usually get something non-random back.\n",
    "RL must be forced to work.\n",
    "If you screw something up or don't tune something well enough you're exceedingly likely to get a policy that is even worse than random.\n",
    "And even if it's all well tuned you'll get a bad policy 30% of the time, just because.\" - Andrej Karpathy\n",
    "'''\n",
    "\n",
    "HUMAN_DATA_PATH = \"D:/Python/Projects/Hakisa/Hakisa/BH_gameplay\"\n",
    "chunk_size = 50 # The size of each chunk of observations(frames) to be saved\n",
    "loss_weights = [3.0, 1.0, 1.0] # Weight for Command Type, Action 1 and Action 2 loss\n",
    "BATCH_SIZE = 16 # In reality, all tensors are batch 1, so we'll use gradient accumulation to simulate multiple batches.\n",
    "EPOCHS = 1\n",
    "lr = 1e-7\n",
    "gamma = 0.995 # Gamma for the Discount Rewards.\n",
    "target_delay = BATCH_SIZE * 16 # Steps before applying update to the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human Teaching Mode\n",
    "\n",
    "chunks = 0\n",
    "\n",
    "states = []\n",
    "next_states = []\n",
    "actions = []\n",
    "probs = []\n",
    "rewards = []\n",
    "\n",
    "sleep(3)\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)\n",
    "\n",
    "while is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    obs, next_observation, action, prob, reward = record(thalamus, keylogger)\n",
    "\n",
    "    states.append(obs)\n",
    "    next_states.append(next_observation)\n",
    "    actions.append(action)\n",
    "    probs.append(prob)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    # Saving chunk of observations\n",
    "\n",
    "    if len(states) == chunk_size:\n",
    "\n",
    "        save_chunk(states, chunks, HUMAN_DATA_PATH)\n",
    "        save_chunk(next_states, chunks, HUMAN_DATA_PATH)\n",
    "\n",
    "        del states, next_states\n",
    "\n",
    "        states = []\n",
    "        next_states = []\n",
    "\n",
    "        chunks += 1\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)\n",
    "print(chunks)\n",
    "torch.save((actions, probs, rewards), f'{HUMAN_DATA_PATH}/human_episode_0-{chunks}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hakisa = Hakisa(command_types, actions1, actions2).to(device)\n",
    "target_network = deepcopy(hakisa).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12536/4217120157.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mcrossentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'probs' is not defined"
     ]
    }
   ],
   "source": [
    "# ATTENTION: the first iteration will take too much time\n",
    "# as Pytorch will be allocating memory for backpropagation.\n",
    "# It's recommended to perform a first forward+backward separately\n",
    "# Make the second forward+backward propagation if you'd like to make sure\n",
    "# This decreases time from ~10 minutes to around 5 seconds\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "labels = probs[0]\n",
    "\n",
    "crossentropy = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "inputest = torch.randn((1, 3, 1920, 1080), device=device)\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "act = hakisa(inputest)\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "print(f\"Forward Propagation time: {end - start}\")\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "fake_loss = crossentropy(act[0], labels[0].to(device)) + crossentropy(act[1], labels[1].to(device)) + crossentropy(act[2], labels[2].to(device))\n",
    "fake_loss.backward()\n",
    "\n",
    "end = datetime.now()\n",
    "\n",
    "print(f\"Backward Computation time: {end - start}\")\n",
    "\n",
    "hakisa.zero_grad()\n",
    "\n",
    "del datetime, inputest, fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hakisa.load_state_dict(torch.load('D:/Python/Projects/Hakisa/Hakisa/Hakisa_BHSL.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING PHASE\n",
    "# She learns from us\n",
    "# Attention: the first iterations until the print\n",
    "# will take longer (~11 minutes/10 steps).\n",
    "# The upcoming ones will be faster (~2 minutes/10 steps)\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=lr)\n",
    "crossentropy = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    epoch_loss, steps = train_one_epoch_study(\n",
    "        probs,\n",
    "        chunks,\n",
    "        chunk_size,\n",
    "        BATCH_SIZE,\n",
    "        hakisa,\n",
    "        optimizer,\n",
    "        HUMAN_DATA_PATH,\n",
    "        loss_weights,\n",
    "        print_delay = 1000\n",
    "    )\n",
    "\n",
    "    print(f\"{epoch}/{EPOCHS}\")\n",
    "    print(f\"{steps}\")\n",
    "    print(f\"Epoch Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSOLIDATION PHASE - https://en.wikipedia.org/wiki/Memory_consolidation\n",
    "# She remembers what she studied, and learns from it.\n",
    "\n",
    "# In RL, an Epoch consists of an entire episode + training from that episode,\n",
    "# while a Batch consists of an episode.\n",
    "# DQN also uses Batch as a number of samples extracted from memory.\n",
    "# https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\n",
    "# We'll stick to traditional terminology of ML, to avoid needless confusion.\n",
    "\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    epoch_loss, steps = train_one_epoch_consolidation(\n",
    "        actions,\n",
    "        rewards,\n",
    "        chunks,\n",
    "        chunk_size,\n",
    "        BATCH_SIZE,\n",
    "        hakisa,\n",
    "        target_network,\n",
    "        optimizer,\n",
    "        HUMAN_DATA_PATH,\n",
    "        loss_weights,\n",
    "        gamma,\n",
    "        target_delay = 16 * BATCH_SIZE,\n",
    "        print_delay = 1000\n",
    "    )\n",
    "\n",
    "    print(f\"{epoch}/{EPOCHS}\")\n",
    "    print(f\"{steps}\")\n",
    "    print(f\"Epoch Loss: {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Command Type Loss: 366238080.0\tAction1 Loss: 366240992.0\tAction2 Loss: 366239872.0\n",
      "Current Loss: 1831195136.0\n",
      "200\n",
      "Command Type Loss: 430958144.0\tAction1 Loss: 430961856.0\tAction2 Loss: 430959424.0\n",
      "Current Loss: 2154795776.0\n",
      "300\n",
      "Command Type Loss: 431722368.0\tAction1 Loss: 431725120.0\tAction2 Loss: 431726592.0\n",
      "Current Loss: 2158618880.0\n",
      "400\n",
      "Command Type Loss: 443825088.0\tAction1 Loss: 443829024.0\tAction2 Loss: 443828128.0\n",
      "Current Loss: 2219132416.0\n",
      "500\n",
      "Command Type Loss: 347929664.0\tAction1 Loss: 347930400.0\tAction2 Loss: 347930976.0\n",
      "Current Loss: 1739650432.0\n",
      "600\n",
      "Command Type Loss: 28274830.0\tAction1 Loss: 28275750.0\tAction2 Loss: 28276020.0\n",
      "Current Loss: 141376256.0\n",
      "700\n",
      "Command Type Loss: 448387904.0\tAction1 Loss: 448390784.0\tAction2 Loss: 448390720.0\n",
      "Current Loss: 2241945344.0\n"
     ]
    }
   ],
   "source": [
    "# Play and Learn\n",
    "# This method is more unstable than using Exploration + Consolidation.\n",
    "# It's recommended to use Exploration + Consolidation and, only then, this method.\n",
    "\n",
    "'''\n",
    "\"Use reinforcement learning just as the fine-tuning step:\n",
    "The first AlphaGo paper started with supervised learning, and then did RL fine-tuning on top of it.\n",
    "This is a nice recipe, since it lets you use a faster-but-less-powerful method to speed up initial learning.\n",
    "It's worked in other contexts - see Sequence Tutor (Jaques et al, ICML 2017).\n",
    "You can view this as starting the RL process with a reasonable prior, instead of a random one,\n",
    "where the problem of learning the prior is offloaded to some other approach.\"\n",
    "- Alex Irpan, https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "\n",
    "\"In SL training, we found a learning rate of 1e-4 and 10 training epochs achieve the\n",
    "best result. The best model achieves a 0.15 win rate against the level-1 built-in AI.\n",
    "Note that though this result is not as good as that we acquire in the HRL method, the training\n",
    "here faces 564 actions, thus is much difficult.\"\n",
    "- Liu, Ruo-Ze et al. On Efficient Reinforcement Learning for Full-length Game of StarCraft II\n",
    "'''\n",
    "\n",
    "epsilon = 0.99 # For exploration x exploitation (epsilon-greedy)\n",
    "# Remember that 0.99 decays faster than 0.999\n",
    "\n",
    "steps = 0\n",
    "optimizer = torch.optim.Adam(hakisa.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)\n",
    "\n",
    "while is_pressed('esc') == False: # Exit loop when Esc is pressed\n",
    "\n",
    "    obs = thalamus.grab_frame()\n",
    "\n",
    "    Q_s = hakisa(obs.to(device))\n",
    "\n",
    "    if torch.rand((1,)).item() < epsilon: # Epsilon-greedy\n",
    "\n",
    "        command_type = torch.nn.functional.softmax(Q_s[0], -1).multinomial(1).item()\n",
    "        Q_s_a_command_type = Q_s[0][0, command_type]\n",
    "\n",
    "        action1 = torch.nn.functional.softmax(Q_s[1], -1).multinomial(1).item()\n",
    "        Q_s_a_action1 = Q_s[1][0, action1]\n",
    "\n",
    "        action2 = torch.nn.functional.softmax(Q_s[2], -1).multinomial(1).item()\n",
    "        Q_s_a_action2 = Q_s[2][0, action2]\n",
    "    \n",
    "    else:\n",
    "\n",
    "        command_type = Q_s[0].argmax(-1).item()\n",
    "        Q_s_a_command_type = Q_s[0][0, command_type]\n",
    "\n",
    "        action1 = Q_s[1].argmax(-1).item()\n",
    "        Q_s_a_action1 = Q_s[1][0, action1]\n",
    "\n",
    "        action2 = Q_s[2].argmax(-1).item()\n",
    "        Q_s_a_action2 = Q_s[2][0, action2]\n",
    "\n",
    "    thalamus.execute_command(command_type, action1, action2)\n",
    "\n",
    "    # Getting consequences\n",
    "\n",
    "    obs_rewards = thalamus.capture_regions()\n",
    "\n",
    "    reward = thalamus.get_reward(obs_rewards)\n",
    "\n",
    "    del obs_rewards\n",
    "\n",
    "    obs = thalamus.grab_frame()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        Q_target = target_network(obs.to(device))\n",
    "\n",
    "    Q_t_a_command_type = Q_target[0][0, Q_target[0].argmax(-1)]\n",
    "    Q_t_a_action1 = Q_target[1][0, Q_target[1].argmax(-1)]\n",
    "    Q_t_a_action2 = Q_target[2][0, Q_target[2].argmax(-1)]\n",
    "\n",
    "    y_i_command_type = reward.item() + gamma * Q_t_a_command_type\n",
    "    y_i_action1 = reward.item() + gamma * Q_t_a_action1\n",
    "    y_i_action2 = reward.item() + gamma * Q_t_a_action2\n",
    "\n",
    "    command_type_loss = criterion(Q_s_a_command_type, y_i_command_type)\n",
    "    action1_loss = criterion(Q_s_a_action1, y_i_action1)\n",
    "    action2_loss = criterion(Q_s_a_action2, y_i_action2)\n",
    "\n",
    "    total_loss = (command_type_loss * 3.0) + action1_loss + action2_loss\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    if steps % BATCH_SIZE == 0:\n",
    "\n",
    "        optimizer.step()\n",
    "        hakisa.zero_grad()\n",
    "\n",
    "        epsilon = (epsilon ** (steps//BATCH_SIZE))\n",
    "\n",
    "    if steps % target_delay:\n",
    "\n",
    "        target_network.load_state_dict(hakisa.state_dict())\n",
    "
    "\n",
    "    if steps % 100 == 0:\n",
    "\n",
    "        print(f\"{steps}\")\n",
    "        print(f\"Command Type Loss: {command_type_loss.item()}\\tAction1 Loss: {action1_loss.item()}\\tAction2 Loss: {action2_loss.item()}\")\n",
    "        print(f\"Current Loss: {total_loss.item()}\")\n",
    "\n",
    "winsound.PlaySound(f'D:/Python/Audio/English/chiara_craftEpic_1_en', winsound.SND_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

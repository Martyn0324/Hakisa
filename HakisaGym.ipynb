{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Alive\\lib\\site-packages\\stable_baselines\\__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('D:/Anaconda/envs/Alive/Lib/site-packages')\n",
    "import retro\n",
    "import time\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.common import set_global_seeds\n",
    "#from OpenAIWrappers\n",
    "from stable_baselines.common.callbacks import CheckpointCallback\n",
    "#from wrapper import wrapper\n",
    "import torch\n",
    "from torch import nn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"ChunLiVsBlanka.1star\", \"ChunLiVsBalrog.1star\", \"ChunLiVsBison.1star\", \"ChunLiVsChunLi.1star\", \"ChunLiVsDhalsim.1star\",\n",
    "            \"ChunLiVsGuille.1star\", \"ChunLiVsHonda.1star\", \"ChunLiVsKen.1star\", \"ChunLiVsRyu.1star\", \"ChunLiVsSagat.1star\", \"ChunLiVsVega.1star\",\n",
    "            \"ChunLiVsZahgief.1star\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\", state=\"ChunLiVsBlanka.1star\")\n",
    "#env = retro.make(game=\"DonkeyKongCountry2-Snes\")\n",
    "#env = wrapper(env)\n",
    "#model = PPO2.load(\"D:/Python/Projects/Hakisa/rl_model_1000000_steps\")\n",
    "#model = PPO2.load(\"D:/Python/Projects/Hakisa/Donkey_Kong/rl_model_1200000_steps\")\n",
    "obs = env.reset()\n",
    "total_reward = []\n",
    "steps = 0\n",
    "end = False\n",
    "\n",
    "#model = PPO2(policy=\"CnnPolicy\", env=env, gamma=0.99, n_steps=64, learning_rate=3e-9, vf_coef=0.5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "obs, reward, end, info = env.step(env.action_space.sample())\n",
    "\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiBinary(12)\n",
      "[0 1 0 0 0 0 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.action_space.sample()) # MultiBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject(nn.Module):\n",
    "\n",
    "    def __init__(self, epsilon=0.1):\n",
    "        # Problem: How to deal with determinism in a Binary Environment (Street Fighter) ?\n",
    "\n",
    "        super(Subject, self).__init__()\n",
    "\n",
    "        # Input = 3x200x256\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(16, 64, 2, 2, 0) # 100x128\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, 2, 2, 0) # 50x64\n",
    "        self.conv5 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.conv6 = nn.Conv2d(128, 256, 2, 2, 0) # 25x32\n",
    "        self.conv7 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.action = nn.Linear(256*25*32, 12)\n",
    "        self.reward = nn.Linear((256*25*32)+12+1, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, obs, previous_cumulative_reward):\n",
    "\n",
    "        x = self.conv1(obs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        actions = self.action(x)\n",
    "\n",
    "        x = torch.cat((x, actions, previous_cumulative_reward), -1)\n",
    "\n",
    "        expected_reward = self.reward(x)\n",
    "\n",
    "        actions = self.sigmoid(actions)\n",
    "\n",
    "        del x\n",
    "\n",
    "        return actions, expected_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Subject().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIALS\n",
    "\n",
    "while steps < 1000:\n",
    "    obs = torch.from_numpy(obs)\n",
    "    obs = obs/255\n",
    "    obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "    env.render()\n",
    "    action, expected_reward = model(obs)\n",
    "    action = action.detach().cpu().squeeze(0).numpy()\n",
    "\n",
    "    teste = []\n",
    "    for x in action:\n",
    "        if x > 0.5:\n",
    "            teste.append(1.)\n",
    "        else:\n",
    "            teste.append(0.)\n",
    "\n",
    "    obs, reward, end, info = env.step(teste)\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    time.sleep(0.005)\n",
    "\n",
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4976626  0.49443606 0.496801   0.49380603 0.504097   0.49821717\n",
      " 0.50026226 0.49854857 0.49533316 0.50014585 0.4992451  0.49504086]\n",
      "[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "tensor([[-0.0261]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "0.0\n",
      "False\n",
      "{'enemy_matches_won': 0, 'score': 0, 'matches_won': 0, 'continuetimer': 0, 'enemy_health': 176, 'health': 176}\n"
     ]
    }
   ],
   "source": [
    "print(action)\n",
    "print(teste)\n",
    "print(expected_reward)\n",
    "\n",
    "print(reward)\n",
    "print(end)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Alive\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:434: UserWarning: [WinError -2147417850] Não é possível alterar o modo de thread depois de o mesmo estar definido\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1.]\n",
      "None\n",
      "0.0\n",
      "False\n",
      "{'enemy_matches_won': 0, 'score': 1200, 'matches_won': 0, 'continuetimer': 0, 'enemy_health': 105, 'health': 176}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "while steps < 1000:\n",
    "    env.render()\n",
    "    #print(obs) # Obs ----> Image [0, 255] ?\n",
    "    action, state = model.predict(obs)\n",
    "    obs, reward, end, info = env.step(action)\n",
    "    steps += 1\n",
    "    #total_reward.append(reward)\n",
    "    time.sleep(0.005)\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(action)\n",
    "print(state)\n",
    "print(reward)\n",
    "print(end)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = torch.zeros((1, 1), device=device) # Cumulative reward\n",
    "advantage = []\n",
    "steps = 0\n",
    "env = retro.make(game=\"StreetFighterIISpecialChampionEdition-Genesis\", state=\"ChunLiVsBlanka.1star\")\n",
    "obs = env.reset()\n",
    "obs = torch.from_numpy(obs)\n",
    "obs = obs/255\n",
    "obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "save_point = 100 # Also optimization point\n",
    "previous_predicted_reward = None # BEWARE: this will be the basis for backpropagation (TD-Learning)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, eps=1e-8)\n",
    "# 1e-4 is a common LR. But 1e-6 is also used by RainbowDQN and it's the best one for HierNet\n",
    "# Note: PPO also uses epsilon = 1e-5, but it seems that eps=1e-7 or 1e-8 are better.\n",
    "\n",
    "old_policy = deepcopy(model) # For Surrogate Loss. Creating here to reserve some space in memory\n",
    "old_policy.eval()\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, gamma=0.1) # The learning rate should decay linearly until it vanishes\n",
    "\n",
    "value_criterion = torch.nn.MSELoss()\n",
    "\n",
    "reward_input = reward.clone() # To avoid issues with inplace operations(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 100\n",
      "Current Loss: 0.003941837698221207\n",
      "Surrogate Loss: -6.768107414245605e-05\tValue Loss: 4.5807277970766336e-09\tEntropy: 4.00951623916626\n",
      "Advantage: 6.768107414245605e-05\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Predicted Reward: -0.13536512851715088\tCurrent Reward: 0.0\n",
      "\n",
      "\n",
      "Current step: 200\n",
      "Current Loss: 0.006508069112896919\n",
      "Surrogate Loss: 0.002565478440374136\tValue Loss: 6.564538125530817e-06\tEntropy: 3.9393084049224854\n",
      "Advantage: -0.0025621354579925537\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Predicted Reward: -0.22080892324447632\tCurrent Reward: 0.0\n",
      "\n",
      "\n",
      "Current step: 300\n",
      "Current Loss: 0.0022563072852790356\n",
      "Surrogate Loss: -0.0017382482765242457\tValue Loss: 3.019752966793021e-06\tEntropy: 3.9930453300476074\n",
      "Advantage: 0.0017377287149429321\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Predicted Reward: -0.15682832896709442\tCurrent Reward: 0.0\n",
      "\n",
      "\n",
      "Current step: 400\n",
      "Current Loss: 0.004116264171898365\n",
      "Surrogate Loss: 0.0001230187772307545\tValue Loss: 1.510924718672868e-08\tEntropy: 3.9932374954223633\n",
      "Advantage: -0.00012291967868804932\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Predicted Reward: -0.15836113691329956\tCurrent Reward: 0.0\n",
      "\n",
      "\n",
      "Current step: 500\n",
      "Current Loss: 0.0041840882040560246\n",
      "Surrogate Loss: 0.00018002750584855676\tValue Loss: 3.2348545175864274e-08\tEntropy: 4.004044055938721\n",
      "Advantage: -0.00017987191677093506\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Predicted Reward: -0.1448812037706375\tCurrent Reward: 0.0\n",
      "\n",
      "\n",
      "Current step: 600\n",
      "Current Loss: 0.004182784352451563\n",
      "Surrogate Loss: 0.0001806928776204586\tValue Loss: 3.259020431300996e-08\tEntropy: 4.0020751953125\n",
      "Advantage: -0.00018052756786346436\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Predicted Reward: -0.14596439898014069\tCurrent Reward: 0.0\n",
      "\n",
      "\n",
      "Current step: 700\n",
      "Current Loss: 0.004174655303359032\n",
      "Surrogate Loss: 0.00017709098756313324\tValue Loss: 3.1301212288781244e-08\tEntropy: 3.9975481033325195\n",
      "Advantage: -0.00017692148685455322\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Predicted Reward: -0.14689040184020996\tCurrent Reward: 0.0\n",
      "\n",
      "\n",
      "Current step: 800\n",
      "Current Loss: 0.004196257796138525\n",
      "Surrogate Loss: 0.00020497999503277242\tValue Loss: 4.19375751903317e-08\tEntropy: 3.9912567138671875\n",
      "Advantage: -0.00020478665828704834\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Predicted Reward: -0.15275651216506958\tCurrent Reward: 0.0\n",
      "\n",
      "\n",
      "Current step: 900\n",
      "Current Loss: 0.004188186954706907\n",
      "Surrogate Loss: 0.0001975430059246719\tValue Loss: 3.894152911243509e-08\tEntropy: 3.990624189376831\n",
      "Advantage: -0.00019735097885131836\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Predicted Reward: -0.15188255906105042\tCurrent Reward: 0.0\n",
      "\n",
      "\n",
      "Current step: 1000\n",
      "Current Loss: 0.004169165156781673\n",
      "Surrogate Loss: 0.00018249903223477304\tValue Loss: 3.3239018648600904e-08\tEntropy: 3.986649513244629\n",
      "Advantage: -0.00018231570720672607\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Predicted Reward: -0.15374413132667542\tCurrent Reward: 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while steps < 1000:\n",
    "    env.render()\n",
    "\n",
    "    reward_input = reward.clone() # To avoid issues with inplace operations(optimizer)\n",
    "\n",
    "    if previous_predicted_reward is None:\n",
    "\n",
    "        _, previous_predicted_reward = model(obs, reward_input)\n",
    "\n",
    "    action, predicted_reward = model(obs, reward_input)\n",
    "    action = action.squeeze(0)\n",
    "\n",
    "    # For Surrogate Loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        previous_action, previous_predicted_reward = old_policy(obs, reward_input)\n",
    "        previous_action = previous_action.squeeze(0)\n",
    "\n",
    "    # MultiBinary Environment --> Only 0.0 or 1.0 accepted\n",
    "    bin = []\n",
    "    for x in action:\n",
    "        if x > 0.5:\n",
    "            bin.append(1.)\n",
    "        else:\n",
    "            bin.append(0.)\n",
    "\n",
    "    obs, scoring, end, info = env.step(bin)\n",
    "    scoring = torch.tensor(scoring, device=device)\n",
    "\n",
    "    reward += scoring\n",
    "\n",
    "    delta = scoring + (0.9995 * predicted_reward.item()) - previous_predicted_reward.item()\n",
    "    advantage.append(delta)\n",
    "\n",
    "    for t in reversed(range(len(advantage) - 1)):\n",
    "\n",
    "        advantage[t] = advantage[t] + 0.9995 * 0.9995 * advantage[t+1]\n",
    "\n",
    "    # Calculating Entropy - Used to avoid deterministic behavior and it's a possible replace/complement to epsilon-greedy.\n",
    "\n",
    "    entropy = -(action * torch.log(torch.clamp(action, 1e-10, 1.0))).sum()\n",
    "\n",
    "    # Since we're dealing with a probability distribution, using exp(log) is more mathmatically correct(and stable)\n",
    "    # In practice, we're doing a KL-Divergence. To make it less computationally expensive, applying clip.\n",
    "\n",
    "    action = torch.clamp(action, 1e-10, 1.0)\n",
    "    previous_action = torch.clamp(previous_action, 1e-10, 1.0)\n",
    "\n",
    "    ratio = torch.exp(torch.log(action) - torch.log(previous_action))\n",
    "    clipped_ratio = torch.clamp(ratio, min=0.8, max=1.2)\n",
    "\n",
    "    surrogate_loss = -torch.mean(torch.minimum(torch.mul(ratio, advantage[-1]), torch.mul(clipped_ratio, advantage[-1])))\n",
    "\n",
    "    # We can use both the actual cumulative (discounted) reward, or the predicted reward for target. Following Ruo-Ze Liu's code.\n",
    "\n",
    "    value_loss = value_criterion(previous_predicted_reward, (scoring + 0.9995 * predicted_reward))\n",
    "\n",
    "    total_loss = surrogate_loss + (value_loss * 0.5) + (entropy * 1e-3)\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Performing a single iteration in order to get previous predicted reward for next backpropagation\n",
    "\n",
    "    obs = torch.from_numpy(obs)\n",
    "    obs = obs/255\n",
    "    obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "    _, previous_predicted_reward = model(obs, reward_input)\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "    #time.sleep(0.005)\n",
    "\n",
    "    if steps % save_point == 0:\n",
    "\n",
    "        old_policy = deepcopy(model)\n",
    "        old_policy.eval()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        print(f\"Current step: {steps}\")\n",
    "        print(f\"Current Loss: {total_loss.item()}\")\n",
    "        print(f\"Surrogate Loss: {surrogate_loss.item()}\\tValue Loss: {value_loss.item()}\\tEntropy: {entropy.item()}\\nAdvantage: {advantage[-1].item()}\")\n",
    "        print(bin)\n",
    "        print(f\"Predicted Reward: {predicted_reward.item()}\\tCurrent Reward: {reward.item()}\\n\\n\")\n",
    "\n",
    "        # Performing a single iteration in order to get previous predicted reward for backpropagation\n",
    "\n",
    "        #obs = torch.from_numpy(obs)\n",
    "        #obs = obs/255\n",
    "        #obs = obs.permute(2, 1, 0).unsqueeze(0).float().to(device)\n",
    "\n",
    "        reward_input = reward.clone()\n",
    "\n",
    "        _, previous_predicted_reward = model(obs, reward_input)\n",
    "\n",
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 200])\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(obs.shape)\n",
    "print(bin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Alive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

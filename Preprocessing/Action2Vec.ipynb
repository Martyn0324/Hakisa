{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from sklearn.neighbors import NearestNeighbors as KNN\n",
    "import pytesseract\n",
    "from time import sleep\n",
    "import winsound\n",
    "from re import sub\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Screen region capture and OCR\n",
    "\n",
    "from cv2 import adaptiveThreshold, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY\n",
    "\n",
    "sleep(3)\n",
    "\n",
    "with mss() as sct:\n",
    "    monitor = {\"top\": 180, \"left\": 1, \"width\": 249-1, \"height\": 213-180} # Memory efficiency.\n",
    "    data = sct.grab(monitor)\n",
    "    data = Image.frombytes(\"RGB\", data.size, data.bgra, 'raw', 'BGRX')\n",
    "    data = data.convert(\"P\") # Thresholding only works on grayscaled images.\n",
    "    data = np.array(data)\n",
    "\n",
    "winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "plt.imshow(data)\n",
    "plt.show()\n",
    "\n",
    "consequence = pytesseract.image_to_string(data, config='--psm 6')\n",
    "print(consequence)\n",
    "\n",
    "consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence)\n",
    "\n",
    "print(consequence)\n",
    "\n",
    "datathresh = adaptiveThreshold(data,255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,63,20)\n",
    "\n",
    "consequence = pytesseract.image_to_string(datathresh, config='--psm 6')\n",
    "print(consequence)\n",
    "\n",
    "#replace w --> 1, f --> / ---> y ---> 1 ---> e --> 2\n",
    "consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence)\n",
    "\n",
    "print(consequence)\n",
    "\n",
    "plt.imshow(datathresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datathresh = adaptiveThreshold(data,255,ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY,11,5) # 11, 5\n",
    "\n",
    "consequence = pytesseract.image_to_string(datathresh, config='--psm 8')\n",
    "print(consequence)\n",
    "\n",
    "#replace w --> 1, f --> / ---> y ---> 1 ---> e --> 2\n",
    "consequence = sub('[^A-Za-z0-9\\/\\.]', '', consequence)\n",
    "\n",
    "print(consequence)\n",
    "\n",
    "plt.imshow(datathresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Generates input maps and commands for Hakisa.\n",
    "\n",
    "    Remember: command_types = list of strings, actions1 and 2 = list of strings(keyboard), X coordinates or None(mouse)\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        command_types = None,\n",
    "        actions1 = None,\n",
    "        actions2 = None,\n",
    "        top=0,\n",
    "        left=0,\n",
    "        width=1920,\n",
    "        height=1080,\n",
    "        resize=None\n",
    "    ):\n",
    "\n",
    "        # Window resolutions for the screen grabber\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.resize = resize # For reducing the images. Must be a tuple (Height, Width)\n",
    "\n",
    "        self.data = None # This will be created during training. However, it's possible to load a ready-made data for training.\n",
    "\n",
    "        # Initially, we'll be using lists. After our vector embedding has been properly trained, we'll create a dictionary\n",
    "        # of input mappings with it.\n",
    "\n",
    "        self.command_type = command_types\n",
    "        self.actions1 = actions1\n",
    "        self.actions2 = actions2\n",
    "\n",
    "        self.encoded_command_type = to_categorical(np.arange(0, len(command_types)), len(command_types))\n",
    "        self.encoded_command_type = torch.from_numpy(self.encoded_command_type)\n",
    "\n",
    "        self.key_actions1 = None # Dictionary of vectors for each action1\n",
    "        self.key_actions2 = None # Dictionary of vectors for each action2\n",
    "\n",
    "        self.knn_actions1 = None # Where we'll store our fitted KNN\n",
    "        self.knn_actions2 = None\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        frames = self.data[idx]\n",
    "        encoded_command_type = self.encoded_command_type[idx]\n",
    "        encoded_actions1 = self.encoded_actions1[idx]\n",
    "        encoded_actions2 = self.encoded_actions2[idx]\n",
    "\n",
    "        return frames, encoded_command_type, encoded_actions1, encoded_actions2\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def create_commands_dictionary(self, map2vec_model):\n",
    "\n",
    "        map2vec_model.evaluate = True\n",
    "\n",
    "        # I don't really know how we could handle vector dimensions\n",
    "\n",
    "        dictionary_actions1 = {}\n",
    "        dictionary_actions2 = {}\n",
    "\n",
    "        empty1 = torch.empty_like(self.encoded_actions1, device=device) # The vectorizer demands both actions as input, but they're vectorized independently.\n",
    "        empty2 = torch.empty_like(self.encoded_actions2, device=device)\n",
    "\n",
    "        empty_frame = torch.empty((1, 400*5*5), device=device) # The vectorizer requires a context as input, but for evaluation this isn't necessary.\n",
    "\n",
    "        empty_type = torch.empty_like(self.encoded_command_type, device=device)\n",
    "\n",
    "        for i in range(len(self.actions1)):\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                output, _ = map2vec_model(empty_frame, empty_type[0].unsqueeze(0), self.encoded_actions1[i].unsqueeze(0), empty2[0].unsqueeze(0))\n",
    "\n",
    "                output = output.view(-1)\n",
    "\n",
    "                vector = output[torch.argmax(output)].item()\n",
    "            \n",
    "            dictionary_actions1[self.actions1[i]] = vector\n",
    "\n",
    "        self.key_actions1 = dictionary_actions1\n",
    "\n",
    "        del dictionary_actions1\n",
    "\n",
    "        for i in range(len(self.actions2)):\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                _, output = map2vec_model(empty_frame, empty_type[0].unsqueeze(0), empty1[0].unsqueeze(0), self.encoded_actions2[i].unsqueeze(0))\n",
    "\n",
    "                output = output.view(-1)\n",
    "\n",
    "                vector = output[torch.argmax(output)].item()\n",
    "\n",
    "            dictionary_actions2[self.actions2[i]] = vector\n",
    "\n",
    "        self.key_actions2 = dictionary_actions2\n",
    "\n",
    "        del dictionary_actions2\n",
    "\n",
    "        print(f\"Dict input maps created successfully!\\nActions 1 dict length: {len(self.key_actions1)}\\nActions 2 dict length: {len(self.key_actions2)}\")\n",
    "\n",
    "        self.knn_actions1 = self._fit_knn(self.key_actions1)\n",
    "        self.knn_actions2 = self._fit_knn(self.key_actions2)\n",
    "\n",
    "        print(\"All action maps have been properly fitted by their respective KNN algorithm\")\n",
    "\n",
    "\n",
    "    def _fit_knn(self, dictionary):\n",
    "        \n",
    "        values = list(dictionary.values())\n",
    "\n",
    "        values = np.array(values).reshape(-1,1)\n",
    "\n",
    "        knn = KNN(n_neighbors=1, algorithm='kd_tree').fit(values)\n",
    "\n",
    "        del values\n",
    "\n",
    "        return knn\n",
    "\n",
    "\n",
    "    def record_gameplay(self, number_of_screenshots, screenshot_delay, grayscale=False, resize=False, path=None):\n",
    "\n",
    "        # Resizing and grayscaling isn't really necessary here, but can save you some time later.\n",
    "        # Both saving you from writing more code and from making your hardware having to process more and more data at once.\n",
    "\n",
    "        print(f\"Ok. Screenshot capture will begin in 5 seconds\")\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "        for i in range(number_of_screenshots):\n",
    "\n",
    "            with mss() as sct:\n",
    "\n",
    "                frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "                frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if grayscale:\n",
    "\n",
    "                frame = frame.convert('L')\n",
    "\n",
    "            if resize:\n",
    "\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame.save(f\"{path}/Screenshot_{i}.png\")\n",
    "\n",
    "            sleep(screenshot_delay)\n",
    "        \n",
    "        print(\"Screenshot capture finished!\")\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "\n",
    "    def create_data(self, data, commands):\n",
    "        '''\n",
    "        data: a tensor of size (N_Samples, Channels, Height, Width) containing the game frames. The pixels values must be within range [0., 255.].\n",
    "        commands: a list of tuples with length (N_samples), with each sample being a tuple composed of (command_type, action1, action2), where:\n",
    "\n",
    "            command_type: a tensor the action command type index-encoded with indices within range [0, len(command_types)].\n",
    "            action1: the action1 index-encoded with indices within range [0, len(actions1)].\n",
    "            action2: the action2 index-encoded with indices within range [0, len(action2)].\n",
    "        '''\n",
    "\n",
    "        # We aren't using data in time_steps mode, like we do for gifs, time series and forecasting in general.\n",
    "        # I thought it might be a good idea to also train Hakisa with that.\n",
    "        # This might also be the best way to train her in frames forecasting, as the process is probably too slow to be made while playing.\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        encoded_command_type = []\n",
    "        encoded_actions1 = []\n",
    "        encoded_actions2 = []\n",
    "\n",
    "        for sample in commands:\n",
    "\n",
    "            command_type = to_categorical(sample[0], len(self.command_type))\n",
    "            command_type = torch.from_numpy(command_type)\n",
    "            command_type = command_type.unsqueeze(0).to(device) # So you don't have to use [number] for your commands tuple to get a command_type with shape [N_samples, 1]\n",
    "            encoded_command_type.append(command_type)\n",
    "\n",
    "            encoded_action1 = to_categorical(sample[1], len(self.actions1))\n",
    "            encoded_action1 = torch.from_numpy(encoded_action1)\n",
    "            encoded_action1 = encoded_action1.unsqueeze(0).to(device)\n",
    "            encoded_actions1.append(encoded_action1)\n",
    "\n",
    "            encoded_action2 = to_categorical(sample[2], len(self.actions2))\n",
    "            encoded_action2 = torch.from_numpy(encoded_action2)\n",
    "            encoded_action2 = encoded_action2.unsqueeze(0).to(device)\n",
    "            encoded_actions2.append(encoded_action2)\n",
    "\n",
    "        encoded_command_type = torch.cat(encoded_command_type, 0)\n",
    "        encoded_actions1 = torch.cat(encoded_actions1, 0)\n",
    "        encoded_actions2 = torch.cat(encoded_actions2, 0)\n",
    "\n",
    "        self.encoded_command_type = encoded_command_type\n",
    "        self.encoded_actions1 = encoded_actions1\n",
    "        self.encoded_actions2 = encoded_actions2\n",
    "\n",
    "        print(\"All done! Train the vectorizer and then use it to generate the input mapping dictionary\")\n",
    "\n",
    "    def save_dicts(self, path, file_name):\n",
    "\n",
    "        with open(f'{path}/{file_name}_actions1.pkl', 'wb') as f:\n",
    "            pickle.dump(self.key_actions1, f)\n",
    "        f.close()\n",
    "        \n",
    "        with open(f'{path}/{file_name}_KNNactions1.pkl', 'wb') as f:\n",
    "            pickle.dump(self.knn_actions1, f)\n",
    "        f.close()\n",
    "\n",
    "        with open(f'{path}/{file_name}_actions2.pkl', 'wb') as f:\n",
    "            pickle.dump(self.key_actions2, f)\n",
    "        f.close()\n",
    "\n",
    "        with open(f'{path}/{file_name}_KNNactions2.pkl', 'wb') as f:\n",
    "            pickle.dump(self.knn_actions2, f)\n",
    "        f.close()\n",
    "\n",
    "        print(f\"Dictionaries saved at {path}/{file_name} with their respectives KNNs.\\nRemember: Each dict must be correctly fit with its respective KNN algorithm.\")\n",
    "        print(f\"When using Hakisa, simply use the dataset function '.load_dicts(path, file_name)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jigoku Kisetsukan\n",
    "\n",
    "command_type = ['key']\n",
    "\n",
    "actions1 = ['Down', 'Up']\n",
    "\n",
    "actions2 = ['up', 'down', 'left', 'right', 'z', 'x', 'shift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bullet Heaven\n",
    "\n",
    "command_types = ['move', 'click', 'rightclick']\n",
    "\n",
    "actions1 = [i for i in range(1, 1919)] # Avoiding using the extremes so we don't have to shut down PyAutoGUI safety lock.\n",
    "\n",
    "actions2 = [i for i in range(1, 1079)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(command_types=command_type, actions1=actions1, actions2=actions2, resize=(200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.record_gameplay(2000, 1, grayscale=False, resize=False, path=\"Hakisa/JK_gameplay/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "images_path = []\n",
    "\n",
    "for directory, _, files in os.walk(\"Hakisa/JK_gameplay/\"):\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        images_path.append(directory+\"/\"+file)\n",
    "\n",
    "# Problem: for strings, Python considers that 1000 < 2. Maybe something related to how the string is assembled? I don't know how to fix this yet.\n",
    "\n",
    "images_data = []\n",
    "\n",
    "for i in images_path[0:10]:\n",
    "\n",
    "    image = Image.open(i)\n",
    "    image = image.resize((200, 200))\n",
    "    array = np.array(image, dtype=np.float32)\n",
    "    image.close()\n",
    "    images_data.append(array)\n",
    "\n",
    "images_data = np.stack(images_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the boring part: visualizing each screenshot and labeling them\n",
    "\n",
    "image = Image.open(images_path[9]) # Remember that [0] = screenshot 0, but [1] = screenshot 10, due to Python considering 10 < 2 and 100000 < 2 and so on...\n",
    "image.show()\n",
    "image.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = [(0, 0, 3), (0, 0, 4), (0, 0, 6), (0, 0, 2), (0, 0, 0), (0, 0, 0), (0, 1, 0), (0, 0, 2), (0, 0, 6), (0, 0, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_data = torch.from_numpy(images_data)\n",
    "images_data = images_data.view(images_data.size(0), images_data.size(3), images_data.size(1), images_data.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done! Train the vectorizer and then use it to generate the input mapping dictionary\n"
     ]
    }
   ],
   "source": [
    "dataset.create_data(images_data, commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 7])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.encoded_command_type.size())\n",
    "print(dataset.encoded_actions1.size())\n",
    "print(dataset.encoded_actions2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action2Vec(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    The Vectorizer model will assign vectors to each action1 and each action2 according to its context.\n",
    "    In NLP, the context is determined by the position of certain word according to other words.\n",
    "\n",
    "    For us, we could determine the context according to the game state(the frame) and the command used in that state.\n",
    "\n",
    "    But it might be interesting to use other metrics for context, such as HP, MP, Power, Aura, Score...\n",
    "\n",
    "    In order to correctly get the context, we'll be using feature extraction with Conv2Ds on the frames.\n",
    "    This context(or the features extracted from the frames) is gonna be used to condition the action vector.\n",
    "\n",
    "\n",
    "    Game Frame ------> Feature Extraction (Conv2D + MaxPool) ----> Context\n",
    "    O-H action ------> FCC layer --------------------------------> some output?\n",
    "\n",
    "    concatenation(Context, some output) ---> FCC layer ----------> Vector\n",
    "    '''\n",
    "\n",
    "    def __init__(self, command_type, actions1, actions2, evaluate=False):\n",
    "\n",
    "        super(Action2Vec, self).__init__()\n",
    "\n",
    "        self.command_type = len(command_type) # For initialization, the length is what matters.\n",
    "        self.actions1 = len(actions1)\n",
    "        self.actions2 = len(actions2)\n",
    "\n",
    "        self.evaluate = evaluate\n",
    "\n",
    "        # Considering a frame size 200x200x3\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 100, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(100)\n",
    "        self.conv2 = torch.nn.Conv2d(100, 200, kernel_size=3, stride=1, padding=1, bias=False) # 200x200\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(200)\n",
    "        self.pool2x2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # 100x100\n",
    "        self.conv3 = torch.nn.Conv2d(200, 400, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm3 = torch.nn.BatchNorm2d(400)\n",
    "        self.conv4 = torch.nn.Conv2d(400, 600, kernel_size=3, stride=1, padding=1, bias=False) # 100x100\n",
    "        self.batchnorm4 = torch.nn.BatchNorm2d(600)\n",
    "        # Add pool 2x2 ---> 50x50\n",
    "        self.conv5 = torch.nn.Conv2d(600, 800, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm5 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv6 = torch.nn.Conv2d(800, 1000, kernel_size=3, stride=1, padding=1, bias=False) # 50x50\n",
    "        self.batchnorm6 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 25x25\n",
    "        self.conv7 = torch.nn.Conv2d(1000, 1200, kernel_size=4, stride=1, bias=False) # 22x22\n",
    "        self.batchnorm7 = torch.nn.BatchNorm2d(1200)\n",
    "        self.conv8 = torch.nn.Conv2d(1200, 1000, kernel_size=3, stride=1, bias=False) # 20x20\n",
    "        self.batchnorm8 = torch.nn.BatchNorm2d(1000)\n",
    "        # Add pool 2x2 ---> 10x10\n",
    "        self.conv9 = torch.nn.Conv2d(1000, 800, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm9 = torch.nn.BatchNorm2d(800)\n",
    "        self.conv10 = torch.nn.Conv2d(800, 400, kernel_size=3, stride=1, padding=1, bias=False) # 10x10\n",
    "        self.batchnorm10 = torch.nn.BatchNorm2d(400)\n",
    "        # Add pool 2x2 ---> 5x5\n",
    "        self.neuron_frames = torch.nn.Linear(400*5*5, 200*2*2, bias=False)\n",
    "\n",
    "        self.neuron_command_type1 = torch.nn.Linear(self.command_type, 200*2*2, bias=False) # The command type will be used to condition the actions\n",
    "        self.neuron_actions1A = torch.nn.Linear(self.actions1, 200*2*2, bias=False)\n",
    "        self.neuron_actions2A = torch.nn.Linear(self.actions2, 200*2*2, bias=False)\n",
    "\n",
    "        #self.neuron_actions1B = torch.nn.Linear(200*2*6, self.actions1, bias=False)\n",
    "        #self.neuron_actions2B = torch.nn.Linear(200*2*6, self.actions2, bias=False)\n",
    "\n",
    "        self.neuron_actions1B = torch.nn.Linear(200*2*6, self.actions1, bias=False)\n",
    "        self.neuron_actions2B = torch.nn.Linear(200*2*6, self.actions2, bias=False)\n",
    "\n",
    "        self.layer_normA = torch.nn.LayerNorm(200*2*6)\n",
    "        self.layer_normB = torch.nn.LayerNorm(200*2*6)\n",
    "\n",
    "        self.leakyrelu = torch.nn.LeakyReLU(0.25)\n",
    "        #self.softmax = torch.nn.LogSoftmax(-1) # Won't be used ----> Already included in Pytorch's Cross Entropy Loss\n",
    "\n",
    "    def forward(self, game_frame, encoded_command_type, encoded_action1, encoded_action2):\n",
    "\n",
    "        if self.evaluate == False:\n",
    "\n",
    "            x = self.conv1(game_frame)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm2(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv3(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm3(x)\n",
    "            x = self.conv4(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm4(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv5(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm5(x)\n",
    "            x = self.conv6(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm6(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv7(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm7(x)\n",
    "            x = self.conv8(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm8(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = self.conv9(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm9(x)\n",
    "            x = self.conv10(x)\n",
    "            x = self.leakyrelu(x)\n",
    "            x = self.batchnorm10(x)\n",
    "\n",
    "            x = self.pool2x2(x)\n",
    "\n",
    "            x = x.view(x.size(0), -1)\n",
    "\n",
    "            context = self.neuron_frames(x) # (Batch, 200*2*2)\n",
    "\n",
    "            encoded_command_type = self.neuron_command_type1(encoded_command_type) # (Batch, 200*2*2)\n",
    "\n",
    "            context = torch.cat((context, encoded_command_type), -1) # (Batch, 200*2*4)\n",
    "\n",
    "            x = self.neuron_actions1A(encoded_action1)\n",
    "            \n",
    "            x = torch.cat((context, x), -1) # (Batch, 200*2*6)\n",
    "\n",
    "            x = self.layer_normA(x)\n",
    "\n",
    "            output1 = self.neuron_actions1B(x)\n",
    "\n",
    "            x = self.neuron_actions2A(encoded_action2)\n",
    "\n",
    "            x = torch.cat((context, x), -1)\n",
    "\n",
    "            x = self.layer_normB(x)\n",
    "\n",
    "            output2 = self.neuron_actions2B(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "            return output1, output2\n",
    "        \n",
    "        else:\n",
    "\n",
    "            context = self.neuron_frames(game_frame) # (Batch, 200*2*2)\n",
    "\n",
    "            encoded_command_type = self.neuron_command_type1(encoded_command_type) # (Batch, 200*2*2)\n",
    "\n",
    "            context = torch.cat((context, encoded_command_type), -1) # (Batch, 200*2*4)\n",
    "\n",
    "            x = self.neuron_actions1A(encoded_action1)\n",
    "            \n",
    "            x = torch.cat((context, x), -1) # (Batch, 200*2*6)\n",
    "\n",
    "            x = self.layer_normA(x)\n",
    "\n",
    "            output1 = self.neuron_actions1B(x)\n",
    "\n",
    "            x = self.neuron_actions2A(encoded_action2)\n",
    "\n",
    "            x = torch.cat((context, x), -1)\n",
    "\n",
    "            x = self.layer_normB(x)\n",
    "\n",
    "            output2 = self.neuron_actions2B(x)\n",
    "\n",
    "            del x\n",
    "\n",
    "            return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "action2vec_model = Action2Vec(command_type, actions1, actions2, evaluate=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(action2vec_model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, gamma=0.1)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "grads = []\n",
    "\n",
    "#epochs = 10000\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100\tCurrent Loss: 2.683117628097534\tCurrent Learning Rate: 1e-05\n",
      "Gradients Average: -7.03003752278164e-05\n",
      "10/100\tCurrent Loss: 0.0019489850383251905\tCurrent Learning Rate: 1e-05\n",
      "Gradients Average: 1.1707386420312105e-06\n",
      "20/100\tCurrent Loss: 0.005426548887044191\tCurrent Learning Rate: 1e-05\n",
      "Gradients Average: 8.033214271563338e-07\n",
      "30/100\tCurrent Loss: 0.0031433142721652985\tCurrent Learning Rate: 1e-05\n",
      "Gradients Average: -2.583366551789368e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22940/1211286482.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0maction2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mencoded_command_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_command_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mencoded_actions1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_actions1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (frames, encoded_command_type, encoded_actions1, encoded_actions2) in enumerate(dataloader):\n",
    "        action2vec_model.zero_grad()\n",
    "\n",
    "        frames = frames.to(device)\n",
    "        encoded_command_type = encoded_command_type.to(device)\n",
    "        encoded_actions1 = encoded_actions1.to(device)\n",
    "        encoded_actions2 = encoded_actions2.to(device)\n",
    "\n",
    "        output1, output2 = action2vec_model(frames, encoded_command_type, encoded_actions1, encoded_actions2)\n",
    "\n",
    "        cost1 = loss(output1, encoded_actions1)\n",
    "\n",
    "        cost2 = loss(output2, encoded_actions2)\n",
    "\n",
    "        cost = cost1 + cost2\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        for n, p in action2vec_model.named_parameters():\n",
    "            if 'neuron_frames.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"{epoch}/{epochs}\\tCurrent Loss: {cost.item()}\\tCurrent Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "        print(f\"Gradients Average: {grads[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [1., 0.]], device='cuda:0')\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(encoded_actions1)\n",
    "print(encoded_actions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.1970, -5.3581],\n",
      "        [ 3.1824, -4.5489]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "tensor([[ 6.5323, -2.0989, -0.6335, -0.4155, -1.5959, -2.7090, -1.9323],\n",
      "        [-2.0638, -2.1036, -0.8067, -0.2807, -0.6650, -3.2912,  7.3846]],\n",
      "       device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output1)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict input maps created successfully!\n",
      "Actions 1 dict length: 2\n",
      "Actions 2 dict length: 7\n",
      "All action maps have been properly fitted by their respective KNN algorithm\n"
     ]
    }
   ],
   "source": [
    "dataset.create_commands_dictionary(action2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': 2.2056455612182617, 'Up': 2.2056455612182617}\n",
      "{'up': 1.4485101699829102, 'down': 1.355112075805664, 'left': 1.3761229515075684, 'right': 1.4581246376037598, 'z': 1.3342045545578003, 'x': 1.3342045545578003, 'shift': 1.3342045545578003}\n"
     ]
    }
   ],
   "source": [
    "print(dataset.key_actions1)\n",
    "print(dataset.key_actions2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from PIL import Image\n",
    "#import pyautogui\n",
    "from time import sleep\n",
    "import winsound\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Generates commands for Reader, a multi-label classifier\n",
    "    (do not confuse with multi-class classification)\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data=None,\n",
    "        labels=None,\n",
    "        top=0,\n",
    "        left=0,\n",
    "        width=1920,\n",
    "        height=1080,\n",
    "        resize=None\n",
    "    ):\n",
    "\n",
    "        # Window resolutions for the screen grabber\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.resize = resize # For reducing the images. Must be a tuple (Height, Width)\n",
    "\n",
    "        self.data = data # The inputs\n",
    "\n",
    "        self.labels = labels # The list of possible outputs, where each item is a character\n",
    "\n",
    "\n",
    "    # Pytorch's Dataset functions will only be used in Studying mode\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        inputs = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def record_screen_region(self, number_of_screenshots, screenshot_delay, grayscale=False, resize=False, path=None):\n",
    "        '''\n",
    "        To capture screen regions, which will serve as inputs to the model\n",
    "        '''\n",
    "\n",
    "        print(f\"Ok. Screenshot capture will begin in 5 seconds\")\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "        for i in range(number_of_screenshots):\n",
    "\n",
    "            with mss() as sct:\n",
    "\n",
    "                frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "                frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if grayscale:\n",
    "\n",
    "                frame = frame.convert('L')\n",
    "\n",
    "            if resize:\n",
    "\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame.save(f\"{path}/{i}.png\")\n",
    "\n",
    "            sleep(screenshot_delay)\n",
    "        \n",
    "        print(\"Screenshot capture finished!\")\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [str(x) for x in range(501)]\n",
    "characters = characters + ['LIFE', 'SCORE', 'AURA', 'BOMBS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    (0., 0., 0., 0.), (1., 0., 0., 0.), (1., 0., 0., 2.), (1., 0., 0., 4.), (1., 0., 0., 8.),\n",
    "    (1., 0., 0., 9.), (1., 1., 0., 10.), (2., 2., 0., 12.), (2., 2., 0., 23.), (3., 6., 0., 23.),\n",
    "    (3., 10., 0., 23.), (3., 11., 0., 23.), (3., 12., 0., 23.), (3., 13., 0., 23.), (3., 14., 0., 23.),\n",
    "    (3., 14., 0., 23.), (3., 16., 0., 23.), (3., 17., 0., 23.), (3., 17., 0., 23.), (3., 18., 0., 23.),\n",
    "    (3., 19., 0., 23.), (3., 19., 0., 23.), (3., 20., 0., 23.), (3., 21., 0., 23.), (3., 21., 0., 23.),\n",
    "    (3., 22., 0., 23.), (3., 22., 0., 23.), (3., 22., 0., 23.), (3., 22., 0., 23.), (3., 22., 0., 23.),\n",
    "    (3., 22., 0., 23.), (3., 22., 0., 23.), (3., 22., 0., 23.), (3., 22., 0., 24.), (4., 22., 0., 27.),\n",
    "    (4., 22., 0., 27.), (4., 22., 0., 34.), (4., 22., 0., 53.), (4., 22., 0., 56.), (5., 22., 0., 61.),\n",
    "    (5., 22., 0., 65.), (5., 22., 0., 71.), (5., 22., 0., 76.), (6., 22., 0., 100.), (6., 22., 0., 115.),\n",
    "    (6., 22., 0., 132.), (6., 22., 0., 148.), (6., 22., 0., 151.), (7., 23., 0., 172.), (7., 23., 0., 182.),\n",
    "    (7., 23., 0., 190.), (8., 23., 0., 204.), (9., 23., 0., 221.), (10., 23., 0., 233.), (12., 23., 0., 234.)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(data=None, labels=labels, top=1, left=1632, width=1823-1632, height=30-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.record_screen_region(2000, 1, path='D:/Python/Projects/Hakisa/Preprocessing/Reader_BH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "images_by_order = []\n",
    "\n",
    "for directory, _, files in os.walk(\"D:/Python/Projects/Hakisa/Preprocessing/Reader_BH\"):\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        file = file.split('.')\n",
    "        file = file[0] # Getting exclusively the number\n",
    "\n",
    "        images_by_order.append(file)\n",
    "\n",
    "images_by_order = sorted([int(x) for x in images_by_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since many images are equal to each other, we'll select a few.\n",
    "\n",
    "images = [\n",
    "    0, 4, 7, 10, 18, 19, 32, 56, 75, 95, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n",
    "    121, 122, 123, 124, 125, 126, 127, 128, 129, 142, 159, 164, 173, 174, 184, 187, 217, 237, 242, 249, 252, 267, 295, 304, 334,\n",
    "    374, 396, 408\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [i for i in images_by_order if i in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_data = []\n",
    "\n",
    "for i in images:\n",
    "\n",
    "    i = directory + '/' + str(i) + '.png'\n",
    "    image = Image.open(i)\n",
    "    array = np.array(image, dtype=np.float32)\n",
    "    image.close()\n",
    "    array = array/255 # Note that the data must be within [0, 1] for matplotlib.\n",
    "    images_data.append(array)\n",
    "\n",
    "images_data = np.stack(images_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_data = torch.from_numpy(images_data)\n",
    "images_data = images_data.view(images_data.size(0), images_data.size(3), images_data.size(1), images_data.size(2))\n",
    "print(images_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data = images_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCR(torch.nn.Module):\n",
    "    '''\n",
    "    Simple model for Optical Character Recognition\n",
    "\n",
    "    Use it on your desired game specifically to help with the reward function.\n",
    "    Be careful with overfitting.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_channels, height, width, n_characters):\n",
    "\n",
    "        super(OCR, self).__init__()\n",
    "\n",
    "        # Input size: 3x191x29 = 16,617\n",
    "\n",
    "        self.neuron1 = torch.nn.Linear(n_channels*height*width, 1000)\n",
    "        self.neuron2 = torch.nn.Linear(1000, 1000)\n",
    "        self.neuron3 = torch.nn.Linear(1000, 1000)\n",
    "\n",
    "        # Remove or add neurons according to how many strings you wish to extract\n",
    "\n",
    "        self.neuron_stringA = torch.nn.Linear(1000, n_characters)\n",
    "        self.neuron_stringB = torch.nn.Linear(1000, n_characters)\n",
    "        self.neuron_stringC = torch.nn.Linear(1000, n_characters)\n",
    "        self.neuron_stringD = torch.nn.Linear(1000, n_characters)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.PRelu = torch.nn.PReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        input = input.view(input.size(0), -1)\n",
    "\n",
    "        x = self.neuron1(input)\n",
    "        x = self.dropout(x)\n",
    "        x = self.PRelu(x)\n",
    "\n",
    "        x = self.neuron2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.PRelu(x)\n",
    "\n",
    "        x = self.neuron3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.PRelu(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        stringA = self.neuron_stringA(x)\n",
    "        stringB = self.neuron_stringB(x)\n",
    "        stringC = self.neuron_stringC(x)\n",
    "        stringD = self.neuron_stringD(x)\n",
    "\n",
    "        output = (stringA, stringB, stringC, stringD)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = OCR(3, 191, 29, len(characters)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(reader, (3, 191, 29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(reader.parameters(), lr=1e-4, eps=1e-8)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS = 10 # 100 epochs at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    for i, (frame, labels) in enumerate(dataloader):\n",
    "\n",
    "        reader.zero_grad()\n",
    "\n",
    "        frame = frame.to(device).double()\n",
    "\n",
    "        output = reader(frame)\n",
    "\n",
    "        kills_loss = criterion(output[0], labels[0].to(device).long())\n",
    "        deaths_loss = criterion(output[1], labels[1].to(device).long())\n",
    "        assists_loss = criterion(output[2], labels[2].to(device).long())\n",
    "        farm_loss = criterion(output[3], labels[3].to(device).long())\n",
    "\n",
    "        loss = kills_loss + deaths_loss + assists_loss + farm_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        grads = torch.mean(reader.neuron1.weight.grad)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % (EPOCHS*0.1) == 0:\n",
    "\n",
    "        print(f\"{epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Current Loss: {epoch_loss}\\tGradients Average: {grads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = frame.view(frame.size(0), frame.size(2), frame.size(3), frame.size(1))\n",
    "teste = teste[0].cpu().numpy()\n",
    "\n",
    "plt.imshow(teste)\n",
    "plt.show()\n",
    "\n",
    "print(output[0][0].argmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

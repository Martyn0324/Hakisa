{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "import winsound\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Generates input maps and commands for Hakisa.\n",
    "\n",
    "    Remember: command_types = list of strings, actions1 and 2 = list of strings(keyboard), X coordinates or None(mouse)\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        command_types = None,\n",
    "        actions1 = None,\n",
    "        actions2 = None,\n",
    "        top=0,\n",
    "        left=0,\n",
    "        width=1920,\n",
    "        height=1080,\n",
    "        resize=None\n",
    "    ):\n",
    "\n",
    "        # Window resolutions for the screen grabber\n",
    "        self.top = top\n",
    "        self.left = left\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.resize = resize # For reducing the images. Must be a tuple (Height, Width)\n",
    "\n",
    "        self.data = None # This will be created during training. However, it's possible to load a ready-made data for training.\n",
    "\n",
    "        self.command_type = command_types\n",
    "        self.actions1 = actions1\n",
    "        self.actions2 = actions2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        frames = self.data[idx]\n",
    "        encoded_command_type = self.encoded_command_type[idx]\n",
    "        encoded_actions1 = self.encoded_actions1[idx]\n",
    "        encoded_actions2 = self.encoded_actions2[idx]\n",
    "\n",
    "        return frames, encoded_command_type, encoded_actions1, encoded_actions2\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def record_gameplay(self, number_of_screenshots, screenshot_delay, grayscale=False, resize=False, path=None):\n",
    "\n",
    "        # Resizing and grayscaling isn't really necessary here, but can save you some time later.\n",
    "        # Both saving you from writing more code and from making your hardware having to process more and more data at once.\n",
    "\n",
    "        print(f\"Ok. Screenshot capture will begin in 5 seconds\")\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME) # Just to know if everything's ok\n",
    "\n",
    "        for i in range(number_of_screenshots):\n",
    "\n",
    "            with mss() as sct:\n",
    "\n",
    "                frame = sct.grab(monitor={\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height})\n",
    "                frame = Image.frombytes(\"RGB\", frame.size, frame.bgra, 'raw', 'BGRX')\n",
    "\n",
    "            if grayscale:\n",
    "\n",
    "                frame = frame.convert('L')\n",
    "\n",
    "            if resize:\n",
    "\n",
    "                frame = frame.resize(self.resize)\n",
    "\n",
    "            frame.save(f\"{path}/{i}.png\")\n",
    "\n",
    "            sleep(screenshot_delay)\n",
    "        \n",
    "        print(\"Screenshot capture finished!\")\n",
    "\n",
    "        winsound.PlaySound('D:/Python/Audio/English/chiara_hacking_1_en.wav', winsound.SND_FILENAME)\n",
    "\n",
    "    def create_data(self, data, commands):\n",
    "        '''\n",
    "        data: a tensor of size (N_Samples, Channels, Height, Width) containing the game frames. The pixels values must be within range [0., 255.].\n",
    "        commands: a list of tuples with length (N_samples), with each sample being a tuple composed of (command_type, action1, action2), where:\n",
    "\n",
    "            command_type: a tensor the action command type index-encoded with indices within range [0, len(command_types)].\n",
    "            action1: the action1 index-encoded with indices within range [0, len(actions1)].\n",
    "            action2: the action2 index-encoded with indices within range [0, len(action2)].\n",
    "        '''\n",
    "\n",
    "        # We aren't using data in time_steps mode, like we do for gifs, time series and forecasting in general.\n",
    "        # I thought it might be a good idea to also train Hakisa with that.\n",
    "        # This might also be the best way to train her in frames forecasting, as the process is probably too slow to be made while playing.\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        encoded_command_type = []\n",
    "        encoded_actions1 = []\n",
    "        encoded_actions2 = []\n",
    "\n",
    "        for sample in commands:\n",
    "\n",
    "            command_type = to_categorical(sample[0], len(self.command_type))\n",
    "            command_type = torch.from_numpy(command_type)\n",
    "            command_type = command_type.unsqueeze(0).to(device) # So you don't have to use [number] for your commands tuple to get a command_type with shape [N_samples, 1]\n",
    "            encoded_command_type.append(command_type)\n",
    "\n",
    "            encoded_action1 = to_categorical(sample[1], len(self.actions1))\n",
    "            encoded_action1 = torch.from_numpy(encoded_action1)\n",
    "            encoded_action1 = encoded_action1.unsqueeze(0).to(device)\n",
    "            encoded_actions1.append(encoded_action1)\n",
    "\n",
    "            encoded_action2 = to_categorical(sample[2], len(self.actions2))\n",
    "            encoded_action2 = torch.from_numpy(encoded_action2)\n",
    "            encoded_action2 = encoded_action2.unsqueeze(0).to(device)\n",
    "            encoded_actions2.append(encoded_action2)\n",
    "\n",
    "        encoded_command_type = torch.cat(encoded_command_type, 0)\n",
    "        encoded_actions1 = torch.cat(encoded_actions1, 0)\n",
    "        encoded_actions2 = torch.cat(encoded_actions2, 0)\n",
    "\n",
    "        self.encoded_command_type = encoded_command_type\n",
    "        self.encoded_actions1 = encoded_actions1\n",
    "        self.encoded_actions2 = encoded_actions2\n",
    "\n",
    "        print(\"All done! Train the vectorizer and then use it to generate the input mapping dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jigoku Kisetsukan\n",
    "\n",
    "command_types = ['key']\n",
    "\n",
    "actions1 = ['Down', 'Up']\n",
    "\n",
    "actions2 = ['up', 'down', 'left', 'right', 'z', 'x', 'shift']\n",
    "\n",
    "dataset = Dataset(command_types, actions1, actions2, resize=(200,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = [\n",
    "    (0, 0, 4), (0, 0, 6), (0, 0, 2), (0, 1, 2), (0, 0, 3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "images_by_order = []\n",
    "\n",
    "for directory, _, files in os.walk(\"D:/Python/Projects/Hakisa/Hakisa/JK_gameplay\"):\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        file = file.split('.')\n",
    "        file = file[0] # Getting exclusively the number\n",
    "\n",
    "        images_by_order.append(file)\n",
    "\n",
    "images_by_order = sorted([int(x) for x in images_by_order])\n",
    "\n",
    "# Problem: for strings, Python considers that 1000 < 2. Maybe something related to how the string is assembled?\n",
    "\n",
    "images_data = []\n",
    "\n",
    "for i in images_by_order:\n",
    "\n",
    "    i = directory + '/' + str(i) + '.png'\n",
    "    image = Image.open(i)\n",
    "    image = image.resize((200, 200))\n",
    "    array = np.array(image, dtype=np.float32)\n",
    "    image.close()\n",
    "    array = array/255\n",
    "    images_data.append(array)\n",
    "\n",
    "images_data = np.stack(images_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data\n",
    "\n",
    "with open(\"D:/Python/Projects/Hakisa/Preprocessing/JK_commands_05000.pkl\", 'wb') as f:\n",
    "    pickle.dump(commands, f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "with open(\"D:/Python/Projects/Hakisa/Preprocessing/JK_screenshots_05000.pkl\", 'wb') as f:\n",
    "    pickle.dump(images_data, f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data - If you bump into some problems with this, try loading it into chunks of data\n",
    "\n",
    "with open(\"D:/Python/Projects/Hakisa/Preprocessing/JK_commands_05000.pkl\", 'rb') as f:\n",
    "    commands = pickle.load(f)\n",
    "\n",
    "f.close()\n",
    "\n",
    "with open(\"D:/Python/Projects/Hakisa/Preprocessing/JK_screenshots_05000.pkl\", 'rb') as f:\n",
    "    images_data = pickle.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_data = torch.from_numpy(images_data)\n",
    "images_data = images_data.view(images_data.size(0), images_data.size(3), images_data.size(1), images_data.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.create_data(images_data[0:5000], commands)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Architecture\n",
    "\n",
    "Instead of using the classic Feature Extractor architecture from VGG19, we'll be using Attention Layers in order to assign weights to each feature.\n",
    "\n",
    "This way, most relevant features should get higher weights, thus, will be remarked through Convolution and Linear Layers\n",
    "\n",
    "Since we'll be using big amounts of data (200x200x3 = 120,000 data points), we'll avoid using linear layers and matrix multiplications, resorting to arrays multiplications,\n",
    "due to those being element-wise operations.\n",
    "\n",
    "For the output, we can simply use Linear Layers. Conv2Ds can be used between Attention Layers and the Output layers in order to filter the amount of data.\n",
    "\n",
    " Input Image -------------> MultiHead Attention ---------> Image with weighted features ------------> (Conv2D -----> MultiHead Attention ----->) Linear Layers ------> Output\n",
    "\n",
    "**Follow the Transformer Encoder pattern.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    Attention Layer might be useful to detect most relevant features in the images.\n",
    "\n",
    "    Adapted in order to be used with element-wise operations directly to images and feature maps.\n",
    "\n",
    "    There's no pad masks and, instead of sequence of vectors, we'll be dealing with entires feature maps.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, batch_size, input_channels, input_height, input_width):\n",
    "\n",
    "        super(HeadAttention, self).__init__()\n",
    "\n",
    "        # Creating array of weights for element-wise operations\n",
    "        self.queries_weights = torch.randn((batch_size, input_channels, input_height, input_width), device=device, requires_grad=True)\n",
    "        self.keys_weights = torch.randn((batch_size, input_channels, input_height, input_width), device=device, requires_grad=True)\n",
    "        self.values_weights = torch.randn((batch_size, input_channels, input_height, input_width), device=device, requires_grad=True)\n",
    "\n",
    "        self.batchnorm = torch.nn.BatchNorm2d(input_channels) # To compensate the sqrt(d_key) scaling factor\n",
    "\n",
    "        self.softmax = torch.nn.Softmax2d() # Computes softmax over each channel\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        batch_size = input.size(0) # (Batch, Sequences, d_model)\n",
    "\n",
    "        queries = input * self.queries_weights # (Batch, channels, height, width)\n",
    "        keys = input * self.keys_weights\n",
    "        values = input * self.values_weights\n",
    "\n",
    "        similarity_matrix = queries * keys # (Batch, channels, height, width)\n",
    "\n",
    "        similarity_matrix = self.batchnorm(similarity_matrix)\n",
    "\n",
    "        attention_weights = self.softmax(similarity_matrix) # (Batch, channels, height, width)\n",
    "\n",
    "        attention_output = values * attention_weights # (Batch, channels, height, width)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(torch.nn.Module):\n",
    "    '''\n",
    "    Uses the MultiHead Attention to extract the most relevant features\n",
    "    through element-wise operations.\n",
    "\n",
    "    It's similar to the Transformer Encoder, but discards the necessity of PositionWise FeedForward layers, since those were\n",
    "    used to compensate the lack of information about positions in the input.\n",
    "    Since we're dealing with feature maps, we already have that information.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_heads, batch_size, n_channels, height, width):\n",
    "\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.attention_heads = torch.nn.ModuleList([HeadAttention(batch_size, n_channels, height, width) for i in range(n_heads)]) # Extract most relevant features\n",
    "\n",
    "        self.conv = torch.nn.Conv2d(n_channels*n_heads, n_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm = torch.nn.BatchNorm2d(n_channels)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.Relu = torch.nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        residual_block1 = input # (Batch, n_channels, Height, Width)\n",
    "\n",
    "        attention_output = []\n",
    "\n",
    "        for head in range(self.n_heads):\n",
    "\n",
    "            x = self.attention_heads[head](input)\n",
    "\n",
    "            attention_output.append(x)\n",
    "\n",
    "            del x\n",
    "        \n",
    "        attention_output = torch.cat(attention_output, 1) # (Batch, n_channels*n_heads, Height, Width)\n",
    "\n",
    "        attention_output = self.conv(attention_output) # (Batch, n_channels, Height, Width)\n",
    "\n",
    "        del input\n",
    "\n",
    "        attention_output = residual_block1 + attention_output # (Batch, 3, Height, Width)\n",
    "\n",
    "        attention_output = self.dropout(attention_output)\n",
    "\n",
    "        attention_output = self.batchnorm(attention_output)\n",
    "\n",
    "        attention_output = self.Relu(attention_output)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coach(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    Self-Learning Model that uses modified MultiHead Attention Layers in order to extract most relevant features\n",
    "    and returns pseudolabels, which should be the best command choices to be made in the state given by the input.\n",
    "\n",
    "    Warning: It appears that using a batch higher than 1 tends to make all outputs conditioned by the first batch.\n",
    "    I'm not sure, though. It's just my impression.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, command_types, actions1, actions2, n_heads, batch_size):\n",
    "\n",
    "        super(Coach, self).__init__()\n",
    "\n",
    "        self.command_types = len(command_types) # For initialization, the length is what matters.\n",
    "        self.actions1 = len(actions1)\n",
    "        self.actions2 = len(actions2)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Considering a frame size 200x200x3\n",
    "\n",
    "        self.feature_extractor1 = FeatureExtractor(n_heads, batch_size, 3, 200, 200)\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(3, 100, kernel_size=2, stride=2, bias=False) # 100x100\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(100)\n",
    "\n",
    "        self.feature_extractor2 = FeatureExtractor(n_heads, batch_size, 100, 100, 100)\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(100, 3, kernel_size=2, stride=2, bias=False) # 50x50\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(3)\n",
    "\n",
    "        self.feature_extractor3 = FeatureExtractor(n_heads, batch_size, 3, 50, 50)\n",
    "\n",
    "        self.neuron_type = torch.nn.Linear(3*50*50, self.command_types, bias=True)\n",
    "        self.neuron_action1 = torch.nn.Linear(3*50*50, self.actions1, bias=True)\n",
    "        self.neuron_action2 = torch.nn.Linear(3*50*50, self.actions2, bias=True)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.3) # Adds randomness and makes the classification task more robust. Essential for Self-Learning\n",
    "        self.PRelu = torch.nn.PReLU()\n",
    "        self.softmax = torch.nn.LogSoftmax(-1)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.feature_extractor1(input) # (Batch, 3, 200, 200)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.PRelu(x)\n",
    "\n",
    "        x = self.feature_extractor2(x) # (Batch, 100, 100, 100)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.PRelu(x)\n",
    "\n",
    "        x = self.feature_extractor3(x) # (Batch, 3, 50, 50)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        command_type = self.neuron_type(x) # (Batch, n_command_types)\n",
    "        command_type = self.softmax(command_type)\n",
    "        action1 = self.neuron_action1(x)\n",
    "        action1 = self.softmax(action1)\n",
    "        action2 = self.neuron_action2(x)\n",
    "        action2 = self.softmax(action2)\n",
    "\n",
    "        del x\n",
    "\n",
    "        return (command_type, action1, action2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coach = Coach(command_types=command_types, actions1=actions1, actions2=actions2, n_heads=4, batch_size=4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch Summary uses a batch_size = 2. So this won't work if you've initialized your model with batch_size != 2. Use 2 just to check how things are going here.\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(coach, (3, 200, 200))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixing Self-Supervised training and Supervised fine-tuning:\n",
    "\n",
    "https://lilianweng.github.io/posts/2021-12-05-semi-supervised/#consistency-regularization\n",
    "\n",
    "\"Chen et al. (2020) proposed a three-step procedure to merge the benefits of self-supervised pretraining, supervised fine-tuning and self-training together:\n",
    "\n",
    "Unsupervised or self-supervised pretrain a big model.\n",
    "Supervised fine-tune it on a few labeled examples. It is important to use a big (deep and wide) neural network. **Bigger models yield better performance with fewer labeled samples.**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(coach.parameters(), lr=1e-3) # The Transformer began with lr=5, but we don't really need to be that radical.\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1000, gamma=0.1)\n",
    "alpha = 0.99 # This will be a discount factor through training. The discount factor must be maximum with epoch = 0 and decay overtime. Tip: 0.99 decays 10x faster than 0.999\n",
    "loss = torch.nn.NLLLoss()\n",
    "best_loss = float('inf')\n",
    "\n",
    "supervised_learning = 10 # After each N epochs, apply supervised fine-tuning.\n",
    "\n",
    "grads = []\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "epochs = 10000\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "\n",
    "params = torch.load(\"AutoLabeler_Checkpoint.tar\")\n",
    "\n",
    "start_epoch = params['Epoch']\n",
    "coach.load_state_dict(params['Best_Params'])\n",
    "optimizer = params['Optimizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting Training from epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    for i, (frames, _, _, _) in enumerate(dataloader):\n",
    "        coach.zero_grad()\n",
    "\n",
    "        frames = frames.to(device)\n",
    "\n",
    "        output = coach(frames)\n",
    "        outputalt = coach(frames)\n",
    "\n",
    "        command_type_consistency = loss(output[0], torch.argmax(outputalt[0].detach(), -1))\n",
    "        action1_consistency = loss(output[1], torch.argmax(outputalt[1].detach(), -1))\n",
    "        action2_consistency = loss(output[2], torch.argmax(outputalt[2].detach(), -1))\n",
    "\n",
    "        unsupervised_batch_loss = (command_type_consistency + action1_consistency + action2_consistency) * (1 - (alpha**epoch))\n",
    "\n",
    "        unsupervised_batch_loss.backward()\n",
    "\n",
    "        epoch_loss += unsupervised_batch_loss.item()\n",
    "\n",
    "        for n, p in coach.named_parameters():\n",
    "            if 'feature_extractor3.conv.weight' in n:\n",
    "                grads.append(torch.mean(p.grad))\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    del output, outputalt, frames, unsupervised_batch_loss\n",
    "\n",
    "    if 0 < epoch_loss < best_loss and epoch > 0:\n",
    "\n",
    "        best_loss = epoch_loss\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"{epoch}/{epochs}\\nCurrent Loss: {epoch_loss}\\tBest Loss: {best_loss}\\tCurrent Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "        print(f\"Command Type Consistency: {command_type_consistency}\\nAction1 Consistency: {action1_consistency}\\nAction2 Consistency: {action2_consistency}\")\n",
    "        print(f\"Gradients Average: {grads[-1]}\")\n",
    "\n",
    "        try:\n",
    "            torch.save({\n",
    "                        'Epoch': epoch,\n",
    "                        'Best_Params': coach.state_dict(),\n",
    "                        'Optimizer': optimizer\n",
    "                    }, f\"AutoLabeler_Checkpoint.tar\")\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if epoch % supervised_learning == 0 and epoch > 0: # Beginning supervised learning fine-tuning. Using less labeled data and a single epoch for this.\n",
    "\n",
    "        print(\"Beginning Supervised Fine-Tuning\")\n",
    "\n",
    "        supervised_loss = 0.\n",
    "\n",
    "        for i, (frames, encoded_command_type, encoded_actions1, encoded_actions2) in enumerate(dataloader):\n",
    "\n",
    "            if i < 3000:\n",
    "                pass\n",
    "            \n",
    "            coach.zero_grad()\n",
    "\n",
    "            frames = frames.to(device)\n",
    "            encoded_command_type = encoded_command_type.to(device)\n",
    "            encoded_actions1 = encoded_actions1.to(device)\n",
    "            encoded_actions2 = encoded_actions2.to(device)\n",
    "\n",
    "            output = coach(frames)\n",
    "\n",
    "            command_type_loss = loss(output[0], torch.argmax(encoded_command_type, -1))\n",
    "            action1_loss = loss(output[1], torch.argmax(encoded_actions1, -1))\n",
    "            action2_loss = loss(output[2], torch.argmax(encoded_actions2, -1))\n",
    "\n",
    "            batch_loss = command_type_loss + action1_loss + action2_loss\n",
    "\n",
    "            batch_loss.backward()\n",
    "\n",
    "            supervised_loss += batch_loss.item()\n",
    "\n",
    "            for n, p in coach.named_parameters():\n",
    "                if 'feature_extractor3.conv.weight' in n:\n",
    "                    grads.append(torch.mean(p.grad))\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Fine-tuning complete!\")\n",
    "        print(f\"Last Losses:\\nCommand Type: {command_type_loss}\\tAction 1: {action1_loss}\\tAction 2: {action2_loss}\\nTotal Batch Loss: {batch_loss}\")\n",
    "        print(f\"Total Loss: {supervised_loss}\")\n",
    "        print(f\"Gradients Average: {grads[-1]}\")\n",
    "\n",
    "print(f\"{epoch}/{epochs}\\nCurrent Loss: {epoch_loss}\\tBest Loss: {best_loss}\\tCurrent Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "print(f\"Command Type Consistency: {command_type_consistency}\\nAction1 Consistency: {action1_consistency}\\nAction2 Consistency: {action2_consistency}\")\n",
    "print(f\"Gradients Average: {grads[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = dataset.data[0:10]\n",
    "\n",
    "test_command_type = dataset.encoded_command_type[0:10]\n",
    "test_action1 = dataset.encoded_actions1[0:10]\n",
    "test_action2 = dataset.encoded_actions2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coach.eval()\n",
    "\n",
    "for i in range(10):\n",
    "    output = coach(test_images[i].to(device).unsqueeze(0))\n",
    "\n",
    "    print(output[0][0].argmax(), output[1][0].argmax(), output[2][0].argmax()) # Remember that output[0] will have size (Batch, command_types)\n",
    "    print(test_command_type[i].argmax(), test_action1[i].argmax(), test_action2[i].argmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
